{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132581/1009332340.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model 1번 불러오기 완료.\n",
      "Text model 2번 불러오기 완료.\n",
      "Text model 3번 불러오기 완료.\n",
      "Text model 4번 불러오기 완료.\n",
      "Text model 5번 불러오기 완료.\n",
      "Text model 6번 불러오기 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132581/1009332340.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 1번 불러오기 완료.\n",
      "Image model 2번 불러오기 완료.\n",
      "Image model 3번 불러오기 완료.\n",
      "Image model 4번 불러오기 완료.\n",
      "Image model 5번 불러오기 완료.\n",
      "Image model 6번 불러오기 완료.\n",
      "Text val. loader 1번 불러오기 완료.\n",
      "Text val. loader 2번 불러오기 완료.\n",
      "Text val. loader 3번 불러오기 완료.\n",
      "Text val. loader 4번 불러오기 완료.\n",
      "Text val. loader 5번 불러오기 완료.\n",
      "Text val. loader 6번 불러오기 완료.\n",
      "Image val. loader 1번 불러오기 완료.\n",
      "Image val. loader 2번 불러오기 완료.\n",
      "Image val. loader 3번 불러오기 완료.\n",
      "Image val. loader 4번 불러오기 완료.\n",
      "Image val. loader 5번 불러오기 완료.\n",
      "Image val. loader 6번 불러오기 완료.\n",
      "Starting gradual pruning process...\n",
      "[TEXT Model 1] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 1] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 1] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 1] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 1] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 1] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_1_pruned.pkl\n",
      "[TEXT Model 1] Final Test Accuracy: 0.8416\n",
      "---------\n",
      "[TEXT Model 2] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 2] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 2] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 2] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 2] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 2] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_2_pruned.pkl\n",
      "[TEXT Model 2] Final Test Accuracy: 0.7999\n",
      "---------\n",
      "[TEXT Model 3] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 3] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 3] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 3] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 3] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 3] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_3_pruned.pkl\n",
      "[TEXT Model 3] Final Test Accuracy: 0.8423\n",
      "---------\n",
      "[TEXT Model 4] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 4] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 4] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 4] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 4] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 4] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_4_pruned.pkl\n",
      "[TEXT Model 4] Final Test Accuracy: 0.8503\n",
      "---------\n",
      "[TEXT Model 5] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 5] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 5] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 5] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 5] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 5] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_5_pruned.pkl\n",
      "[TEXT Model 5] Final Test Accuracy: 0.8446\n",
      "---------\n",
      "[TEXT Model 6] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 6] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 6] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 6] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 6] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 6] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_6_pruned.pkl\n",
      "[TEXT Model 6] Final Test Accuracy: 0.7687\n",
      "---------\n",
      "[IMAGE Model 1] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 1] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 1] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 1] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 1] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 1] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_1_pruned.pkl\n",
      "[IMAGE Model 1] Final Test Accuracy: 0.5720\n",
      "---------\n",
      "[IMAGE Model 2] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 2] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 2] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 2] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 2] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 2] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_2_pruned.pkl\n",
      "[IMAGE Model 2] Final Test Accuracy: 0.5404\n",
      "---------\n",
      "[IMAGE Model 3] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 3] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 3] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 3] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 3] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 3] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_3_pruned.pkl\n",
      "[IMAGE Model 3] Final Test Accuracy: 0.4073\n",
      "---------\n",
      "[IMAGE Model 4] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 4] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 4] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 4] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 4] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 4] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_4_pruned.pkl\n",
      "[IMAGE Model 4] Final Test Accuracy: 0.4826\n",
      "---------\n",
      "[IMAGE Model 5] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 5] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 5] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 5] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 5] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 5] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_5_pruned.pkl\n",
      "[IMAGE Model 5] Final Test Accuracy: 0.3534\n",
      "---------\n",
      "[IMAGE Model 6] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 6] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 6] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 6] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 6] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 6] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_6_pruned.pkl\n",
      "[IMAGE Model 6] Final Test Accuracy: 0.4589\n",
      "---------\n",
      "Gradual pruning process finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from perceiver import tokenize_data, CustomDataset, PerceiverBlock, Perceiver, CombinedModel\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "root_dir = '/home/youlee/perceiver/perceiver/model/'\n",
    "loader_dir = '/home/youlee/perceiver/perceiver/loader/'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "input_models = []\n",
    "valid_loaders = []\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n",
    "    input_models.append(text_model)\n",
    "    print(f\"Text model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "for i in range(6):\n",
    "    img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n",
    "    input_models.append(img_model)\n",
    "    print(f\"Image model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    with open(loader_dir + f'text_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loaders.append(loaded_valid_dataset)\n",
    "    print(f\"Text val. loader {i+1}번 불러오기 완료.\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "for i in range(6):\n",
    "    with open(loader_dir + f'image_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)\n",
    "    print(f\"Image val. loader {i+1}번 불러오기 완료.\")\n",
    "\n",
    "\n",
    "\n",
    "class PackNet(nn.Module):\n",
    " \n",
    "    def __init__(self, model):\n",
    "        super(PackNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.masks = {}\n",
    "        self.current_task = None\n",
    "\n",
    "    def set_task(self, task_id):\n",
    "\n",
    "        self.current_task = task_id\n",
    "        if task_id not in self.masks:\n",
    "            self.masks[task_id] = {\n",
    "                name: torch.ones_like(param, device=param.device, dtype=torch.float32)\n",
    "                for name, param in self.model.named_parameters()\n",
    "                if param.requires_grad\n",
    "            }\n",
    "\n",
    "    def apply_mask(self):\n",
    "        if self.current_task not in self.masks:\n",
    "            return \n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    mask = self.masks[self.current_task][name]\n",
    "                    param.mul_(mask)  \n",
    "\n",
    "    def prune(self, prune_perc=0.2):\n",
    "        if self.current_task is None:\n",
    "            raise ValueError(\"Task must be set before pruning.\")\n",
    " \n",
    "        current_mask_dict = self.masks[self.current_task]\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                mask = current_mask_dict[name]\n",
    "                \n",
    "                param_data = param.data[mask.eq(1)]\n",
    "                if param_data.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                abs_tensor = param_data.abs().view(-1).cpu()\n",
    "\n",
    "                cutoff_rank = int(round(prune_perc * param_data.numel()))\n",
    "                \n",
    "                if cutoff_rank < 1:\n",
    "                    continue\n",
    "\n",
    "                cutoff_value = abs_tensor.kthvalue(cutoff_rank)[0].item()\n",
    "                to_zero = (param.abs() <= cutoff_value) & (mask.eq(1))\n",
    "                mask[to_zero] = 0.0\n",
    "\n",
    "                current_mask_dict[name] = mask\n",
    "\n",
    "        self.apply_mask()\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        self.apply_mask()\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device, is_text: bool):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if is_text:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def gradual_pruning(packnet_model, model_type, model_index, criterion, device, \n",
    "                    start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders):\n",
    "\n",
    "    model_path = f\"{checkpoint_dir}/{model_type}_model_{model_index+1}_pruned.pkl\"\n",
    "    sparsity_increment = (end_sparsity - start_sparsity) / pruning_steps\n",
    "    current_sparsity = start_sparsity\n",
    "\n",
    "    if model_type == \"text\":\n",
    "        test_loader = valid_loaders[model_index]\n",
    "        is_text = True\n",
    "    else:\n",
    "        test_loader = valid_loaders[model_index + 6]\n",
    "        is_text = False\n",
    "\n",
    "\n",
    "    for step in range(pruning_steps):\n",
    "        print(f\"[{model_type.upper()} Model {model_index+1}] Pruning Step {step+1}/{pruning_steps}, \"\n",
    "              f\"Target prune perc={current_sparsity:.2f}\")\n",
    "        packnet_model.prune(prune_perc=current_sparsity)\n",
    "        current_sparsity += sparsity_increment\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"model_state_dict\": packnet_model.state_dict(),\n",
    "            \"masks\": packnet_model.masks\n",
    "        }, f)\n",
    "    print(f\"[{model_type.upper()} Model {model_index+1}] Pruned model saved: {model_path}\")\n",
    "\n",
    "    test_loss, test_acc = eval_epoch(packnet_model, test_loader, criterion, device, is_text=is_text)\n",
    "    print(f\"[{model_type.upper()} Model {model_index+1}] Final Test Accuracy: {test_acc:.4f}\")\n",
    "    print(\"---------\")\n",
    "    \n",
    "    return packnet_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_sparsity = 0.05\n",
    "    end_sparsity = 0.2\n",
    "    pruning_steps = 5\n",
    "    checkpoint_dir = \"/home/youlee/perceiver/perceiver/checkpoints_pruned2\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting gradual pruning process...\")\n",
    "\n",
    "    text_models = input_models[:6]\n",
    "    image_models = input_models[6:]\n",
    "\n",
    "    pruned_text_models = []\n",
    "    for i, model in enumerate(text_models):\n",
    "        packnet_model = PackNet(model)\n",
    "        packnet_model.set_task(f\"text_task_{i+1}\")\n",
    "        packnet_model.to(device)\n",
    "\n",
    "        pruned_text_model = gradual_pruning(\n",
    "            packnet_model, \"text\", i, criterion, device,\n",
    "            start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders\n",
    "        )\n",
    "        pruned_text_models.append(pruned_text_model)\n",
    "\n",
    "    pruned_image_models = []\n",
    "    for i, model in enumerate(image_models):\n",
    "        packnet_model = PackNet(model)\n",
    "        packnet_model.set_task(f\"image_task_{i+1}\")\n",
    "        packnet_model.to(device)\n",
    "\n",
    "        pruned_image_model = gradual_pruning(\n",
    "            packnet_model, \"image\", i, criterion, device,\n",
    "            start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders\n",
    "        )\n",
    "        pruned_image_models.append(pruned_image_model)\n",
    "\n",
    "    print(\"Gradual pruning process finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
