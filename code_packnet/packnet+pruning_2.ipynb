{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132581/1009332340.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model 1번 불러오기 완료.\n",
      "Text model 2번 불러오기 완료.\n",
      "Text model 3번 불러오기 완료.\n",
      "Text model 4번 불러오기 완료.\n",
      "Text model 5번 불러오기 완료.\n",
      "Text model 6번 불러오기 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132581/1009332340.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 1번 불러오기 완료.\n",
      "Image model 2번 불러오기 완료.\n",
      "Image model 3번 불러오기 완료.\n",
      "Image model 4번 불러오기 완료.\n",
      "Image model 5번 불러오기 완료.\n",
      "Image model 6번 불러오기 완료.\n",
      "Text val. loader 1번 불러오기 완료.\n",
      "Text val. loader 2번 불러오기 완료.\n",
      "Text val. loader 3번 불러오기 완료.\n",
      "Text val. loader 4번 불러오기 완료.\n",
      "Text val. loader 5번 불러오기 완료.\n",
      "Text val. loader 6번 불러오기 완료.\n",
      "Image val. loader 1번 불러오기 완료.\n",
      "Image val. loader 2번 불러오기 완료.\n",
      "Image val. loader 3번 불러오기 완료.\n",
      "Image val. loader 4번 불러오기 완료.\n",
      "Image val. loader 5번 불러오기 완료.\n",
      "Image val. loader 6번 불러오기 완료.\n",
      "Starting gradual pruning process...\n",
      "[TEXT Model 1] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 1] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 1] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 1] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 1] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 1] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_1_pruned.pkl\n",
      "[TEXT Model 1] Final Test Accuracy: 0.8416\n",
      "---------\n",
      "[TEXT Model 2] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 2] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 2] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 2] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 2] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 2] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_2_pruned.pkl\n",
      "[TEXT Model 2] Final Test Accuracy: 0.7999\n",
      "---------\n",
      "[TEXT Model 3] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 3] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 3] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 3] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 3] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 3] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_3_pruned.pkl\n",
      "[TEXT Model 3] Final Test Accuracy: 0.8423\n",
      "---------\n",
      "[TEXT Model 4] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 4] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 4] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 4] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 4] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 4] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_4_pruned.pkl\n",
      "[TEXT Model 4] Final Test Accuracy: 0.8503\n",
      "---------\n",
      "[TEXT Model 5] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 5] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 5] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 5] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 5] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 5] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_5_pruned.pkl\n",
      "[TEXT Model 5] Final Test Accuracy: 0.8446\n",
      "---------\n",
      "[TEXT Model 6] Pruning Step 1/5, Target prune perc=0.05\n",
      "[TEXT Model 6] Pruning Step 2/5, Target prune perc=0.08\n",
      "[TEXT Model 6] Pruning Step 3/5, Target prune perc=0.11\n",
      "[TEXT Model 6] Pruning Step 4/5, Target prune perc=0.14\n",
      "[TEXT Model 6] Pruning Step 5/5, Target prune perc=0.17\n",
      "[TEXT Model 6] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/text_model_6_pruned.pkl\n",
      "[TEXT Model 6] Final Test Accuracy: 0.7687\n",
      "---------\n",
      "[IMAGE Model 1] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 1] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 1] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 1] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 1] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 1] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_1_pruned.pkl\n",
      "[IMAGE Model 1] Final Test Accuracy: 0.5720\n",
      "---------\n",
      "[IMAGE Model 2] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 2] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 2] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 2] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 2] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 2] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_2_pruned.pkl\n",
      "[IMAGE Model 2] Final Test Accuracy: 0.5404\n",
      "---------\n",
      "[IMAGE Model 3] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 3] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 3] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 3] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 3] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 3] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_3_pruned.pkl\n",
      "[IMAGE Model 3] Final Test Accuracy: 0.4073\n",
      "---------\n",
      "[IMAGE Model 4] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 4] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 4] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 4] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 4] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 4] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_4_pruned.pkl\n",
      "[IMAGE Model 4] Final Test Accuracy: 0.4826\n",
      "---------\n",
      "[IMAGE Model 5] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 5] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 5] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 5] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 5] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 5] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_5_pruned.pkl\n",
      "[IMAGE Model 5] Final Test Accuracy: 0.3534\n",
      "---------\n",
      "[IMAGE Model 6] Pruning Step 1/5, Target prune perc=0.05\n",
      "[IMAGE Model 6] Pruning Step 2/5, Target prune perc=0.08\n",
      "[IMAGE Model 6] Pruning Step 3/5, Target prune perc=0.11\n",
      "[IMAGE Model 6] Pruning Step 4/5, Target prune perc=0.14\n",
      "[IMAGE Model 6] Pruning Step 5/5, Target prune perc=0.17\n",
      "[IMAGE Model 6] Pruned model saved: /home/youlee/perceiver/perceiver/checkpoints_pruned2/image_model_6_pruned.pkl\n",
      "[IMAGE Model 6] Final Test Accuracy: 0.4589\n",
      "---------\n",
      "Gradual pruning process finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from perceiver import tokenize_data, CustomDataset, PerceiverBlock, Perceiver, CombinedModel\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "root_dir = '/home/youlee/perceiver/perceiver/model/'\n",
    "loader_dir = '/home/youlee/perceiver/perceiver/loader/'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "input_models = []\n",
    "valid_loaders = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n",
    "    input_models.append(text_model)\n",
    "    print(f\"Text model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "for i in range(6):\n",
    "    img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n",
    "    input_models.append(img_model)\n",
    "    print(f\"Image model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    with open(loader_dir + f'text_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loaders.append(loaded_valid_dataset)\n",
    "    print(f\"Text val. loader {i+1}번 불러오기 완료.\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for i in range(6):\n",
    "    with open(loader_dir + f'image_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)\n",
    "    print(f\"Image val. loader {i+1}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PackNet 클래스: 원본 모델에 대해 pruning 적용용\n",
    "class PackNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(PackNet, self).__init__()\n",
    "        self.model = model        # 원본 모델을 저장\n",
    "        self.masks = {}           # task 별 마스크를 저장할 딕셔너리\n",
    "        self.current_task = None  # 현재 task ID\n",
    "\n",
    "    # 특정 task 설정 -> 해당 task에 대해 모든 파라미터 1로 초기화한 mask 생성\n",
    "    def set_task(self, task_id):\n",
    "        self.current_task = task_id\n",
    "        if task_id not in self.masks:\n",
    "            self.masks[task_id] = {\n",
    "                # 각 파라미터와 같은 shape, 같은 device, float32 타입의 1로 채워진 텐서를 생성\n",
    "                name: torch.ones_like(param, device=param.device, dtype=torch.float32)\n",
    "                for name, param in self.model.named_parameters()\n",
    "                if param.requires_grad\n",
    "            }\n",
    "\n",
    "    # 현재 mask를 모델의 파라미터에 적용 (element-wise 곱셈)\n",
    "    def apply_mask(self):\n",
    "        if self.current_task not in self.masks:\n",
    "            return \n",
    "        with torch.no_grad():\n",
    "            # 모든 trainable 파라미터에 대해 마스크를 곱하여, 마스크 값이 0인 요소는 0으로 만듦\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    mask = self.masks[self.current_task][name]\n",
    "                    param.mul_(mask)  \n",
    "\n",
    "    # 주어진 prune_perc 비율에 따라 가중치 중 작은 값들을 pruning (마스크 0으로 변경)\n",
    "    def prune(self, prune_perc=0.2):\n",
    "        if self.current_task is None:\n",
    "            raise ValueError(\"Task must be set before pruning.\")\n",
    " \n",
    "        current_mask_dict = self.masks[self.current_task]\n",
    "\n",
    "        # 모델의 각 파라미터에 대해 pruning 적용\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                mask = current_mask_dict[name]\n",
    "                # 현재 마스크가 1인 파라미터 값들만 선택\n",
    "                param_data = param.data[mask.eq(1)]\n",
    "                if param_data.numel() == 0:\n",
    "                    continue  # 해당 파라미터가 이미 모두 pruning된 경우 continue\n",
    "                abs_tensor = param_data.abs().view(-1).cpu()\n",
    "\n",
    "                # prune_perc 비율에 해당하는 인덱스 계산 \n",
    "                cutoff_rank = int(round(prune_perc * param_data.numel()))\n",
    "                \n",
    "                if cutoff_rank < 1:\n",
    "                    continue  \n",
    "\n",
    "                # 절대값 기준으로 cutoff_rank번째 값 (kthvalue)을 cutoff_value로 선정\n",
    "                cutoff_value = abs_tensor.kthvalue(cutoff_rank)[0].item()\n",
    "                # 원래 파라미터 값 중 절대값이 cutoff_value 이하인 부분과, 현재 마스크가 1인 부분을 찾아 0으로 만듦\n",
    "                to_zero = (param.abs() <= cutoff_value) & (mask.eq(1))\n",
    "                mask[to_zero] = 0.0\n",
    "\n",
    "                # 업데이트된 마스크를 저장\n",
    "                current_mask_dict[name] = mask\n",
    "\n",
    "        # pruning 후, 마스크를 모델 파라미터에 적용\n",
    "        self.apply_mask()\n",
    "\n",
    "    # forward 함수: 마스크를 적용한 후, 원본 모델의 forward를 호출하여 결과 반환\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        self.apply_mask()\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "# 주어진 데이터 로더에 대해 모델을 평가\n",
    "def eval_epoch(model, dataloader, criterion, device, is_text: bool):\n",
    "    model.eval()  \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch in dataloader:\n",
    "            if is_text:\n",
    "                # 텍스트 데이터인 경우, 입력 데이터와 attention mask, label을 device로 이동\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                # 텍스트 모델은 보통 input_ids를 이용하여 forward 수행 \n",
    "                outputs = model(input_ids)\n",
    "            else:\n",
    "                # 이미지 데이터인 경우, 입력과 label을 device로 이동\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# gradual_pruning 함수: 주어진 모델에 대해 gradual pruning을 진행\n",
    "def gradual_pruning(packnet_model, model_type, model_index, criterion, device, \n",
    "                    start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders):\n",
    "\n",
    "    model_path = f\"{checkpoint_dir}/{model_type}_model_{model_index+1}_pruned.pkl\"\n",
    "    # pruning 단계마다 증가할 sparsity (prune_perc) 값 계산\n",
    "    sparsity_increment = (end_sparsity - start_sparsity) / pruning_steps\n",
    "    current_sparsity = start_sparsity\n",
    "\n",
    "    # 텍스트 모델과 이미지 모델의 검증 데이터 로더 선택\n",
    "    if model_type == \"text\":\n",
    "        test_loader = valid_loaders[model_index]\n",
    "        is_text = True\n",
    "    else:\n",
    "        test_loader = valid_loaders[model_index + 6]\n",
    "        is_text = False\n",
    "\n",
    "    # 지정된 pruning step 횟수만큼 반복하면서 pruning 수행\n",
    "    for step in range(pruning_steps):\n",
    "        print(f\"[{model_type.upper()} Model {model_index+1}] Pruning Step {step+1}/{pruning_steps}, \"\n",
    "              f\"Target prune perc={current_sparsity:.2f}\")\n",
    "        # 현재 sparsity (prune_perc) 비율로 pruning 수행\n",
    "        packnet_model.prune(prune_perc=current_sparsity)\n",
    "        # 다음 step을 위해 sparsity 증가\n",
    "        current_sparsity += sparsity_increment\n",
    "\n",
    "    # pruning이 완료된 모델 상태와 마스크 정보를 pickle 파일로 저장\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"model_state_dict\": packnet_model.state_dict(),\n",
    "            \"masks\": packnet_model.masks\n",
    "        }, f)\n",
    "    print(f\"[{model_type.upper()} Model {model_index+1}] Pruned model saved: {model_path}\")\n",
    "\n",
    "    test_loss, test_acc = eval_epoch(packnet_model, test_loader, criterion, device, is_text=is_text)\n",
    "    print(f\"[{model_type.upper()} Model {model_index+1}] Final Test Accuracy: {test_acc:.4f}\")\n",
    "    print(\"---------\")\n",
    "    \n",
    "    return packnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_sparsity = 0.05\n",
    "    end_sparsity = 0.2\n",
    "    pruning_steps = 5\n",
    "    checkpoint_dir = \"/home/youlee/perceiver/perceiver/checkpoints_pruned2\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting gradual pruning process...\")\n",
    "\n",
    "    text_models = input_models[:6]\n",
    "    image_models = input_models[6:]\n",
    "\n",
    "    pruned_text_models = []\n",
    "    for i, model in enumerate(text_models):\n",
    "        packnet_model = PackNet(model)\n",
    "        packnet_model.set_task(f\"text_task_{i+1}\")\n",
    "        packnet_model.to(device)\n",
    "\n",
    "        pruned_text_model = gradual_pruning(\n",
    "            packnet_model, \"text\", i, criterion, device,\n",
    "            start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders\n",
    "        )\n",
    "        pruned_text_models.append(pruned_text_model)\n",
    "\n",
    "    pruned_image_models = []\n",
    "    for i, model in enumerate(image_models):\n",
    "        packnet_model = PackNet(model)\n",
    "        packnet_model.set_task(f\"image_task_{i+1}\")\n",
    "        packnet_model.to(device)\n",
    "\n",
    "        pruned_image_model = gradual_pruning(\n",
    "            packnet_model, \"image\", i, criterion, device,\n",
    "            start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders\n",
    "        )\n",
    "        pruned_image_models.append(pruned_image_model)\n",
    "\n",
    "    print(\"Gradual pruning process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
