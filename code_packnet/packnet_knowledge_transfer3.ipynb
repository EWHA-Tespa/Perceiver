{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from perceiver import crop, patchify, get_patch_coords, tokenize_data, CustomDataset, PerceiverBlock, Perceiver, CombinedModel, ImageDataset\n",
    "\n",
    "# Device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 경로 설정\n",
    "pruned_model_dir = \"/home/youlee/perceiver/perceiver/checkpoints_pruned2/\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터 전처리 및 로드\n",
    "image_root_dir = \"/home/youlee/n24news/n24news/image\"\n",
    "class_groups = [\n",
    "    [\"Opinion\", \"Art & Design\", \"Television\"],\n",
    "    [\"Music\", \"Travel\", \"Real Estate\"],\n",
    "    [\"Books\", \"Theater\", \"Health\"],\n",
    "    [\"Sports\", \"Science\", \"Food\"],\n",
    "    [\"Fashion & Style\", \"Movies\", \"Technology\"],\n",
    "    [\"Dance\", \"Media\", \"Style\"]\n",
    "]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image_datasets = [ImageDataset(image_root_dir, transform=transform, selected_classes=group) for group in class_groups]\n",
    "# Train Valid Split \n",
    "image_train_loaders, image_valid_loaders = [], []\n",
    "for dataset in image_datasets:\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_set, valid_set = random_split(dataset, [train_size, valid_size])\n",
    "    image_train_loaders.append(DataLoader(train_set, batch_size=32, shuffle=True))\n",
    "    image_valid_loaders.append(DataLoader(valid_set, batch_size=32, shuffle=False))\n",
    "\n",
    "# 텍스트 데이터 전처리 및 로드\n",
    "text_root_dir = \"/home/youlee/n24news/n24news/\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def load_text_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    input_ids, attention_masks = tokenize_data(df, tokenizer=tokenizer, MAX_LENGTH=128)\n",
    "    \n",
    "    if df[\"Label\"].dtype == \"object\":\n",
    "        df[\"Label\"] = label_encoder.fit_transform(df[\"Label\"])\n",
    "    labels = torch.tensor(df[\"Label\"].values, dtype=torch.long)\n",
    "    \n",
    "    return CustomDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "text_datasets = [load_text_data(f\"{text_root_dir}regroup_{i}.csv\") for i in range(1, 7)]\n",
    "\n",
    "# Train Valid Split \n",
    "text_train_loaders, text_valid_loaders = [], []\n",
    "for dataset in text_datasets:\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_set, valid_set = random_split(dataset, [train_size, valid_size])\n",
    "    text_train_loaders.append(DataLoader(train_set, batch_size=32, shuffle=True))\n",
    "    text_valid_loaders.append(DataLoader(valid_set, batch_size=32, shuffle=False))\n",
    "\n",
    "train_loaders = text_train_loaders + image_train_loaders\n",
    "valid_loaders = text_valid_loaders + image_valid_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruned 모델 로드 함수\n",
    "def load_pruned_models(pruned_model_dir, model_dir):\n",
    "    pruned_text_models, pruned_image_models = [], []\n",
    "    \n",
    "    # Text 모델 로드\n",
    "    for i in range(6):\n",
    "        file_path = f\"{pruned_model_dir}/text_model_{i+1}_pruned.pkl\"\n",
    "        model_path = f\"{model_dir}/text_model_{i+1}.pkl\"\n",
    "        \n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = torch.load(model_path)\n",
    "        state_dict = {k.replace(\"model.\", \"\"): v for k, v in model_data[\"model_state_dict\"].items()}\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        pruned_text_models.append(model)\n",
    "\n",
    "    # Image 모델 로드\n",
    "    for i in range(6):\n",
    "        file_path = f\"{pruned_model_dir}/image_model_{i+1}_pruned.pkl\"\n",
    "        model_path = f\"{model_dir}/image_model_{i+1}.pkl\"\n",
    "        \n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = torch.load(model_path)\n",
    "        state_dict = {k.replace(\"model.\", \"\"): v for k, v in model_data[\"model_state_dict\"].items()}\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        pruned_image_models.append(model)\n",
    "\n",
    "    return pruned_text_models, pruned_image_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation 데이터 로드 함수\n",
    "# def load_valid_loaders():\n",
    "#     valid_loaders = []\n",
    "#     for i in range(6):\n",
    "#         with open(f\"{loader_dir}text_val_loader_{i+1}.pkl\", 'rb') as f:\n",
    "#             valid_loaders.append(pickle.load(f))\n",
    "#     for i in range(6):\n",
    "#         with open(f\"{loader_dir}image_val_loader_{i+1}.pkl\", 'rb') as f:\n",
    "#             valid_dataset = pickle.load(f)\n",
    "#             valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "#             valid_loaders.append(valid_loader)\n",
    "#     return valid_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키 매핑 함수\n",
    "def map_keys(state_dict, modality):\n",
    "    key_map = {}\n",
    "    for key in state_dict.keys():\n",
    "        if modality == \"Text\" and \"model.perceiver\" in key:\n",
    "            key_map[key] = key.replace(\"model.perceiver\", \"model\")\n",
    "        elif modality == \"Image\" and \"model.\" in key:\n",
    "            key_map[key] = key.replace(\"model.\", \"model.perceiver\")\n",
    "        else:\n",
    "            key_map[key] = key\n",
    "    return key_map\n",
    "\n",
    "# 가중치 매핑 및 로드 함수\n",
    "def apply_mapped_state_dict(task_model, backbone_model, modality):\n",
    "    backbone_state_dict = backbone_model.state_dict()\n",
    "    key_map = map_keys(backbone_state_dict, modality)\n",
    "    mapped_state_dict = {key_map[k]: v for k, v in backbone_state_dict.items()}\n",
    "    task_model.load_state_dict(mapped_state_dict, strict=False)\n",
    "    print(f\"Successfully applied backbone weights for {modality} task.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_transfer(task_id, backbone_id, backbone_modality, task_modality, \n",
    "                       task_models, backbone_models, train_loaders, criterion, device, epochs=20):\n",
    "    if backbone_modality == \"Text\":\n",
    "        backbone_model = backbone_models[backbone_id]\n",
    "    else:\n",
    "        backbone_model = backbone_models[backbone_id - 6]\n",
    "\n",
    "    task_model = task_models[task_id]\n",
    "    task_model.to(device)\n",
    "\n",
    "    apply_mapped_state_dict(task_model, backbone_model, backbone_modality)\n",
    "\n",
    "    optimizer = optim.SGD(task_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    task_model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loaders[task_id]:\n",
    "            optimizer.zero_grad()\n",
    "            if task_modality == \"Text\":\n",
    "                inputs = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = task_model(inputs)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = task_model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Task {task_id} | Backbone {backbone_id} | Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Accuracy: {correct/total:.4f}\")\n",
    "    return task_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataloader, criterion, device, is_text: bool):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if is_text:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # # 정답 및 오답 샘플 저장\n",
    "            # for i in range(len(labels)):\n",
    "            #     if len(correct_samples) < 4 and predicted[i] == labels[i]:\n",
    "            #         correct_samples.append((input_ids[i], labels[i].item(), predicted[i].item()))\n",
    "            #     elif len(incorrect_samples) < 4 and predicted[i] != labels[i]:\n",
    "            #         incorrect_samples.append((input_ids[i], labels[i].item(), predicted[i].item()))\n",
    "\n",
    "            # # 4개씩만 저장하고 종료\n",
    "            # if len(correct_samples) >= 4 and len(incorrect_samples) >= 4:\n",
    "            #     break\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 평가 함수\n",
    "def evaluate_transfer(models, valid_loaders, criterion, device):\n",
    "    for i, model in enumerate(models):\n",
    "        is_text = i < 6\n",
    "\n",
    "        test_loss, test_acc = eval_epoch(model, valid_loaders[i], criterion, device, is_text)\n",
    "        modality = \"Text\" if is_text else \"Image\"\n",
    "        print(f\"Task {i} ({modality}) | Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        test_loss, test_acc = eval_epoch(model, valid_loaders[i], criterion, device, is_text)\n",
    "        modality = \"Text\" if is_text else \"Image\"\n",
    "        print(f\"Task {i} ({modality}) | Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine 및 Euclidean 결과 로드\n",
    "cosine_results = pd.read_csv(\"/home/youlee/perceiver/perceiver/code/best_cosine_results.txt\", sep='\\t')\n",
    "euclidean_results = pd.read_csv(\"/home/youlee/perceiver/perceiver/code/best_euclidean_results.txt\", sep='\\t')\n",
    "cosine_task_to_backbone = {row[\"Task_ID\"]: row[\"Best_Target_ID\"] for _, row in cosine_results.iterrows()}\n",
    "euclidean_task_to_backbone = {row[\"Task_ID\"]: row[\"Best_Target_ID\"] for _, row in euclidean_results.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pruned models and validation loaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3259257/3255779742.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)\n",
      "/tmp/ipykernel_3259257/3255779742.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cosine-based Knowledge Transfer...\n",
      "Successfully applied backbone weights for Image task.\n",
      "Task 0 | Backbone 9 | Epoch 1/20 | Loss: 207.9272 | Accuracy: 0.5164\n",
      "Task 0 | Backbone 9 | Epoch 2/20 | Loss: 151.2699 | Accuracy: 0.6022\n",
      "Task 0 | Backbone 9 | Epoch 3/20 | Loss: 138.9955 | Accuracy: 0.6577\n",
      "Task 0 | Backbone 9 | Epoch 4/20 | Loss: 125.3024 | Accuracy: 0.7150\n",
      "Task 0 | Backbone 9 | Epoch 5/20 | Loss: 112.9424 | Accuracy: 0.7576\n",
      "Task 0 | Backbone 9 | Epoch 6/20 | Loss: 105.9237 | Accuracy: 0.7818\n",
      "Task 0 | Backbone 9 | Epoch 7/20 | Loss: 99.1049 | Accuracy: 0.7972\n",
      "Task 0 | Backbone 9 | Epoch 8/20 | Loss: 94.5233 | Accuracy: 0.8068\n",
      "Task 0 | Backbone 9 | Epoch 9/20 | Loss: 91.0497 | Accuracy: 0.8151\n",
      "Task 0 | Backbone 9 | Epoch 10/20 | Loss: 87.2934 | Accuracy: 0.8264\n",
      "Task 0 | Backbone 9 | Epoch 11/20 | Loss: 84.6893 | Accuracy: 0.8283\n",
      "Task 0 | Backbone 9 | Epoch 12/20 | Loss: 81.5821 | Accuracy: 0.8331\n",
      "Task 0 | Backbone 9 | Epoch 13/20 | Loss: 79.8972 | Accuracy: 0.8386\n",
      "Task 0 | Backbone 9 | Epoch 14/20 | Loss: 78.5397 | Accuracy: 0.8420\n",
      "Task 0 | Backbone 9 | Epoch 15/20 | Loss: 75.5582 | Accuracy: 0.8482\n",
      "Task 0 | Backbone 9 | Epoch 16/20 | Loss: 73.8635 | Accuracy: 0.8511\n",
      "Task 0 | Backbone 9 | Epoch 17/20 | Loss: 71.5785 | Accuracy: 0.8571\n",
      "Task 0 | Backbone 9 | Epoch 18/20 | Loss: 69.6118 | Accuracy: 0.8598\n",
      "Task 0 | Backbone 9 | Epoch 19/20 | Loss: 68.0121 | Accuracy: 0.8655\n",
      "Task 0 | Backbone 9 | Epoch 20/20 | Loss: 65.9713 | Accuracy: 0.8712\n",
      "Successfully applied backbone weights for Image task.\n",
      "Task 1 | Backbone 10 | Epoch 1/20 | Loss: 241.3594 | Accuracy: 0.4089\n",
      "Task 1 | Backbone 10 | Epoch 2/20 | Loss: 176.0021 | Accuracy: 0.4916\n",
      "Task 1 | Backbone 10 | Epoch 3/20 | Loss: 171.8413 | Accuracy: 0.5224\n",
      "Task 1 | Backbone 10 | Epoch 4/20 | Loss: 162.0244 | Accuracy: 0.5648\n",
      "Task 1 | Backbone 10 | Epoch 5/20 | Loss: 153.3432 | Accuracy: 0.5917\n",
      "Task 1 | Backbone 10 | Epoch 6/20 | Loss: 145.5983 | Accuracy: 0.6228\n",
      "Task 1 | Backbone 10 | Epoch 7/20 | Loss: 139.0596 | Accuracy: 0.6529\n",
      "Task 1 | Backbone 10 | Epoch 8/20 | Loss: 131.2230 | Accuracy: 0.6834\n",
      "Task 1 | Backbone 10 | Epoch 9/20 | Loss: 123.9984 | Accuracy: 0.7102\n",
      "Task 1 | Backbone 10 | Epoch 10/20 | Loss: 120.6978 | Accuracy: 0.7352\n",
      "Task 1 | Backbone 10 | Epoch 11/20 | Loss: 111.4582 | Accuracy: 0.7516\n",
      "Task 1 | Backbone 10 | Epoch 12/20 | Loss: 105.8615 | Accuracy: 0.7675\n",
      "Task 1 | Backbone 10 | Epoch 13/20 | Loss: 100.8608 | Accuracy: 0.7813\n",
      "Task 1 | Backbone 10 | Epoch 14/20 | Loss: 96.0228 | Accuracy: 0.7968\n",
      "Task 1 | Backbone 10 | Epoch 15/20 | Loss: 92.3904 | Accuracy: 0.8072\n",
      "Task 1 | Backbone 10 | Epoch 16/20 | Loss: 88.3321 | Accuracy: 0.8151\n",
      "Task 1 | Backbone 10 | Epoch 17/20 | Loss: 85.9054 | Accuracy: 0.8201\n",
      "Task 1 | Backbone 10 | Epoch 18/20 | Loss: 82.1918 | Accuracy: 0.8291\n",
      "Task 1 | Backbone 10 | Epoch 19/20 | Loss: 77.6993 | Accuracy: 0.8384\n",
      "Task 1 | Backbone 10 | Epoch 20/20 | Loss: 74.0296 | Accuracy: 0.8446\n",
      "Successfully applied backbone weights for Image task.\n",
      "Task 2 | Backbone 8 | Epoch 1/20 | Loss: 31.4265 | Accuracy: 0.9575\n",
      "Task 2 | Backbone 8 | Epoch 2/20 | Loss: 24.7320 | Accuracy: 0.9659\n",
      "Task 2 | Backbone 8 | Epoch 3/20 | Loss: 22.2597 | Accuracy: 0.9711\n",
      "Task 2 | Backbone 8 | Epoch 4/20 | Loss: 20.3619 | Accuracy: 0.9718\n",
      "Task 2 | Backbone 8 | Epoch 5/20 | Loss: 18.7408 | Accuracy: 0.9775\n",
      "Task 2 | Backbone 8 | Epoch 6/20 | Loss: 17.6323 | Accuracy: 0.9791\n",
      "Task 2 | Backbone 8 | Epoch 7/20 | Loss: 16.7936 | Accuracy: 0.9810\n",
      "Task 2 | Backbone 8 | Epoch 8/20 | Loss: 16.3308 | Accuracy: 0.9813\n",
      "Task 2 | Backbone 8 | Epoch 9/20 | Loss: 15.2902 | Accuracy: 0.9839\n",
      "Task 2 | Backbone 8 | Epoch 10/20 | Loss: 14.9132 | Accuracy: 0.9844\n",
      "Task 2 | Backbone 8 | Epoch 11/20 | Loss: 14.5908 | Accuracy: 0.9850\n",
      "Task 2 | Backbone 8 | Epoch 12/20 | Loss: 14.1265 | Accuracy: 0.9853\n",
      "Task 2 | Backbone 8 | Epoch 13/20 | Loss: 13.6651 | Accuracy: 0.9857\n",
      "Task 2 | Backbone 8 | Epoch 14/20 | Loss: 13.3341 | Accuracy: 0.9869\n",
      "Task 2 | Backbone 8 | Epoch 15/20 | Loss: 13.0475 | Accuracy: 0.9867\n",
      "Task 2 | Backbone 8 | Epoch 16/20 | Loss: 12.8381 | Accuracy: 0.9879\n",
      "Task 2 | Backbone 8 | Epoch 17/20 | Loss: 12.4553 | Accuracy: 0.9877\n",
      "Task 2 | Backbone 8 | Epoch 18/20 | Loss: 12.1648 | Accuracy: 0.9886\n",
      "Task 2 | Backbone 8 | Epoch 19/20 | Loss: 11.9341 | Accuracy: 0.9884\n",
      "Task 2 | Backbone 8 | Epoch 20/20 | Loss: 11.6748 | Accuracy: 0.9891\n",
      "Successfully applied backbone weights for Image task.\n",
      "Task 3 | Backbone 10 | Epoch 1/20 | Loss: 225.5947 | Accuracy: 0.4519\n",
      "Task 3 | Backbone 10 | Epoch 2/20 | Loss: 172.3789 | Accuracy: 0.5062\n",
      "Task 3 | Backbone 10 | Epoch 3/20 | Loss: 167.1762 | Accuracy: 0.5191\n",
      "Task 3 | Backbone 10 | Epoch 4/20 | Loss: 162.7337 | Accuracy: 0.5320\n",
      "Task 3 | Backbone 10 | Epoch 5/20 | Loss: 158.8841 | Accuracy: 0.5488\n",
      "Task 3 | Backbone 10 | Epoch 6/20 | Loss: 154.7304 | Accuracy: 0.5595\n",
      "Task 3 | Backbone 10 | Epoch 7/20 | Loss: 150.1457 | Accuracy: 0.5809\n",
      "Task 3 | Backbone 10 | Epoch 8/20 | Loss: 145.8653 | Accuracy: 0.5972\n",
      "Task 3 | Backbone 10 | Epoch 9/20 | Loss: 141.1858 | Accuracy: 0.6309\n",
      "Task 3 | Backbone 10 | Epoch 10/20 | Loss: 135.1362 | Accuracy: 0.6622\n",
      "Task 3 | Backbone 10 | Epoch 11/20 | Loss: 128.4379 | Accuracy: 0.6941\n",
      "Task 3 | Backbone 10 | Epoch 12/20 | Loss: 120.8606 | Accuracy: 0.7230\n",
      "Task 3 | Backbone 10 | Epoch 13/20 | Loss: 112.4782 | Accuracy: 0.7513\n",
      "Task 3 | Backbone 10 | Epoch 14/20 | Loss: 104.7899 | Accuracy: 0.7773\n",
      "Task 3 | Backbone 10 | Epoch 15/20 | Loss: 97.2884 | Accuracy: 0.7961\n",
      "Task 3 | Backbone 10 | Epoch 16/20 | Loss: 90.4269 | Accuracy: 0.8224\n",
      "Task 3 | Backbone 10 | Epoch 17/20 | Loss: 85.9863 | Accuracy: 0.8264\n",
      "Task 3 | Backbone 10 | Epoch 18/20 | Loss: 81.4787 | Accuracy: 0.8382\n",
      "Task 3 | Backbone 10 | Epoch 19/20 | Loss: 78.1140 | Accuracy: 0.8459\n",
      "Task 3 | Backbone 10 | Epoch 20/20 | Loss: 73.3858 | Accuracy: 0.8624\n",
      "Successfully applied backbone weights for Image task.\n",
      "Task 4 | Backbone 10 | Epoch 1/20 | Loss: 28.2973 | Accuracy: 0.9626\n",
      "Task 4 | Backbone 10 | Epoch 2/20 | Loss: 19.9400 | Accuracy: 0.9746\n",
      "Task 4 | Backbone 10 | Epoch 3/20 | Loss: 17.5057 | Accuracy: 0.9806\n",
      "Task 4 | Backbone 10 | Epoch 4/20 | Loss: 15.5428 | Accuracy: 0.9843\n",
      "Task 4 | Backbone 10 | Epoch 5/20 | Loss: 14.2675 | Accuracy: 0.9858\n",
      "Task 4 | Backbone 10 | Epoch 6/20 | Loss: 13.6234 | Accuracy: 0.9869\n",
      "Task 4 | Backbone 10 | Epoch 7/20 | Loss: 12.9379 | Accuracy: 0.9874\n",
      "Task 4 | Backbone 10 | Epoch 8/20 | Loss: 12.1657 | Accuracy: 0.9886\n",
      "Task 4 | Backbone 10 | Epoch 9/20 | Loss: 11.6879 | Accuracy: 0.9890\n",
      "Task 4 | Backbone 10 | Epoch 10/20 | Loss: 11.0952 | Accuracy: 0.9900\n",
      "Task 4 | Backbone 10 | Epoch 11/20 | Loss: 10.5778 | Accuracy: 0.9902\n",
      "Task 4 | Backbone 10 | Epoch 12/20 | Loss: 10.1139 | Accuracy: 0.9913\n",
      "Task 4 | Backbone 10 | Epoch 13/20 | Loss: 9.7000 | Accuracy: 0.9920\n",
      "Task 4 | Backbone 10 | Epoch 14/20 | Loss: 9.4039 | Accuracy: 0.9925\n",
      "Task 4 | Backbone 10 | Epoch 15/20 | Loss: 9.2499 | Accuracy: 0.9927\n",
      "Task 4 | Backbone 10 | Epoch 16/20 | Loss: 8.9717 | Accuracy: 0.9928\n",
      "Task 4 | Backbone 10 | Epoch 17/20 | Loss: 9.0505 | Accuracy: 0.9925\n",
      "Task 4 | Backbone 10 | Epoch 18/20 | Loss: 8.6263 | Accuracy: 0.9930\n",
      "Task 4 | Backbone 10 | Epoch 19/20 | Loss: 8.3357 | Accuracy: 0.9934\n",
      "Task 4 | Backbone 10 | Epoch 20/20 | Loss: 8.0974 | Accuracy: 0.9935\n",
      "Successfully applied backbone weights for Text task.\n",
      "Task 5 | Backbone 4 | Epoch 1/20 | Loss: 211.7096 | Accuracy: 0.4984\n",
      "Task 5 | Backbone 4 | Epoch 2/20 | Loss: 150.5278 | Accuracy: 0.5925\n",
      "Task 5 | Backbone 4 | Epoch 3/20 | Loss: 138.3215 | Accuracy: 0.6415\n",
      "Task 5 | Backbone 4 | Epoch 4/20 | Loss: 129.7561 | Accuracy: 0.6682\n",
      "Task 5 | Backbone 4 | Epoch 5/20 | Loss: 123.6898 | Accuracy: 0.6918\n",
      "Task 5 | Backbone 4 | Epoch 6/20 | Loss: 118.3568 | Accuracy: 0.7136\n",
      "Task 5 | Backbone 4 | Epoch 7/20 | Loss: 113.6256 | Accuracy: 0.7227\n",
      "Task 5 | Backbone 4 | Epoch 8/20 | Loss: 108.9281 | Accuracy: 0.7400\n",
      "Task 5 | Backbone 4 | Epoch 9/20 | Loss: 104.5513 | Accuracy: 0.7451\n",
      "Task 5 | Backbone 4 | Epoch 10/20 | Loss: 101.3593 | Accuracy: 0.7580\n",
      "Task 5 | Backbone 4 | Epoch 11/20 | Loss: 97.5472 | Accuracy: 0.7724\n",
      "Task 5 | Backbone 4 | Epoch 12/20 | Loss: 94.7608 | Accuracy: 0.7795\n",
      "Task 5 | Backbone 4 | Epoch 13/20 | Loss: 91.3841 | Accuracy: 0.7844\n",
      "Task 5 | Backbone 4 | Epoch 14/20 | Loss: 88.7328 | Accuracy: 0.7940\n",
      "Task 5 | Backbone 4 | Epoch 15/20 | Loss: 86.2133 | Accuracy: 0.8035\n",
      "Task 5 | Backbone 4 | Epoch 16/20 | Loss: 83.7245 | Accuracy: 0.8091\n",
      "Task 5 | Backbone 4 | Epoch 17/20 | Loss: 80.9297 | Accuracy: 0.8211\n",
      "Task 5 | Backbone 4 | Epoch 18/20 | Loss: 79.5834 | Accuracy: 0.8224\n",
      "Task 5 | Backbone 4 | Epoch 19/20 | Loss: 76.6364 | Accuracy: 0.8284\n",
      "Task 5 | Backbone 4 | Epoch 20/20 | Loss: 74.2855 | Accuracy: 0.8313\n",
      "Successfully applied backbone weights for Image task.\n",
      "Task 6 | Backbone 10 | Epoch 1/20 | Loss: 204.5017 | Accuracy: 0.3302\n",
      "Task 6 | Backbone 10 | Epoch 2/20 | Loss: 201.2323 | Accuracy: 0.3448\n",
      "Task 6 | Backbone 10 | Epoch 3/20 | Loss: 197.8592 | Accuracy: 0.3855\n",
      "Task 6 | Backbone 10 | Epoch 4/20 | Loss: 192.6956 | Accuracy: 0.4273\n",
      "Task 6 | Backbone 10 | Epoch 5/20 | Loss: 193.5701 | Accuracy: 0.4268\n",
      "Task 6 | Backbone 10 | Epoch 6/20 | Loss: 193.4809 | Accuracy: 0.4251\n",
      "Task 6 | Backbone 10 | Epoch 7/20 | Loss: 192.1673 | Accuracy: 0.4251\n",
      "Task 6 | Backbone 10 | Epoch 8/20 | Loss: 192.2440 | Accuracy: 0.4344\n",
      "Task 6 | Backbone 10 | Epoch 9/20 | Loss: 193.1346 | Accuracy: 0.4268\n",
      "Task 6 | Backbone 10 | Epoch 10/20 | Loss: 192.0601 | Accuracy: 0.4273\n",
      "Task 6 | Backbone 10 | Epoch 11/20 | Loss: 192.2296 | Accuracy: 0.4275\n",
      "Task 6 | Backbone 10 | Epoch 12/20 | Loss: 192.4476 | Accuracy: 0.4297\n",
      "Task 6 | Backbone 10 | Epoch 13/20 | Loss: 192.3935 | Accuracy: 0.4344\n",
      "Task 6 | Backbone 10 | Epoch 14/20 | Loss: 191.6812 | Accuracy: 0.4263\n",
      "Task 6 | Backbone 10 | Epoch 15/20 | Loss: 191.9678 | Accuracy: 0.4323\n",
      "Task 6 | Backbone 10 | Epoch 16/20 | Loss: 192.1425 | Accuracy: 0.4323\n",
      "Task 6 | Backbone 10 | Epoch 17/20 | Loss: 191.7236 | Accuracy: 0.4304\n",
      "Task 6 | Backbone 10 | Epoch 18/20 | Loss: 191.6934 | Accuracy: 0.4301\n",
      "Task 6 | Backbone 10 | Epoch 19/20 | Loss: 191.4422 | Accuracy: 0.4321\n",
      "Task 6 | Backbone 10 | Epoch 20/20 | Loss: 191.7249 | Accuracy: 0.4316\n",
      "Successfully applied backbone weights for Text task.\n",
      "Task 7 | Backbone 0 | Epoch 1/20 | Loss: 198.8511 | Accuracy: 0.3867\n",
      "Task 7 | Backbone 0 | Epoch 2/20 | Loss: 180.9970 | Accuracy: 0.4626\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading pruned models and validation loaders...\")\n",
    "    \n",
    "    pruned_text_models, pruned_image_models = load_pruned_models(pruned_model_dir, \"/home/youlee/perceiver/perceiver/model\")\n",
    "    pruned_models = pruned_text_models + pruned_image_models\n",
    "\n",
    "    print(\"Starting Cosine-based Knowledge Transfer...\")\n",
    "    for task_id, backbone_id in cosine_task_to_backbone.items():\n",
    "        backbone_modality = cosine_results.loc[cosine_results[\"Task_ID\"] == task_id, \"Target_Modality\"].values[0]\n",
    "        task_modality = cosine_results.loc[cosine_results[\"Task_ID\"] == task_id, \"Task_Modality\"].values[0]\n",
    "        knowledge_transfer(task_id, backbone_id, backbone_modality, task_modality, \n",
    "                           pruned_models, pruned_models, train_loaders, criterion, device)\n",
    "\n",
    "    print(\"Starting Euclidean-based Knowledge Transfer...\")\n",
    "    for task_id, backbone_id in euclidean_task_to_backbone.items():\n",
    "        backbone_modality = euclidean_results.loc[euclidean_results[\"Task_ID\"] == task_id, \"Target_Modality\"].values[0]\n",
    "        task_modality = euclidean_results.loc[euclidean_results[\"Task_ID\"] == task_id, \"Task_Modality\"].values[0]\n",
    "        knowledge_transfer(task_id, backbone_id, backbone_modality, task_modality, \n",
    "                           pruned_models, pruned_models, train_loaders, criterion, device)\n",
    "\n",
    "    print(\"Evaluating final results...\")\n",
    "    evaluate_transfer(pruned_models, valid_loaders, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the file. Keys in the file:\n",
      "dict_keys(['model_state_dict', 'masks'])\n",
      "Keys in model_state_dict:\n",
      "['model.embedding.weight', 'model.perceiver.latents', 'model.perceiver.input_projection.weight', 'model.perceiver.input_projection.bias', 'model.perceiver.blocks.0.cross_attn.in_proj_weight', 'model.perceiver.blocks.0.cross_attn.in_proj_bias', 'model.perceiver.blocks.0.cross_attn.out_proj.weight', 'model.perceiver.blocks.0.cross_attn.out_proj.bias', 'model.perceiver.blocks.0.cross_ln.weight', 'model.perceiver.blocks.0.cross_ln.bias']\n",
      "Keys in masks:\n",
      "['text_task_1']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"/home/youlee/perceiver/perceiver/checkpoints_pruned/text_model_1_pruned.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    print(\"Successfully loaded the file. Keys in the file:\")\n",
    "    print(model_data.keys())\n",
    "except Exception as e:\n",
    "    print(f\"Error while opening the file: {e}\")\n",
    "\n",
    "\n",
    "# model_state_dict 내부 키 확인\n",
    "state_dict = model_data[\"model_state_dict\"]\n",
    "print(\"Keys in model_state_dict:\")\n",
    "print(list(state_dict.keys())[:10])  # 첫 10개의 키\n",
    "\n",
    "# masks 내부 키 확인\n",
    "masks = model_data[\"masks\"]\n",
    "print(\"Keys in masks:\")\n",
    "print(list(masks.keys())[:10])  # 첫 10개의 키\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the file. Keys in the file:\n",
      "dict_keys(['model_state_dict', 'masks'])\n",
      "Keys in model_state_dict:\n",
      "['model.latents', 'model.input_projection.weight', 'model.input_projection.bias', 'model.blocks.0.cross_attn.in_proj_weight', 'model.blocks.0.cross_attn.in_proj_bias', 'model.blocks.0.cross_attn.out_proj.weight', 'model.blocks.0.cross_attn.out_proj.bias', 'model.blocks.0.cross_ln.weight', 'model.blocks.0.cross_ln.bias', 'model.blocks.0.self_attn_layers.0.self_attn.in_proj_weight']\n",
      "Keys in masks:\n",
      "['image_task_1']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"/home/youlee/perceiver/perceiver/checkpoints_pruned/image_model_1_pruned.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    print(\"Successfully loaded the file. Keys in the file:\")\n",
    "    print(model_data.keys())\n",
    "except Exception as e:\n",
    "    print(f\"Error while opening the file: {e}\")\n",
    "\n",
    "\n",
    "# model_state_dict 내부 키 확인\n",
    "state_dict = model_data[\"model_state_dict\"]\n",
    "print(\"Keys in model_state_dict:\")\n",
    "print(list(state_dict.keys())[:10])  # 첫 10개의 키\n",
    "\n",
    "# masks 내부 키 확인\n",
    "masks = model_data[\"masks\"]\n",
    "print(\"Keys in masks:\")\n",
    "print(list(masks.keys())[:10])  # 첫 10개의 키"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
