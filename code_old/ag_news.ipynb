{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. 데이터셋 로드 및 전처리\n",
    "dataset = load_dataset('ag_news')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 2. 데이터셋 및 데이터로더 준비\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attention_mask = encodings['attention_mask']\n",
    "        self.labels = encodings['label']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = AGNewsDataset(encoded_dataset['train'])\n",
    "test_dataset = AGNewsDataset(encoded_dataset['test'])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/perceiver/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델 정의 및 인스턴스화\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.key_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.query_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.value_proj = nn.Linear(d_in, d_out_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # attention 확률로 변환\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        keys = self.key_proj(x)\n",
    "        queries = self.query_proj(latent)\n",
    "        values = self.value_proj(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        attended_values = torch.matmul(attention_probs, values)\n",
    "        return attended_values\n",
    "\n",
    "class LatentTransformer(nn.Module):\n",
    "    def __init__(self, latent_dim, num_heads, num_layers, embed_dim):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        latent = latent.permute(1,0,2)  # Transformer는 (seq_len, batch_size, latent_dim) 형식으로 데이터 받음.\n",
    "        latent = self.transformer(latent)\n",
    "        return latent.permute(1,0,2)    # 다시 (batch_size, latent_len, latent_dim) 형식으로 변환\n",
    "\n",
    "class Averaging(nn.Module):\n",
    "    def forward(self, latent):\n",
    "        return latent.mean(dim=1)   # latent vector를 평균내서 최종 logits 계산\n",
    "\n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, latent_dim, num_heads, num_layers, num_classes):\n",
    "        super(Perceiver, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # 임베딩 레이어 추가\n",
    "        self.input_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, latent_dim, embed_dim))\n",
    "\n",
    "        self.cross_attention = CrossAttention(d_in=embed_dim, d_out_kq=embed_dim, d_out_v=embed_dim)\n",
    "        self.latent_transformer = LatentTransformer(latent_dim=latent_dim, num_heads=num_heads, \n",
    "                                                    num_layers=num_layers, embed_dim=embed_dim)\n",
    "        \n",
    "        self.averaging = Averaging()\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)  # (batch_size, seq_length, embed_dim)\n",
    "        x = self.input_proj(x)  # 동일한 임베딩 차원 유지\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        latent = self.latents.repeat(batch_size, 1, 1)    # (batch_size, latent_dim, embed_dim)\n",
    "        latent = self.cross_attention(x, latent)          # Cross Attention\n",
    "        latent = self.latent_transformer(latent)          # Transformer\n",
    "        latent_avg = self.averaging(latent)               # Averaging\n",
    "        logits = self.classifier(latent_avg)             # 최종 분류\n",
    "        return logits\n",
    "\n",
    "# 모델 인스턴스화\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "LATENT_DIM = 64\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "NUM_CLASSES = 4  # AG News는 4개의 클래스\n",
    "\n",
    "model = Perceiver(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, latent_dim=LATENT_DIM,\n",
    "                 num_heads=NUM_HEADS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES)\n",
    "\n",
    "# 4. 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "  Train Loss: 0.5646, Train Acc: 0.7868\n",
      "  Test Loss: 0.4011, Test Acc: 0.8553\n",
      "Epoch 2/5:\n",
      "  Train Loss: 0.3683, Train Acc: 0.8708\n",
      "  Test Loss: 0.3545, Test Acc: 0.8745\n",
      "Epoch 3/5:\n",
      "  Train Loss: 0.3145, Train Acc: 0.8890\n",
      "  Test Loss: 0.3119, Test Acc: 0.8939\n",
      "Epoch 4/5:\n",
      "  Train Loss: 0.2779, Train Acc: 0.9023\n",
      "  Test Loss: 0.3005, Test Acc: 0.8964\n",
      "Epoch 5/5:\n",
      "  Train Loss: 0.2498, Train Acc: 0.9121\n",
      "  Test Loss: 0.3014, Test Acc: 0.8976\n"
     ]
    }
   ],
   "source": [
    "# 5. 훈련 루프 구현\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = eval_epoch(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1727   59   66   48]\n",
      " [  27 1849    9   15]\n",
      " [  85   48 1572  195]\n",
      " [  75   46  105 1674]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.90      0.91      0.91      1900\n",
      "      Sports       0.92      0.97      0.95      1900\n",
      "    Business       0.90      0.83      0.86      1900\n",
      "    Sci/Tech       0.87      0.88      0.87      1900\n",
      "\n",
      "    accuracy                           0.90      7600\n",
      "   macro avg       0.90      0.90      0.90      7600\n",
      "weighted avg       0.90      0.90      0.90      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def detailed_evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=dataset['train'].features['label'].names)\n",
    "    print('Classification Report:')\n",
    "    print(report)\n",
    "\n",
    "detailed_evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 계산\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "    # 분류 리포트 출력\n",
    "    report = classification_report(all_labels, all_preds, target_names=dataset['train'].features['label'].names)\n",
    "    print('Classification Report:')\n",
    "    print(report)\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=dataset['train'].features['label'].names,\n",
    "                yticklabels=dataset['train'].features['label'].names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    detailed_evaluate(model, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
