{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import copy, math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from models.shared_perceiver import crop, patchify, get_patch_coords, ImageDataset, PerceiverBlock, Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, device, scheduler=None):\n",
    "    best_model = None \n",
    "    best_val_score = 0\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    start = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # GPU로 옮기기\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        accuracy = evaluate_model(model, valid_loader, device=device, log_results=False)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # Scheduler step 추가\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.2f}%\")\n",
    "        if accuracy > best_val_score:\n",
    "            best_val_score = accuracy\n",
    "            best_model_state = model.state_dict()  # 모델 상태 저장\n",
    "            print(f\"New best model found at epoch {epoch+1} with accuracy: {best_val_score:.2f}%\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    hour = (end-start) // 3600\n",
    "    min = ((end-start) % 3600) // 60\n",
    "    sec = int((end-start) % 60)\n",
    "    print(f\"Total Train time: {hour}h {min}m {sec}s\")\n",
    "\n",
    "    return train_losses, val_accuracies, best_model_state\n",
    "\n",
    "def evaluate_model(model, data_loader, device, log_results=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        start = time.perf_counter()\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        end = time.perf_counter()\n",
    "        hour = (end-start) // 3600\n",
    "        min = ((end-start) % 3600) // 60\n",
    "        sec = (end-start) % 60\n",
    "        print(f\"Elapsed time on CPU: {hour}h {min}m {sec}s\")\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    if log_results:\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/youlee/n24news/n24news/image'\n",
    "crop_size = 0\n",
    "patch_size = 16\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "group_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Opinion',\n",
       " 'Art & Design',\n",
       " 'Television',\n",
       " 'Music',\n",
       " 'Travel',\n",
       " 'Real Estate',\n",
       " 'Books',\n",
       " 'Theater',\n",
       " 'Health',\n",
       " 'Sports',\n",
       " 'Science',\n",
       " 'Food',\n",
       " 'Fashion & Style',\n",
       " 'Movies',\n",
       " 'Technology',\n",
       " 'Dance',\n",
       " 'Media',\n",
       " 'Style']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random.shuffle(target_classes)\n",
    "target_classes = [ # 임의로 순서지정\n",
    "    \"Opinion\", \"Art & Design\", \"Television\",\n",
    "    \"Music\", \"Travel\", \"Real Estate\",\n",
    "    \"Books\", \"Theater\", \"Health\",\n",
    "    \"Sports\", \"Science\", \"Food\",\n",
    "    \"Fashion & Style\", \"Movies\", \"Technology\",\n",
    "    \"Dance\", \"Media\", \"Style\"\n",
    "]\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = '/home/Minju/Perceiver/model'\n",
    "loader_path = '/home/Minju/Perceiver/loader'\n",
    "group_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 1 시작\n",
      "Selected Feature: ['Opinion', 'Art & Design', 'Television']\n",
      "Unique numeric labels: [0 1 2]\n",
      "Number of unique numeric labels: 3\n",
      "Label distribution (index: count): Counter({np.int64(1): 2437, np.int64(0): 2431, np.int64(2): 2419})\n",
      "train: 5829, valid: 1458\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.911117488984019s\n",
      "Epoch 1/20, Loss: 1.0793, Val Accuracy: 40.33%\n",
      "New best model found at epoch 1 with accuracy: 40.33%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.82281571207568s\n",
      "Epoch 2/20, Loss: 1.0528, Val Accuracy: 40.95%\n",
      "New best model found at epoch 2 with accuracy: 40.95%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.944319583941251s\n",
      "Epoch 3/20, Loss: 1.0527, Val Accuracy: 39.85%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.983476229943335s\n",
      "Epoch 4/20, Loss: 1.0472, Val Accuracy: 41.77%\n",
      "New best model found at epoch 4 with accuracy: 41.77%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.977669610874727s\n",
      "Epoch 5/20, Loss: 1.0433, Val Accuracy: 46.02%\n",
      "New best model found at epoch 5 with accuracy: 46.02%\n",
      "Elapsed time on CPU: 0.0h 0.0m 9.077643975848332s\n",
      "Epoch 6/20, Loss: 1.0153, Val Accuracy: 48.22%\n",
      "New best model found at epoch 6 with accuracy: 48.22%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.949166669044644s\n",
      "Epoch 7/20, Loss: 0.8215, Val Accuracy: 63.99%\n",
      "New best model found at epoch 7 with accuracy: 63.99%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.81694730790332s\n",
      "Epoch 8/20, Loss: 0.7528, Val Accuracy: 64.40%\n",
      "New best model found at epoch 8 with accuracy: 64.40%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.926798377884552s\n",
      "Epoch 9/20, Loss: 0.7333, Val Accuracy: 66.80%\n",
      "New best model found at epoch 9 with accuracy: 66.80%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.881261196918786s\n",
      "Epoch 10/20, Loss: 0.7155, Val Accuracy: 69.34%\n",
      "New best model found at epoch 10 with accuracy: 69.34%\n",
      "Elapsed time on CPU: 0.0h 0.0m 8.848037710878998s\n",
      "Epoch 11/20, Loss: 0.6761, Val Accuracy: 68.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# Learning rate decay 추가\u001b[39;00m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 46\u001b[0m train_losses, val_accuracies, best_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# best_model_state를 모델에 로드\u001b[39;00m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_state)\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, epochs, device, scheduler)\u001b[0m\n\u001b[1;32m     23\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     24\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[0;32m---> 26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m val_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Scheduler step 추가\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader, device, log_results)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     52\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     54\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/home/Minju/Perceiver/code/models/shared_perceiver.py:131\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    130\u001b[0m     image_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m--> 131\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m    134\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)  \u001b[38;5;66;03m# (3, 224, 224)\u001b[39;00m\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/PIL/Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0, len(target_classes), group_class):  \n",
    "    print(f\"실험 {i//group_class + 1} 시작\")\n",
    "    selected_classes = target_classes[i:i+group_class]\n",
    "    print(f\"Selected Feature: {selected_classes}\")\n",
    "\n",
    "    filtered_dataset = ImageDataset(root_dir=data_dir, \n",
    "                                    transform=transform, \n",
    "                                    crop_size=crop_size, \n",
    "                                    patch_size=patch_size,\n",
    "                                    selected_classes=selected_classes)\n",
    "    all_labels = [label_idx for (_, label_idx) in filtered_dataset.data]\n",
    "\n",
    "    # 1) 유니크 라벨과 개수\n",
    "    unique_label_ids = np.unique(all_labels)\n",
    "    print(\"Unique numeric labels:\", unique_label_ids)\n",
    "    print(\"Number of unique numeric labels:\", len(unique_label_ids))\n",
    "\n",
    "    # 2) 라벨별 개수 (분포)\n",
    "    label_counts = Counter(all_labels)\n",
    "    print(\"Label distribution (index: count):\", label_counts)\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    train_size = int(len(filtered_dataset) * train_ratio)\n",
    "    valid_size = len(filtered_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(filtered_dataset, [train_size, valid_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"train: {train_size}, valid: {valid_size}\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    NUM_CLASSES = len(filtered_dataset.label_encoder.classes_)\n",
    "    model = Perceiver(input_dim=(patch_size**2) * 3 + 2,\n",
    "                        latent_dim=128, \n",
    "                        latent_size=64, \n",
    "                        num_classes=NUM_CLASSES, \n",
    "                        num_blocks=4, \n",
    "                        self_attn_layers_per_block=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Learning rate decay 추가\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    train_losses, val_accuracies, best_model_state = train_model(\n",
    "        model, train_loader, valid_loader,\n",
    "        criterion, optimizer, epochs,\n",
    "        device=device,\n",
    "        scheduler=scheduler  \n",
    "    )\n",
    "    \n",
    "    # best_model_state를 모델에 로드\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    final_acc = evaluate_model(model, valid_loader, device=device, log_results=True)\n",
    "    end = time.perf_counter()\n",
    "    hour = (end-start) // 3600\n",
    "    min = ((end-start) % 3600) // 60\n",
    "    sec = int((end-start) % 60)\n",
    "    print(f\"Train time: {hour}h {min}m {sec}s\")\n",
    "    print(f\"Final Validation Accuracy: {final_acc:.2f}%\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    \n",
    "    torch.save(model, f'{output_path}/image_model_{i//group_class+1}.pkl')\n",
    "\n",
    "    val_loader_save_path = f\"{loader_path}/image_val_loader_{i//group_class+1}.pkl\"\n",
    "    with open(val_loader_save_path, 'wb') as f:\n",
    "        pickle.dump(valid_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clones(module, N):\n",
    "#     return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attention(query, key, value, mask=None, dropout=None):\n",
    "#   d_k = query.size(-1)\n",
    "#   scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "#   if mask is not None:\n",
    "#     scores =scores.masked_fill(mask==0, -1e9)\n",
    "#   p_attn = scores.softmax(dim=-1)\n",
    "#   if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "#   return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSharableMultiheadAttention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(SharableMultiheadAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# class SharableMultiheadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "#         super(SharableMultiheadAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.d_k = embed_dim // num_heads\n",
    "#         self.linears = clones(nn.Linear(embed_dim))\n",
    "#         self.attn = None\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "#         self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.scale = self.d_k  ** -0.5\n",
    "\n",
    "#     def forward(self, query, key, value, mask=None):\n",
    "#         if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)\n",
    "#         batch_size, seq_length, embed_dim = query.shape\n",
    "#         qkv = self.qkv_proj(torch.cat[query, key, value], dim=1)\n",
    "#         qkv = qkv.view(batch_size, seq_length, 3, self.num_heads, self.head_dim)\n",
    "#         q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "#         self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "#         x = (\n",
    "#             x.transpose(1,2)\n",
    "#             .contiguous()\n",
    "#             .view(batch_size, -1, self.num_heads * self.d_k)\n",
    "#         )\n",
    "#         del query\n",
    "#         del key\n",
    "#         del value\n",
    "#         return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_THRESHOLD = 5e-3\n",
    "# class SharableMultiheadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, bias=True, \n",
    "#                  dropout=0., mask_init='1s', mask_scale=1e-2,\n",
    "#                  threshold_fn='binarizer', threshold=None):\n",
    "#         super(SharableMultiheadAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.dropout = dropout\n",
    "#         self.mask_init= mask_init\n",
    "#         self.mask_scale = mask_scale\n",
    "\n",
    "#         if threshold is not None:\n",
    "#             threshold = DEFAULT_THRESHOLD\n",
    "#         self.info = {\n",
    "#             'threshold_fn': threshold_fn,\n",
    "#             'threshold': threshold,\n",
    "#         }\n",
    "\n",
    "#         self.weight = Parameter(torch.Tensor()) # 이 부분 맞춰주어야함함\n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.Tensor())   # 이 부분 맞춰주어야함\n",
    "#         else:\n",
    "#             self.register_parameter()\n",
    "#         self.piggymask = None\n",
    "\n",
    "#         if threshold_fn == 'binarizer':\n",
    "#             self.threshold_fn = Binarizer.apply\n",
    "#         elif threshold_fn == 'tenarizer':\n",
    "#             self.threshold_fn = Tenarizer(threshold=threshold)\n",
    "\n",
    "#         self.q_proj = SharableLinear(embed_dim, embed_dim)\n",
    "#         self.k_proj = SharableLinear(embed_dim, embed_dim)\n",
    "#         self.v_proj = SharableLinear(embed_dim, embed_dim)\n",
    "#         self.out_proj = SharableLinear(embed_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, query, key, value, attn_mask=None):\n",
    "#         Q = self.q_proj(query)\n",
    "#         K = self.k_proj(key)\n",
    "#         V = self.v_proj(value)\n",
    "\n",
    "#         if self.piggymask is not None:\n",
    "#             mask_thresholded = self.threshold_fn(self.piggymask, self.info['threshold'])\n",
    "#             weight = mask_thresholded * self.weight\n",
    "#         else:\n",
    "#             weight = self.weight ########일단 여기까지 따라침.\n",
    "\n",
    "#         attn_output, _ = F.multi_head_attention_forward(\n",
    "#             Q, K, V,\n",
    "#             self.embed_dim, self.num_heads,\n",
    "#             None, None, None,  # Scaling 및 Bias 없음\n",
    "#             attn_mask,\n",
    "#             dropout_p=self.dropout,\n",
    "#             out_proj_weight=self.out_proj.weight * self.out_proj.mask,\n",
    "#             out_proj_bias=self.out_proj.bias,\n",
    "#             training=self.training,\n",
    "#             need_weights=False\n",
    "#         )\n",
    "#         return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
