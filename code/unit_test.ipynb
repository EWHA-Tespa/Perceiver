{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import copy, math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from models.shared_perceiver import crop, patchify, get_patch_coords, ImageDataset, PerceiverBlock, Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, device, scheduler=None):\n",
    "    best_model = None \n",
    "    best_val_score = 0\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    start = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # GPU로 옮기기\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        accuracy = evaluate_model(model, valid_loader, device=device, log_results=False)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # Scheduler step 추가\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.2f}%\")\n",
    "        if accuracy > best_val_score:\n",
    "            best_val_score = accuracy\n",
    "            best_model_state = model.state_dict()  # 모델 상태 저장\n",
    "            best_model = copy.deepcopy(model) \n",
    "            print(f\"New best model found at epoch {epoch+1} with accuracy: {best_val_score:.2f}%\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    hour = (end-start) // 3600\n",
    "    min = ((end-start) % 3600) // 60\n",
    "    sec = int((end-start) % 60)\n",
    "    print(f\"Total Train time: {hour}h {min}m {sec}s\")\n",
    "\n",
    "    return train_losses, val_accuracies, best_model\n",
    "\n",
    "def evaluate_model(model, data_loader, device, log_results=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        start = time.perf_counter()\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        end = time.perf_counter()\n",
    "        hour = (end-start) // 3600\n",
    "        min = ((end-start) % 3600) // 60\n",
    "        sec = (end-start) % 60\n",
    "        print(f\"Elapsed time on CPU: {hour}h {min}m {sec}s\")\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    if log_results:\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/youlee/n24news/n24news/image'\n",
    "crop_size = 0\n",
    "patch_size = 16\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "group_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Opinion',\n",
       " 'Art & Design',\n",
       " 'Television',\n",
       " 'Music',\n",
       " 'Travel',\n",
       " 'Real Estate',\n",
       " 'Books',\n",
       " 'Theater',\n",
       " 'Health',\n",
       " 'Sports',\n",
       " 'Science',\n",
       " 'Food',\n",
       " 'Fashion & Style',\n",
       " 'Movies',\n",
       " 'Technology',\n",
       " 'Dance',\n",
       " 'Media',\n",
       " 'Style']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random.shuffle(target_classes)\n",
    "target_classes = [ # 임의로 순서지정\n",
    "    \"Opinion\", \"Art & Design\", \"Television\",\n",
    "    \"Music\", \"Travel\", \"Real Estate\",\n",
    "    \"Books\", \"Theater\", \"Health\",\n",
    "    \"Sports\", \"Science\", \"Food\",\n",
    "    \"Fashion & Style\", \"Movies\", \"Technology\",\n",
    "    \"Dance\", \"Media\", \"Style\"\n",
    "]\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = '/home/Minju/Perceiver/shared_layer_model'\n",
    "loader_path = '/home/Minju/Perceiver/loader'\n",
    "group_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 1 시작\n",
      "Selected Feature: ['Opinion', 'Art & Design', 'Television']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique numeric labels: [0 1 2]\n",
      "Number of unique numeric labels: 3\n",
      "Label distribution (index: count): Counter({np.int64(1): 2437, np.int64(0): 2431, np.int64(2): 2419})\n",
      "train: 5829, valid: 1458\n",
      "Elapsed time on CPU: 0.0h 0.0m 9.01374229695648s\n",
      "Epoch 1/20, Loss: 1.0643, Val Accuracy: 42.87%\n",
      "New best model found at epoch 1 with accuracy: 42.87%\n",
      "Elapsed time on CPU: 0.0h 0.0m 9.319800229975954s\n",
      "Epoch 2/20, Loss: 1.0514, Val Accuracy: 43.00%\n",
      "New best model found at epoch 2 with accuracy: 43.00%\n",
      "Elapsed time on CPU: 0.0h 0.0m 9.251910794060677s\n",
      "Epoch 3/20, Loss: 1.0481, Val Accuracy: 42.80%\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(target_classes), group_class):  \n",
    "    print(f\"실험 {i//group_class + 1} 시작\")\n",
    "    selected_classes = target_classes[i:i+group_class]\n",
    "    print(f\"Selected Feature: {selected_classes}\")\n",
    "\n",
    "    filtered_dataset = ImageDataset(root_dir=data_dir, \n",
    "                                    transform=transform, \n",
    "                                    crop_size=crop_size, \n",
    "                                    patch_size=patch_size,\n",
    "                                    selected_classes=selected_classes)\n",
    "    all_labels = [label_idx for (_, label_idx) in filtered_dataset.data]\n",
    "\n",
    "    # 1) 유니크 라벨과 개수\n",
    "    unique_label_ids = np.unique(all_labels)\n",
    "    print(\"Unique numeric labels:\", unique_label_ids)\n",
    "    print(\"Number of unique numeric labels:\", len(unique_label_ids))\n",
    "\n",
    "    # 2) 라벨별 개수 (분포)\n",
    "    label_counts = Counter(all_labels)\n",
    "    print(\"Label distribution (index: count):\", label_counts)\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    train_size = int(len(filtered_dataset) * train_ratio)\n",
    "    valid_size = len(filtered_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(filtered_dataset, [train_size, valid_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"train: {train_size}, valid: {valid_size}\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    NUM_CLASSES = len(filtered_dataset.label_encoder.classes_)\n",
    "    model = Perceiver(input_dim=(patch_size**2) * 3 + 2,\n",
    "                        latent_dim=128, \n",
    "                        latent_size=64, \n",
    "                        num_classes=NUM_CLASSES, \n",
    "                        num_blocks=4, \n",
    "                        self_attn_layers_per_block=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Learning rate decay 추가\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    train_losses, val_accuracies, best_model = train_model(\n",
    "        model, train_loader, valid_loader,\n",
    "        criterion, optimizer, epochs,\n",
    "        device=device,\n",
    "        scheduler=scheduler  \n",
    "    )\n",
    "    \n",
    "    final_acc = evaluate_model(best_model, valid_loader, device=device, log_results=True)\n",
    "    end = time.perf_counter()\n",
    "    hour = (end-start) // 3600\n",
    "    min = ((end-start) % 3600) // 60\n",
    "    sec = int((end-start) % 60)\n",
    "    print(f\"Train time: {hour}h {min}m {sec}s\")\n",
    "    print(f\"Final Validation Accuracy: {final_acc:.2f}%\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    \n",
    "    torch.save(model, f'{output_path}/image_model_{i//group_class+1}.pkl')\n",
    "\n",
    "    # val_loader_save_path = f\"{loader_path}/image_val_loader_{i//group_class+1}.pkl\"\n",
    "    # with open(val_loader_save_path, 'wb') as f:\n",
    "    #     pickle.dump(valid_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clones(module, N):\n",
    "#     return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attention(query, key, value, mask=None, dropout=None):\n",
    "#   d_k = query.size(-1)\n",
    "#   scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "#   if mask is not None:\n",
    "#     scores =scores.masked_fill(mask==0, -1e9)\n",
    "#   p_attn = scores.softmax(dim=-1)\n",
    "#   if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "#   return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SharableMultiheadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "#         super(SharableMultiheadAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.d_k = embed_dim // num_heads\n",
    "#         self.linears = clones(nn.Linear(embed_dim))\n",
    "#         self.attn = None\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "#         self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.scale = self.d_k  ** -0.5\n",
    "\n",
    "#     def forward(self, query, key, value, mask=None):\n",
    "#         if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)\n",
    "#         batch_size, seq_length, embed_dim = query.shape\n",
    "#         qkv = self.qkv_proj(torch.cat[query, key, value], dim=1)\n",
    "#         qkv = qkv.view(batch_size, seq_length, 3, self.num_heads, self.head_dim)\n",
    "#         q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "#         self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "#         x = (\n",
    "#             x.transpose(1,2)\n",
    "#             .contiguous()\n",
    "#             .view(batch_size, -1, self.num_heads * self.d_k)\n",
    "#         )\n",
    "#         del query\n",
    "#         del key\n",
    "#         del value\n",
    "#         return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_THRESHOLD = 5e-3\n",
    "# class SharableMultiheadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, bias=True, \n",
    "#                  dropout=0., mask_init='1s', mask_scale=1e-2,\n",
    "#                  threshold_fn='binarizer', threshold=None):\n",
    "#         super(SharableMultiheadAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.dropout = dropout\n",
    "#         self.mask_init= mask_init\n",
    "#         self.mask_scale = mask_scale\n",
    "\n",
    "#         if threshold is not None:\n",
    "#             threshold = DEFAULT_THRESHOLD\n",
    "#         self.info = {\n",
    "#             'threshold_fn': threshold_fn,\n",
    "#             'threshold': threshold,\n",
    "#         }\n",
    "\n",
    "#         self.weight = Parameter(torch.Tensor()) # 이 부분 맞춰주어야함함\n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.Tensor())   # 이 부분 맞춰주어야함\n",
    "#         else:\n",
    "#             self.register_parameter()\n",
    "#         self.piggymask = None\n",
    "\n",
    "#         if threshold_fn == 'binarizer':\n",
    "#             self.threshold_fn = Binarizer.apply\n",
    "#         elif threshold_fn == 'tenarizer':\n",
    "#             self.threshold_fn = Tenarizer(threshold=threshold)\n",
    "\n",
    "#         self.q_proj = SharableLinear(embed_dim, embed_dim)\n",
    "#         self.k_proj = SharableLinear(embed_dim, embed_dim)\n",
    "#         self.v_proj = SharableLinear(embed_dim, embed_dim)\n",
    "#         self.out_proj = SharableLinear(embed_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, query, key, value, attn_mask=None):\n",
    "#         Q = self.q_proj(query)\n",
    "#         K = self.k_proj(key)\n",
    "#         V = self.v_proj(value)\n",
    "\n",
    "#         if self.piggymask is not None:\n",
    "#             mask_thresholded = self.threshold_fn(self.piggymask, self.info['threshold'])\n",
    "#             weight = mask_thresholded * self.weight\n",
    "#         else:\n",
    "#             weight = self.weight ########일단 여기까지 따라침.\n",
    "\n",
    "#         attn_output, _ = F.multi_head_attention_forward(\n",
    "#             Q, K, V,\n",
    "#             self.embed_dim, self.num_heads,\n",
    "#             None, None, None,  # Scaling 및 Bias 없음\n",
    "#             attn_mask,\n",
    "#             dropout_p=self.dropout,\n",
    "#             out_proj_weight=self.out_proj.weight * self.out_proj.mask,\n",
    "#             out_proj_bias=self.out_proj.bias,\n",
    "#             training=self.training,\n",
    "#             need_weights=False\n",
    "#         )\n",
    "#         return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
