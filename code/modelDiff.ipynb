{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "from scipy import spatial\n",
    "import csv\n",
    "\n",
    "from perceiver import crop, patchify, get_patch_coords, ImageDataset, PerceiverBlock, Perceiver, CustomDataset, CombinedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/youlee/perceiver/perceiver/model/'\n",
    "loader_dir = '/home/youlee/perceiver/perceiver/loader/'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_models = []\n",
    "valid_loaders = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277349/2310590346.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model 1번 불러오기 완료.\n",
      "Text val. loader 0번 불러오기 완료.\n",
      "Text model 2번 불러오기 완료.\n",
      "Text val. loader 1번 불러오기 완료.\n",
      "Text model 3번 불러오기 완료.\n",
      "Text val. loader 2번 불러오기 완료.\n",
      "Text model 4번 불러오기 완료.\n",
      "Text val. loader 3번 불러오기 완료.\n",
      "Text model 5번 불러오기 완료.\n",
      "Text val. loader 4번 불러오기 완료.\n",
      "Text model 6번 불러오기 완료.\n",
      "Text val. loader 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range (6):\n",
    "    text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n",
    "    input_models.append(text_model)\n",
    "    print(f\"Text model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "    with open(loader_dir+f'text_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loaders.append(loaded_valid_dataset)\n",
    "    print(f\"Text val. loader {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 0번 불러오기 완료.\n",
      "Image val. loader 0번 불러오기 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277349/520048891.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 1번 불러오기 완료.\n",
      "Image val. loader 1번 불러오기 완료.\n",
      "Image model 2번 불러오기 완료.\n",
      "Image val. loader 2번 불러오기 완료.\n",
      "Image model 3번 불러오기 완료.\n",
      "Image val. loader 3번 불러오기 완료.\n",
      "Image model 4번 불러오기 완료.\n",
      "Image val. loader 4번 불러오기 완료.\n",
      "Image model 5번 불러오기 완료.\n",
      "Image val. loader 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n",
    "    input_models.append(img_model)\n",
    "    print(f\"Image model {i}번 불러오기 완료.\")\n",
    "\n",
    "    with open(loader_dir+f'image_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)\n",
    "    print(f\"Image val. loader {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataloader.DataLoader at 0x79a669f306d0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a661645930>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a661645630>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a661647340>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a7b275b8b0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a660dde9e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a6613e3cd0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a6615efe80>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a660af7b50>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a6609a3dc0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a66046fd00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x79a7b130a290>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loaders  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Valid Loader 1 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 2 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 3 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 4 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 5 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 6 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 7 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 8 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 9 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 10 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 11 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 12 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# valid_loader 1-12까지 첫번째 배치 shape 확인\n",
    "\n",
    "for idx, loader in enumerate(valid_loaders):\n",
    "    print(f\"--- Valid Loader {idx + 1} ---\")\n",
    "    \n",
    "    batch = next(iter(loader))  \n",
    "    \n",
    "    # 배치가 리스트 형식일 경우, inputs과 labels 출력\n",
    "    if isinstance(batch, list):\n",
    "        print(f\"Inputs shape: {batch[0].shape}, Labels shape: {batch[1].shape}\")\n",
    "    elif isinstance(batch, dict):\n",
    "        # 배치가 딕셔너리 형식일 때, 각 key-value 쌍의 shape 출력\n",
    "        print({key: value.shape for key, value in batch.items()})\n",
    "    elif isinstance(batch, tuple):\n",
    "        # 배치가 튜플 형식일 때, Inputs과 Labels의 shape 출력\n",
    "        print(f\"Inputs shape: {batch[0].shape}, Labels shape: {batch[1].shape}\")\n",
    "    else:\n",
    "        print(f\"Unknown batch format: {type(batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Model:\n",
      "Perceiver(\n",
      "  (input_projection): Linear(in_features=770, out_features=128, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x PerceiverBlock(\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (cross_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn_layers): ModuleList(\n",
      "        (0-9): 10 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Text Model:\n",
      "CombinedModel(\n",
      "  (embedding): Embedding(30522, 128)\n",
      "  (perceiver): Perceiver(\n",
      "    (input_projection): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x PerceiverBlock(\n",
      "        (cross_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (cross_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn_layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277349/3364340435.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image_model = torch.load(image_model_path)\n",
      "/tmp/ipykernel_277349/3364340435.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(text_model_path)\n"
     ]
    }
   ],
   "source": [
    "# image, text 모델 확인\n",
    "image_model_path = \"/home/youlee/perceiver/perceiver/model/image_model_1.pkl\"\n",
    "text_model_path = \"/home/youlee/perceiver/perceiver/model/text_model_1.pkl\"\n",
    "\n",
    "image_model = torch.load(image_model_path)\n",
    "print(\"Image Model:\")\n",
    "print(image_model)\n",
    "\n",
    "text_model = torch.load(text_model_path)\n",
    "print(\"\\nText Model:\")\n",
    "print(text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ModelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = -1\n",
    "lr = 0.1\n",
    "batch_size = 32\n",
    "val_batch_size = 100\n",
    "workers = 24\n",
    "weight_decay = 4e-5\n",
    "dataset_name = ''\n",
    "train_path = ''\n",
    "val_path = ''\n",
    "cuda = True\n",
    "seed = 1\n",
    "epochs = 160\n",
    "restore_epoch = 0\n",
    "save_folder = ''\n",
    "load_folder = ''\n",
    "one_shot_prune_perc = 0.5\n",
    "mode = ''\n",
    "logfile = ''\n",
    "initial_from_task = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    'Opinion','Art & Design','Television',\n",
    "    'Music','Travel','Real Estate',\n",
    "    'Books','Theater','Health',\n",
    "    'Sports','Science','Food',\n",
    "    'Fashion & Style','Movies','Technology',\n",
    "    'Dance', 'Media', 'Style'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유사도검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task_id 책정방식: \\\n",
    "0~5 : text modality \\\n",
    "6~11: image modality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: 특정 input data로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDV 계산: Cosine Distance\n",
    "def compute_ddv_cos(\n",
    "    model1, model2,\n",
    "    inputs1, inputs2,\n",
    "    is_model1_text=False,\n",
    "    is_model2_text=False,\n",
    "    device='cuda'\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        # model1\n",
    "        if is_model1_text:\n",
    "            output1 = model1(inputs1)  \n",
    "        else:\n",
    "            output1 = model1(inputs1.float())  \n",
    "\n",
    "        n_pairs = output1.shape[0] // 2\n",
    "        dists1 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output1[i].cpu().numpy()  # GPU -> CPU 변환\n",
    "            yb = output1[i + n_pairs].cpu().numpy()  # GPU -> CPU 변환\n",
    "            dist = spatial.distance.cosine(ya, yb)\n",
    "            dists1.append(dist)\n",
    "\n",
    "        # model2\n",
    "        if is_model2_text:\n",
    "            output2 = model2(inputs2)  \n",
    "        else:\n",
    "            output2 = model2(inputs2.float())  \n",
    "\n",
    "        dists2 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output2[i].cpu().numpy()  # GPU -> CPU 변환\n",
    "            yb = output2[i + n_pairs].cpu().numpy()  # GPU -> CPU 변환\n",
    "            dist = spatial.distance.cosine(ya, yb)\n",
    "            dists2.append(dist)\n",
    "\n",
    "    return np.array(dists1), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDV 계산: Euclidean Distance\n",
    "def compute_ddv_euc(\n",
    "    model1, model2,\n",
    "    inputs1, inputs2,\n",
    "    is_model1_text=False,\n",
    "    is_model2_text=False,\n",
    "    device='cuda'\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        # model1\n",
    "        if is_model1_text:\n",
    "            output1 = model1(inputs1)        \n",
    "        else:\n",
    "            output1 = model1(inputs1.float()) \n",
    "\n",
    "        n_pairs = output1.shape[0] // 2\n",
    "        dists1 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output1[i].cpu().numpy()  # GPU -> CPU 변환\n",
    "            yb = output1[i + n_pairs].cpu().numpy()  # GPU -> CPU 변환\n",
    "            dist = spatial.distance.euclidean(ya, yb)\n",
    "            dists1.append(dist)\n",
    "\n",
    "        # model2\n",
    "        if is_model2_text:\n",
    "            output2 = model2(inputs2)\n",
    "        else:\n",
    "            output2 = model2(inputs2.float())\n",
    "\n",
    "        dists2 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output2[i].cpu().numpy()  # GPU -> CPU 변환\n",
    "            yb = output2[i + n_pairs].cpu().numpy()  # GPU -> CPU 변환\n",
    "            dist = spatial.distance.euclidean(ya, yb)\n",
    "            dists2.append(dist)\n",
    "\n",
    "    return np.array(dists1), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### compute_similarity #####\n",
    "def compute_sim_cos(ddv1, ddv2):\n",
    "    return spatial.distance.cosine(ddv1, ddv2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelDiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddvcc_list = []\n",
    "# ddvec_list = []\n",
    "\n",
    "# for task_id in range(6):\n",
    "#     if task_id == target_id:\n",
    "#         continue\n",
    "\n",
    "#     # batch = next(iter(valid_loaders[target_id]))  \n",
    "#     # print(batch)\n",
    "#     # valid_loader = valid_loaders[target_id]  # valid_loader 가져오기\n",
    "#     # data_iter = iter(valid_loader)  # 반복자 생성\n",
    "#     # batch = next(data_iter)  # 첫 번째 배치 가져오기\n",
    "    \n",
    "#     # print(batch)  \n",
    "    \n",
    "#     # # input_ids를 numpy 또는 tensor로 변환\n",
    "#     # inputs = batch['input_ids'].to(DEVICE)\n",
    "#     if target_id >= 6:   # Text Modality\n",
    "#         batch = next(iter(valid_loaders[target_id]))  \n",
    "#         inputs = batch['input_ids'].to(DEVICE)\n",
    "        \n",
    "#     else:               # Image Modality\n",
    "#         inputs = next(iter(valid_loaders[target_id]))[0]\n",
    "\n",
    "#     ddv1, ddv2 = compute_ddv_cos(input_models[target_id], input_models[task_id], inputs)\n",
    "#     ddv_distance = compute_sim_cos(ddv1, ddv2)\n",
    "#     print('DDV cos-cos [%d => %d] %.5f'%(task_id, target_id, ddv_distance))\n",
    "#     ddvcc_list.append(ddv_distance)\n",
    "\n",
    "#     # DDV-EC\n",
    "#     ddv1, ddv2 = compute_ddv_euc(input_models[target_id], input_models[task_id], inputs)\n",
    "#     ddv_distance = compute_sim_cos(ddv1, ddv2)\n",
    "#     print('DDV euc-cos [%d => %d] %.5f'%(task_id, target_id, ddv_distance))\n",
    "#     ddvec_list.append(ddv_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= TARGET MODEL: 0 =================\n",
      "[task_id 1 => target_id 0] DDV cos-cos: 0.26042, DDV euc-cos: 0.17652\n",
      "[task_id 2 => target_id 0] DDV cos-cos: 0.25366, DDV euc-cos: 0.16226\n",
      "[task_id 3 => target_id 0] DDV cos-cos: 0.29954, DDV euc-cos: 0.25576\n",
      "[task_id 4 => target_id 0] DDV cos-cos: 0.25893, DDV euc-cos: 0.21033\n",
      "[task_id 5 => target_id 0] DDV cos-cos: 0.33834, DDV euc-cos: 0.23479\n",
      "[task_id 6 => target_id 0] DDV cos-cos: 0.41213, DDV euc-cos: 0.26180\n",
      "[task_id 7 => target_id 0] DDV cos-cos: 0.49598, DDV euc-cos: 0.36193\n",
      "[task_id 8 => target_id 0] DDV cos-cos: 0.17219, DDV euc-cos: 0.19028\n",
      "[task_id 9 => target_id 0] DDV cos-cos: 0.32200, DDV euc-cos: 0.27107\n",
      "[task_id 10 => target_id 0] DDV cos-cos: 0.23448, DDV euc-cos: 0.13423\n",
      "[task_id 11 => target_id 0] DDV cos-cos: 0.24710, DDV euc-cos: 0.22079\n",
      "\n",
      "================= TARGET MODEL: 1 =================\n",
      "[task_id 0 => target_id 1] DDV cos-cos: 0.26042, DDV euc-cos: 0.17652\n",
      "[task_id 2 => target_id 1] DDV cos-cos: 0.34104, DDV euc-cos: 0.25814\n",
      "[task_id 3 => target_id 1] DDV cos-cos: 0.27569, DDV euc-cos: 0.21029\n",
      "[task_id 4 => target_id 1] DDV cos-cos: 0.38888, DDV euc-cos: 0.29157\n",
      "[task_id 5 => target_id 1] DDV cos-cos: 0.13538, DDV euc-cos: 0.11349\n",
      "[task_id 6 => target_id 1] DDV cos-cos: 0.40256, DDV euc-cos: 0.22328\n",
      "[task_id 7 => target_id 1] DDV cos-cos: 0.36712, DDV euc-cos: 0.26433\n",
      "[task_id 8 => target_id 1] DDV cos-cos: 0.49146, DDV euc-cos: 0.34326\n",
      "[task_id 9 => target_id 1] DDV cos-cos: 0.20496, DDV euc-cos: 0.19734\n",
      "[task_id 10 => target_id 1] DDV cos-cos: 0.25751, DDV euc-cos: 0.14678\n",
      "[task_id 11 => target_id 1] DDV cos-cos: 0.41128, DDV euc-cos: 0.29351\n",
      "\n",
      "================= TARGET MODEL: 2 =================\n",
      "[task_id 0 => target_id 2] DDV cos-cos: 0.25366, DDV euc-cos: 0.16226\n",
      "[task_id 1 => target_id 2] DDV cos-cos: 0.34104, DDV euc-cos: 0.25814\n",
      "[task_id 3 => target_id 2] DDV cos-cos: 0.31395, DDV euc-cos: 0.22673\n",
      "[task_id 4 => target_id 2] DDV cos-cos: 0.31791, DDV euc-cos: 0.23782\n",
      "[task_id 5 => target_id 2] DDV cos-cos: 0.27107, DDV euc-cos: 0.20193\n",
      "[task_id 6 => target_id 2] DDV cos-cos: 0.28432, DDV euc-cos: 0.19533\n",
      "[task_id 7 => target_id 2] DDV cos-cos: 0.31492, DDV euc-cos: 0.22457\n",
      "[task_id 8 => target_id 2] DDV cos-cos: 0.34408, DDV euc-cos: 0.20768\n",
      "[task_id 9 => target_id 2] DDV cos-cos: 0.22971, DDV euc-cos: 0.22007\n",
      "[task_id 10 => target_id 2] DDV cos-cos: 0.37208, DDV euc-cos: 0.23194\n",
      "[task_id 11 => target_id 2] DDV cos-cos: 0.51319, DDV euc-cos: 0.32122\n",
      "\n",
      "================= TARGET MODEL: 3 =================\n",
      "[task_id 0 => target_id 3] DDV cos-cos: 0.29954, DDV euc-cos: 0.25576\n",
      "[task_id 1 => target_id 3] DDV cos-cos: 0.27569, DDV euc-cos: 0.21029\n",
      "[task_id 2 => target_id 3] DDV cos-cos: 0.31395, DDV euc-cos: 0.22673\n",
      "[task_id 4 => target_id 3] DDV cos-cos: 0.29103, DDV euc-cos: 0.19069\n",
      "[task_id 5 => target_id 3] DDV cos-cos: 0.29204, DDV euc-cos: 0.23033\n",
      "[task_id 6 => target_id 3] DDV cos-cos: 0.36749, DDV euc-cos: 0.27411\n",
      "[task_id 7 => target_id 3] DDV cos-cos: 0.34225, DDV euc-cos: 0.23829\n",
      "[task_id 8 => target_id 3] DDV cos-cos: 0.32345, DDV euc-cos: 0.21023\n",
      "[task_id 9 => target_id 3] DDV cos-cos: 0.15348, DDV euc-cos: 0.15650\n",
      "[task_id 10 => target_id 3] DDV cos-cos: 0.25658, DDV euc-cos: 0.23891\n",
      "[task_id 11 => target_id 3] DDV cos-cos: 0.53684, DDV euc-cos: 0.42730\n",
      "\n",
      "================= TARGET MODEL: 4 =================\n",
      "[task_id 0 => target_id 4] DDV cos-cos: 0.25893, DDV euc-cos: 0.21033\n",
      "[task_id 1 => target_id 4] DDV cos-cos: 0.38888, DDV euc-cos: 0.29157\n",
      "[task_id 2 => target_id 4] DDV cos-cos: 0.31791, DDV euc-cos: 0.23782\n",
      "[task_id 3 => target_id 4] DDV cos-cos: 0.29103, DDV euc-cos: 0.19069\n",
      "[task_id 5 => target_id 4] DDV cos-cos: 0.34648, DDV euc-cos: 0.26816\n",
      "[task_id 6 => target_id 4] DDV cos-cos: 0.53869, DDV euc-cos: 0.40938\n",
      "[task_id 7 => target_id 4] DDV cos-cos: 0.43191, DDV euc-cos: 0.28793\n",
      "[task_id 8 => target_id 4] DDV cos-cos: 0.14845, DDV euc-cos: 0.08030\n",
      "[task_id 9 => target_id 4] DDV cos-cos: 0.35061, DDV euc-cos: 0.25263\n",
      "[task_id 10 => target_id 4] DDV cos-cos: 0.36403, DDV euc-cos: 0.28319\n",
      "[task_id 11 => target_id 4] DDV cos-cos: 0.44288, DDV euc-cos: 0.25290\n",
      "\n",
      "================= TARGET MODEL: 5 =================\n",
      "[task_id 0 => target_id 5] DDV cos-cos: 0.33834, DDV euc-cos: 0.23479\n",
      "[task_id 1 => target_id 5] DDV cos-cos: 0.13538, DDV euc-cos: 0.11349\n",
      "[task_id 2 => target_id 5] DDV cos-cos: 0.27107, DDV euc-cos: 0.20193\n",
      "[task_id 3 => target_id 5] DDV cos-cos: 0.29204, DDV euc-cos: 0.23033\n",
      "[task_id 4 => target_id 5] DDV cos-cos: 0.34648, DDV euc-cos: 0.26816\n",
      "[task_id 6 => target_id 5] DDV cos-cos: 0.31774, DDV euc-cos: 0.17147\n",
      "[task_id 7 => target_id 5] DDV cos-cos: 0.31864, DDV euc-cos: 0.21918\n",
      "[task_id 8 => target_id 5] DDV cos-cos: 0.46321, DDV euc-cos: 0.31671\n",
      "[task_id 9 => target_id 5] DDV cos-cos: 0.28413, DDV euc-cos: 0.26699\n",
      "[task_id 10 => target_id 5] DDV cos-cos: 0.17314, DDV euc-cos: 0.12675\n",
      "[task_id 11 => target_id 5] DDV cos-cos: 0.38819, DDV euc-cos: 0.29615\n",
      "\n",
      "================= TARGET MODEL: 6 =================\n",
      "[task_id 0 => target_id 6] DDV cos-cos: 0.41213, DDV euc-cos: 0.26180\n",
      "[task_id 1 => target_id 6] DDV cos-cos: 0.40256, DDV euc-cos: 0.22328\n",
      "[task_id 2 => target_id 6] DDV cos-cos: 0.28432, DDV euc-cos: 0.19533\n",
      "[task_id 3 => target_id 6] DDV cos-cos: 0.36749, DDV euc-cos: 0.27411\n",
      "[task_id 4 => target_id 6] DDV cos-cos: 0.53869, DDV euc-cos: 0.40938\n",
      "[task_id 5 => target_id 6] DDV cos-cos: 0.31774, DDV euc-cos: 0.17147\n",
      "[task_id 7 => target_id 6] DDV cos-cos: 0.28133, DDV euc-cos: 0.29477\n",
      "[task_id 8 => target_id 6] DDV cos-cos: 0.57406, DDV euc-cos: 0.42887\n",
      "[task_id 9 => target_id 6] DDV cos-cos: 0.51091, DDV euc-cos: 0.41468\n",
      "[task_id 10 => target_id 6] DDV cos-cos: 0.30767, DDV euc-cos: 0.23564\n",
      "[task_id 11 => target_id 6] DDV cos-cos: 0.65086, DDV euc-cos: 0.53523\n",
      "\n",
      "================= TARGET MODEL: 7 =================\n",
      "[task_id 0 => target_id 7] DDV cos-cos: 0.49598, DDV euc-cos: 0.36193\n",
      "[task_id 1 => target_id 7] DDV cos-cos: 0.36712, DDV euc-cos: 0.26433\n",
      "[task_id 2 => target_id 7] DDV cos-cos: 0.31492, DDV euc-cos: 0.22457\n",
      "[task_id 3 => target_id 7] DDV cos-cos: 0.34225, DDV euc-cos: 0.23829\n",
      "[task_id 4 => target_id 7] DDV cos-cos: 0.43191, DDV euc-cos: 0.28793\n",
      "[task_id 5 => target_id 7] DDV cos-cos: 0.31864, DDV euc-cos: 0.21918\n",
      "[task_id 6 => target_id 7] DDV cos-cos: 0.28133, DDV euc-cos: 0.29477\n",
      "[task_id 8 => target_id 7] DDV cos-cos: 0.55694, DDV euc-cos: 0.32361\n",
      "[task_id 9 => target_id 7] DDV cos-cos: 0.40785, DDV euc-cos: 0.33649\n",
      "[task_id 10 => target_id 7] DDV cos-cos: 0.42058, DDV euc-cos: 0.34190\n",
      "[task_id 11 => target_id 7] DDV cos-cos: 0.64998, DDV euc-cos: 0.37739\n",
      "\n",
      "================= TARGET MODEL: 8 =================\n",
      "[task_id 0 => target_id 8] DDV cos-cos: 0.17219, DDV euc-cos: 0.19028\n",
      "[task_id 1 => target_id 8] DDV cos-cos: 0.49146, DDV euc-cos: 0.34326\n",
      "[task_id 2 => target_id 8] DDV cos-cos: 0.34408, DDV euc-cos: 0.20768\n",
      "[task_id 3 => target_id 8] DDV cos-cos: 0.32345, DDV euc-cos: 0.21023\n",
      "[task_id 4 => target_id 8] DDV cos-cos: 0.14845, DDV euc-cos: 0.08030\n",
      "[task_id 5 => target_id 8] DDV cos-cos: 0.46321, DDV euc-cos: 0.31671\n",
      "[task_id 6 => target_id 8] DDV cos-cos: 0.57406, DDV euc-cos: 0.42887\n",
      "[task_id 7 => target_id 8] DDV cos-cos: 0.55694, DDV euc-cos: 0.32361\n",
      "[task_id 9 => target_id 8] DDV cos-cos: 0.41778, DDV euc-cos: 0.29705\n",
      "[task_id 10 => target_id 8] DDV cos-cos: 0.39140, DDV euc-cos: 0.28655\n",
      "[task_id 11 => target_id 8] DDV cos-cos: 0.33268, DDV euc-cos: 0.34272\n",
      "\n",
      "================= TARGET MODEL: 9 =================\n",
      "[task_id 0 => target_id 9] DDV cos-cos: 0.32200, DDV euc-cos: 0.27107\n",
      "[task_id 1 => target_id 9] DDV cos-cos: 0.20496, DDV euc-cos: 0.19734\n",
      "[task_id 2 => target_id 9] DDV cos-cos: 0.22971, DDV euc-cos: 0.22007\n",
      "[task_id 3 => target_id 9] DDV cos-cos: 0.15348, DDV euc-cos: 0.15650\n",
      "[task_id 4 => target_id 9] DDV cos-cos: 0.35061, DDV euc-cos: 0.25263\n",
      "[task_id 5 => target_id 9] DDV cos-cos: 0.28413, DDV euc-cos: 0.26699\n",
      "[task_id 6 => target_id 9] DDV cos-cos: 0.51091, DDV euc-cos: 0.41468\n",
      "[task_id 7 => target_id 9] DDV cos-cos: 0.40785, DDV euc-cos: 0.33649\n",
      "[task_id 8 => target_id 9] DDV cos-cos: 0.41778, DDV euc-cos: 0.29705\n",
      "[task_id 10 => target_id 9] DDV cos-cos: 0.37721, DDV euc-cos: 0.23051\n",
      "[task_id 11 => target_id 9] DDV cos-cos: 0.51753, DDV euc-cos: 0.31908\n",
      "\n",
      "================= TARGET MODEL: 10 =================\n",
      "[task_id 0 => target_id 10] DDV cos-cos: 0.23448, DDV euc-cos: 0.13423\n",
      "[task_id 1 => target_id 10] DDV cos-cos: 0.25751, DDV euc-cos: 0.14678\n",
      "[task_id 2 => target_id 10] DDV cos-cos: 0.37208, DDV euc-cos: 0.23194\n",
      "[task_id 3 => target_id 10] DDV cos-cos: 0.25658, DDV euc-cos: 0.23891\n",
      "[task_id 4 => target_id 10] DDV cos-cos: 0.36403, DDV euc-cos: 0.28319\n",
      "[task_id 5 => target_id 10] DDV cos-cos: 0.17314, DDV euc-cos: 0.12675\n",
      "[task_id 6 => target_id 10] DDV cos-cos: 0.30767, DDV euc-cos: 0.23564\n",
      "[task_id 7 => target_id 10] DDV cos-cos: 0.42058, DDV euc-cos: 0.34190\n",
      "[task_id 8 => target_id 10] DDV cos-cos: 0.39140, DDV euc-cos: 0.28655\n",
      "[task_id 9 => target_id 10] DDV cos-cos: 0.37721, DDV euc-cos: 0.23051\n",
      "[task_id 11 => target_id 10] DDV cos-cos: 0.21165, DDV euc-cos: 0.24669\n",
      "\n",
      "================= TARGET MODEL: 11 =================\n",
      "[task_id 0 => target_id 11] DDV cos-cos: 0.24710, DDV euc-cos: 0.22079\n",
      "[task_id 1 => target_id 11] DDV cos-cos: 0.41128, DDV euc-cos: 0.29351\n",
      "[task_id 2 => target_id 11] DDV cos-cos: 0.51319, DDV euc-cos: 0.32122\n",
      "[task_id 3 => target_id 11] DDV cos-cos: 0.53684, DDV euc-cos: 0.42730\n",
      "[task_id 4 => target_id 11] DDV cos-cos: 0.44288, DDV euc-cos: 0.25290\n",
      "[task_id 5 => target_id 11] DDV cos-cos: 0.38819, DDV euc-cos: 0.29615\n",
      "[task_id 6 => target_id 11] DDV cos-cos: 0.65086, DDV euc-cos: 0.53523\n",
      "[task_id 7 => target_id 11] DDV cos-cos: 0.64998, DDV euc-cos: 0.37739\n",
      "[task_id 8 => target_id 11] DDV cos-cos: 0.33268, DDV euc-cos: 0.34272\n",
      "[task_id 9 => target_id 11] DDV cos-cos: 0.51753, DDV euc-cos: 0.31908\n",
      "[task_id 10 => target_id 11] DDV cos-cos: 0.21165, DDV euc-cos: 0.24669\n",
      "\n",
      "========= Best Cosine Backbone =========\n",
      "Model 0: Best Cosine Task 7 with score 0.49598\n",
      "Model 1: Best Cosine Task 8 with score 0.49146\n",
      "Model 2: Best Cosine Task 11 with score 0.51319\n",
      "Model 3: Best Cosine Task 11 with score 0.53684\n",
      "Model 4: Best Cosine Task 6 with score 0.53869\n",
      "Model 5: Best Cosine Task 8 with score 0.46321\n",
      "Model 6: Best Cosine Task 11 with score 0.65086\n",
      "Model 7: Best Cosine Task 11 with score 0.64998\n",
      "Model 8: Best Cosine Task 6 with score 0.57406\n",
      "Model 9: Best Cosine Task 11 with score 0.51753\n",
      "Model 10: Best Cosine Task 7 with score 0.42058\n",
      "Model 11: Best Cosine Task 6 with score 0.65086\n",
      "\n",
      "========= Best Euclidean Backbone =========\n",
      "Model 0: Best Euclidean Task 7 with score 0.36193\n",
      "Model 1: Best Euclidean Task 8 with score 0.34326\n",
      "Model 2: Best Euclidean Task 11 with score 0.32122\n",
      "Model 3: Best Euclidean Task 11 with score 0.42730\n",
      "Model 4: Best Euclidean Task 6 with score 0.40938\n",
      "Model 5: Best Euclidean Task 8 with score 0.31671\n",
      "Model 6: Best Euclidean Task 11 with score 0.53523\n",
      "Model 7: Best Euclidean Task 11 with score 0.37739\n",
      "Model 8: Best Euclidean Task 6 with score 0.42887\n",
      "Model 9: Best Euclidean Task 6 with score 0.41468\n",
      "Model 10: Best Euclidean Task 7 with score 0.34190\n",
      "Model 11: Best Euclidean Task 6 with score 0.53523\n"
     ]
    }
   ],
   "source": [
    "ddvcc_list_all = []  # (target_id, task_id, cos-distance) \n",
    "ddvec_list_all = []  # (target_id, task_id, euc-distance) \n",
    "\n",
    "best_cos_list = []  # target_id별 best_cos task_id 저장\n",
    "best_euc_list = []  # target_id별 best_euc task_id 저장\n",
    "\n",
    "# target_id vs. task_id\n",
    "for target_id in range(12):\n",
    "    print(f\"\\n================= TARGET MODEL: {target_id} =================\")\n",
    "    \n",
    "    is_target_text = (target_id < 6)  # Target 모델이 텍스트 모델인지 확인\n",
    "\n",
    "    best_score_cos = 0  # 현재 target_id에서 최고 cos 거리\n",
    "    best_id_cos = None  # best_score_cos를 기록한 task_id\n",
    "    best_score_euc = 0  # 현재 target_id에서 최고 euc 거리\n",
    "    best_id_euc = None  # best_score_euc를 기록한 task_id\n",
    "\n",
    "    for task_id in range(12):\n",
    "        if task_id == target_id:  # 동일 모델은 skip\n",
    "            continue\n",
    "\n",
    "        is_task_text = (task_id < 6)  # task 모델이 텍스트 모델인지 확인\n",
    "\n",
    "        # target\n",
    "        batch_target = next(iter(valid_loaders[target_id]))\n",
    "        if is_target_text:\n",
    "            # 텍스트 (B, seq_len)\n",
    "            inputs_target = batch_target['input_ids'].to(DEVICE).long()\n",
    "        else:\n",
    "            # 이미지 (B, T, F)\n",
    "            inputs_target = batch_target[0].to(DEVICE)\n",
    "\n",
    "        # task \n",
    "        batch_task = next(iter(valid_loaders[task_id]))\n",
    "        if is_task_text:\n",
    "            inputs_task = batch_task['input_ids'].to(DEVICE).long()\n",
    "        else:\n",
    "            inputs_task = batch_task[0].to(DEVICE)\n",
    "\n",
    "        # DDV Cosine \n",
    "        ddv1, ddv2 = compute_ddv_cos(\n",
    "            model1=input_models[target_id],\n",
    "            model2=input_models[task_id],\n",
    "            inputs1=inputs_target,\n",
    "            inputs2=inputs_task,\n",
    "            is_model1_text=is_target_text,\n",
    "            is_model2_text=is_task_text,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        ddv_distance_cos = compute_sim_cos(ddv1, ddv2)  # cos-distance\n",
    "\n",
    "        # DDV Euclidean \n",
    "        ddv1, ddv2 = compute_ddv_euc(\n",
    "            model1=input_models[target_id],\n",
    "            model2=input_models[task_id],\n",
    "            inputs1=inputs_target,\n",
    "            inputs2=inputs_task,\n",
    "            is_model1_text=is_target_text,\n",
    "            is_model2_text=is_task_text,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        ddv_distance_euc = compute_sim_cos(ddv1, ddv2)  # euc-distance\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"[task_id {task_id} => target_id {target_id}] \"\n",
    "              f\"DDV cos-cos: {ddv_distance_cos:.5f}, \"\n",
    "              f\"DDV euc-cos: {ddv_distance_euc:.5f}\")\n",
    "\n",
    "        # 리스트에 기록\n",
    "        ddvcc_list_all.append((target_id, task_id, ddv_distance_cos))\n",
    "        ddvec_list_all.append((target_id, task_id, ddv_distance_euc))\n",
    "\n",
    "        # 최고 점수 업데이트 (Cosine)\n",
    "        if ddv_distance_cos > best_score_cos:\n",
    "            best_score_cos = ddv_distance_cos\n",
    "            best_id_cos = task_id\n",
    "\n",
    "        # 최고 점수 업데이트 (Euclidean)\n",
    "        if ddv_distance_euc > best_score_euc:\n",
    "            best_score_euc = ddv_distance_euc\n",
    "            best_id_euc = task_id\n",
    "\n",
    "    # 현재 target_id의 best_cos와 best_euc 추가\n",
    "    best_cos_list.append((target_id, best_id_cos, best_score_cos))\n",
    "    best_euc_list.append((target_id, best_id_euc, best_score_euc))\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"\\n========= Best Cosine Backbone =========\")\n",
    "for target_id, best_id_cos, best_score_cos in best_cos_list:\n",
    "    print(f\"Model {target_id}: Best Cosine Task {best_id_cos} with score {best_score_cos:.5f}\")\n",
    "\n",
    "print(\"\\n========= Best Euclidean Backbone =========\")\n",
    "for target_id, best_id_euc, best_score_euc in best_euc_list:\n",
    "    print(f\"Model {target_id}: Best Euclidean Task {best_id_euc} with score {best_score_euc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: latent vector로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
