{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 1,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "from scipy import spatial\n",
<<<<<<< HEAD
    "import csv\n",
    "\n",
    "from perceiver import crop, patchify, get_patch_coords, ImageDataset, PerceiverBlock, Perceiver"
=======
    "import csv"
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img_tensor, crop_size=32):\n",
    "    C, H, W = img_tensor.shape\n",
    "    img_tensor = img_tensor[:, crop_size:H-crop_size, crop_size:W-crop_size]\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(img_tensor, patch_size=16):\n",
    "    \"\"\"\n",
    "    img_tensor: (C, H, W) 형태 (예: (3, 224, 224))\n",
    "    patch_size: 패치 크기 (16, 16)\n",
    "    return: (num_patches, patch_dim)\n",
    "            예) (196, 768)  # (H/16)*(W/16)=14*14=196, 768=16*16*3\n",
    "    \"\"\"\n",
    "    C, H, W = img_tensor.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"이미지 크기는 patch_size로 나누어 떨어져야 함\"\n",
    "\n",
    "    # unfold로 (patch_size, patch_size)씩 잘라내기\n",
    "    patches = img_tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    " \n",
    "    patches = patches.permute(1, 3, 0, 2, 4).contiguous()\n",
    "\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    num_patches = num_patches_h * num_patches_w\n",
    "\n",
    "    patches = patches.view(num_patches, -1) \n",
    "    return patches\n",
    "\n",
    "def get_patch_coords(num_patches_h, num_patches_w):\n",
    "\n",
    "    y_coord = torch.linspace(0, 1, steps=num_patches_h)\n",
    "    x_coord = torch.linspace(0, 1, steps=num_patches_w)\n",
    "    grid_y, grid_x = torch.meshgrid(y_coord, x_coord, indexing='ij')  # (14,14) each\n",
    "\n",
    "    coords = torch.stack([grid_x, grid_y], dim=-1)  # (14,14,2)\n",
    "    coords = coords.view(-1, 2)                     # (196, 2)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, crop_size=32, patch_size=16):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.crop_size = crop_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # 데이터 및 레이블 추출\n",
    "        labels = []\n",
    "        for folder in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "                if len(image_files) == 1:\n",
    "                    image_path = image_files[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"폴더 {folder}에 JPG 파일이 하나가 아닙니다.\")\n",
    "\n",
    "                label_path = os.path.join(folder_path, \"label.txt\")\n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, \"r\") as f:\n",
    "                        label = f.read().strip()\n",
    "                        labels.append(label)\n",
    "                        self.data.append((image_path, label))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"폴더 {folder}에 label.txt가 없습니다.\")\n",
    "\n",
    "        self.label_encoder.fit(labels)\n",
    "        self.data = [(image_path, self.label_encoder.transform([label])[0]) \n",
    "                     for image_path, label in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # (3, 224, 224)\n",
    "        image = crop(image, crop_size=self.crop_size)\n",
    "        patches = patchify(image, patch_size=self.patch_size)  # (num_patches, patch_dim)\n",
    "\n",
    "        H, W = image.shape[1], image.shape[2]  \n",
    "        num_patches_h = H // self.patch_size   \n",
    "        num_patches_w = W // self.patch_size   \n",
    "        coords = get_patch_coords(num_patches_h, num_patches_w)  \n",
    "\n",
    "        combined = torch.cat([patches, coords], dim=1)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return combined, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    - Cross Attention (latents -> x)\n",
    "    - 이어서 Self Attention (latent들 끼리)\n",
    "    - 보통은 LayerNorm, MLP(FeedForward) 등을 곁들여 residual branch를 구성\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_heads=8, self_attn_layers=1):\n",
    "        super().__init__()\n",
    "        # Cross Attention\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=n_heads)\n",
    "        self.cross_ln = nn.LayerNorm(latent_dim)  # 잊지 말고 layernorm\n",
    "\n",
    "        # Self Attention 여러 층\n",
    "        self.self_attn_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=latent_dim, nhead=n_heads)\n",
    "            for _ in range(self_attn_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, latents, x):\n",
    "        # latents, x: (T, B, dim) 형태로 가정 (주의!)\n",
    "        # Perceiver 원리상 latents는 query, x는 key/value\n",
    "\n",
    "        # 1) Cross Attention\n",
    "        updated_latents, _ = self.cross_attn(latents, x, x)\n",
    "        latents = latents + updated_latents        # Residual\n",
    "        latents = self.cross_ln(latents)           # LayerNorm\n",
    "\n",
    "        # 2) Self Attention 반복\n",
    "        for layer in self.self_attn_layers:\n",
    "            latents = layer(latents)  # 내부적으로 residual/LayerNorm 포함\n",
    "\n",
    "        return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, latent_size, num_classes,\n",
    "                 num_blocks, self_attn_layers_per_block=1):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(latent_size, latent_dim))\n",
    "        self.input_projection = nn.Linear(input_dim, latent_dim)\n",
    "\n",
    "        # 반복될 PerceiverBlock을 여러 개 쌓는다.\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PerceiverBlock(\n",
    "                latent_dim=latent_dim,\n",
    "                n_heads=8,\n",
    "                self_attn_layers=self_attn_layers_per_block\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) = (배치, 시퀀스길이, 피처차원)\n",
    "        \"\"\"\n",
    "        B, T, F = x.size()\n",
    "        x = self.input_projection(x)                 # (B, T, latent_dim)\n",
    "\n",
    "        # latents: (latent_size, latent_dim) -> 배치 차원 확장 (B, latent_size, latent_dim)\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        # MultiHeadAttention은 (seq, batch, dim) 순서를 권장하므로 permute\n",
    "        x = x.permute(1, 0, 2)        # (T, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2)  # (latent_size, B, latent_dim)\n",
    "\n",
    "        # 여러 개의 PerceiverBlock 반복\n",
    "        for block in self.blocks:\n",
    "            latents = block(latents, x)\n",
    "\n",
    "        # 최종 latents: (latent_size, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2).mean(dim=1)  # (B, latent_dim)\n",
    "        return self.output_layer(latents)\n"
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Load Model, DataLoader"
=======
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/Minju/Perceiver/model/'\n",
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/Minju/Perceiver/loader/'\n",
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
    "loader_dir = '/home/Minju/Perceiver/loader/'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model 1번 불러오기 완료.\n",
      "Text val. loader 0번 불러오기 완료.\n",
      "Text model 2번 불러오기 완료.\n",
      "Text val. loader 1번 불러오기 완료.\n",
      "Text model 3번 불러오기 완료.\n",
      "Text val. loader 2번 불러오기 완료.\n",
      "Text model 4번 불러오기 완료.\n",
      "Text val. loader 3번 불러오기 완료.\n",
      "Text model 5번 불러오기 완료.\n",
      "Text val. loader 4번 불러오기 완료.\n",
      "Text model 6번 불러오기 완료.\n",
      "Text val. loader 5번 불러오기 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145447/2580130533.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n"
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133509/3766695569.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'model_image_{i}.pkl')\n"
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "input_models = []\n",
    "valid_loaders = []\n",
    "for i in range (6):\n",
    "    text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n",
    "    input_models.append(text_model)\n",
    "    print(f\"Text model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "    with open(loader_dir+f'text_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    \n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)\n",
    "    print(f\"Text val. loader {i}번 불러오기 완료.\")\n",
    "\n",
    "# for i in range(6):\n",
    "#     img_model = torch.load(root_dir + f'image_model_{i}.pkl')\n",
    "#     input_models.append(img_model)\n",
    "#     print(f\"Image model {i}번 불러오기 완료.\")\n",
    "\n",
    "#     with open(loader_dir+f'image_val_loader_{i}.pkl', 'rb') as f:\n",
    "#         loaded_valid_dataset = pickle.load(f)\n",
    "\n",
    "#     valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     valid_loaders.append(valid_loader)\n",
    "#     print(f\"Image val. loader {i}번 불러오기 완료.\")"
=======
    "text_models = []\n",
    "image_models = []\n",
    "valid_loaders = []\n",
    "\n",
    "for i in range (6):\n",
    "    #text_model = torch.load(root_dir + f'model_text_{i}')\n",
    "    #text_models.append(text_model)\n",
    "\n",
    "    img_model = torch.load(root_dir + f'model_image_{i}.pkl')\n",
    "    image_models.append(img_model)\n",
    "\n",
    "    with open(loader_dir+f'val_loader_{i}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)"
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 9,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ModelDiff"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 10,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = -1\n",
    "lr = 0.1\n",
    "batch_size = 32\n",
    "val_batch_size = 100\n",
    "workers = 24\n",
    "weight_decay = 4e-5\n",
    "dataset_name = ''\n",
    "train_path = ''\n",
    "val_path = ''\n",
    "cuda = True\n",
    "seed = 1\n",
    "epochs = 160\n",
    "restore_epoch = 0\n",
    "save_folder = ''\n",
    "load_folder = ''\n",
    "one_shot_prune_perc = 0.5\n",
    "mode = ''\n",
    "logfile = ''\n",
    "initial_from_task = ''"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 11,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    'Opinion','Art & Design','Television',\n",
    "    'Music','Travel','Real Estate',\n",
    "    'Books','Theater','Health',\n",
    "    'Sports','Science','Food',\n",
    "    'Fashion & Style','Movies','Technology',\n",
    "    'Dance', 'Media', 'Style'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 12,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 13,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유사도검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "task_id 책정방식: \\\n",
    "0~5 : text modality \\\n",
    "6~11: image modality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
    "## Approach 1: 특정 input data로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 14,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv_cos(model1, model2, inputs):\n",
    "    global outputs\n",
    "    global outputs2\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        dists = []\n",
    "        outputs = model1(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs):\n",
    "            ya = outputs[i]\n",
    "            yb = outputs[i + n_pairs]\n",
    "            dist = spatial.distance.cosine(ya, yb)\n",
    "            dists.append(dist)\n",
    "\n",
    "        dists2 = []\n",
    "        outputs2 = model2(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs2 = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs2):\n",
    "            ya = outputs2[i]\n",
    "            yb = outputs2[i + n_pairs]\n",
    "            dist = spatial.distance.cosine(ya, yb)\n",
    "            dists2.append(dist)\n",
    "    return np.array(dists), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 15,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv_euc(model1, model2, inputs):\n",
    "    global outputs\n",
    "    global outputs2\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        dists = []\n",
    "        outputs = model1(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs):\n",
    "            ya = outputs[i]\n",
    "            yb = outputs[i + n_pairs]\n",
    "            dist = spatial.distance.euclidean(ya, yb) # dist = spatial.distance.cosine(ya, yb)\n",
    "            dists.append(dist)\n",
    "\n",
    "        dists2 = []\n",
    "        outputs2 = model2(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs2 = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs2):\n",
    "            ya = outputs2[i]\n",
    "            yb = outputs2[i + n_pairs]\n",
    "            dist = spatial.distance.euclidean(ya, yb) # dist = spatial.distance.cosine(ya, yb)\n",
    "            dists2.append(dist)\n",
    "    return np.array(dists), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 16,
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "metadata": {},
   "outputs": [],
   "source": [
    "##### compute_similarity #####\n",
    "def compute_sim_cos(ddv1, ddv2):\n",
    "    return spatial.distance.cosine(ddv1, ddv2)    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_id \u001b[38;5;241m==\u001b[39m target_id:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m     11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# 첫 번째 배치의 입력만 사용\u001b[39;00m\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/home/Minju/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDV cos-cos [1 => 0] 0.26369\n",
      "DDV euc-cos [1 => 0] 0.21469\n",
      "DDV cos-cos [2 => 0] 0.66172\n",
      "DDV euc-cos [2 => 0] 0.40264\n",
      "DDV cos-cos [3 => 0] 0.14389\n",
      "DDV euc-cos [3 => 0] 0.08295\n",
      "DDV cos-cos [4 => 0] 0.11363\n",
      "DDV euc-cos [4 => 0] 0.07273\n",
      "DDV cos-cos [5 => 0] 0.70317\n",
      "DDV euc-cos [5 => 0] 0.33502\n"
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
     ]
    }
   ],
   "source": [
    "ddvcc_list = []\n",
    "ddvec_list = []\n",
    "\n",
    "for task_id in range(6):\n",
    "    if task_id == target_id:\n",
    "        continue\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "    batch = next(iter(valid_loaders[target_id]))  \n",
    "    print(batch)\n",
    "    inputs = batch['input_ids']  # 첫 번째 배치의 입력만 사용\n",
    "\n",
    "    ddv1, ddv2 = compute_ddv_cos(input_models[target_id], input_models[task_id], inputs)\n",
=======
    "    inputs = next(iter(valid_loaders[target_id]))[0]  # 첫 번째 배치의 입력만 사용\n",
    "\n",
    "    ddv1, ddv2 = compute_ddv_cos(image_models[target_id], image_models[task_id], inputs)\n",
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
    "    ddv_distance = compute_sim_cos(ddv1, ddv2)\n",
    "    print('DDV cos-cos [%d => %d] %.5f'%(task_id, target_id, ddv_distance))\n",
    "    ddvcc_list.append(ddv_distance)\n",
    "\n",
    "    # DDV-EC\n",
<<<<<<< HEAD
    "    ddv1, ddv2 = compute_ddv_euc(input_models[target_id], input_models[task_id], inputs)\n",
=======
    "    ddv1, ddv2 = compute_ddv_euc(image_models[target_id], image_models[task_id], inputs)\n",
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
    "    ddv_distance = compute_sim_cos(ddv1, ddv2)\n",
    "    print('DDV euc-cos [%d => %d] %.5f'%(task_id, target_id, ddv_distance))\n",
    "    ddvec_list.append(ddv_distance)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
=======
>>>>>>> 1eab5a5a0d027bcdfe83c5191b08ca5414f0c7f4
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: latent vector로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
