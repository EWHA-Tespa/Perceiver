{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "from scipy import spatial\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(img_tensor, patch_size=16):\n",
    "    \"\"\"\n",
    "    img_tensor: (C, H, W) 형태 (예: (3, 224, 224))\n",
    "    patch_size: 패치 크기 (16, 16)\n",
    "    return: (num_patches, patch_dim)\n",
    "            예) (196, 768)  # (H/16)*(W/16)=14*14=196, 768=16*16*3\n",
    "    \"\"\"\n",
    "    C, H, W = img_tensor.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"이미지 크기는 patch_size로 나누어 떨어져야 함\"\n",
    "\n",
    "    # unfold로 (patch_size, patch_size)씩 잘라내기\n",
    "    patches = img_tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    " \n",
    "    patches = patches.permute(1, 3, 0, 2, 4).contiguous()\n",
    "\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    num_patches = num_patches_h * num_patches_w\n",
    "\n",
    "    patches = patches.view(num_patches, -1) \n",
    "    return patches\n",
    "\n",
    "def get_patch_coords(num_patches_h, num_patches_w):\n",
    "\n",
    "    y_coord = torch.linspace(0, 1, steps=num_patches_h)\n",
    "    x_coord = torch.linspace(0, 1, steps=num_patches_w)\n",
    "    grid_y, grid_x = torch.meshgrid(y_coord, x_coord, indexing='ij')  # (14,14) each\n",
    "\n",
    "    coords = torch.stack([grid_x, grid_y], dim=-1)  # (14,14,2)\n",
    "    coords = coords.view(-1, 2)                     # (196, 2)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, crop_size=32, patch_size=16):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.crop_size = crop_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # 데이터 및 레이블 추출\n",
    "        labels = []\n",
    "        for folder in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "                if len(image_files) == 1:\n",
    "                    image_path = image_files[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"폴더 {folder}에 JPG 파일이 하나가 아닙니다.\")\n",
    "\n",
    "                label_path = os.path.join(folder_path, \"label.txt\")\n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, \"r\") as f:\n",
    "                        label = f.read().strip()\n",
    "                        labels.append(label)\n",
    "                        self.data.append((image_path, label))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"폴더 {folder}에 label.txt가 없습니다.\")\n",
    "\n",
    "        self.label_encoder.fit(labels)\n",
    "        self.data = [(image_path, self.label_encoder.transform([label])[0]) \n",
    "                     for image_path, label in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # (3, 224, 224)\n",
    "        image = crop(image, crop_size=self.crop_size)\n",
    "        patches = patchify(image, patch_size=self.patch_size)  # (num_patches, patch_dim)\n",
    "\n",
    "        H, W = image.shape[1], image.shape[2]  \n",
    "        num_patches_h = H // self.patch_size   \n",
    "        num_patches_w = W // self.patch_size   \n",
    "        coords = get_patch_coords(num_patches_h, num_patches_w)  \n",
    "\n",
    "        combined = torch.cat([patches, coords], dim=1)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return combined, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    - Cross Attention (latents -> x)\n",
    "    - 이어서 Self Attention (latent들 끼리)\n",
    "    - 보통은 LayerNorm, MLP(FeedForward) 등을 곁들여 residual branch를 구성\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_heads=8, self_attn_layers=1):\n",
    "        super().__init__()\n",
    "        # Cross Attention\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=n_heads)\n",
    "        self.cross_ln = nn.LayerNorm(latent_dim)  # 잊지 말고 layernorm\n",
    "\n",
    "        # Self Attention 여러 층\n",
    "        self.self_attn_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=latent_dim, nhead=n_heads)\n",
    "            for _ in range(self_attn_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, latents, x):\n",
    "        # latents, x: (T, B, dim) 형태로 가정 (주의!)\n",
    "        # Perceiver 원리상 latents는 query, x는 key/value\n",
    "\n",
    "        # 1) Cross Attention\n",
    "        updated_latents, _ = self.cross_attn(latents, x, x)\n",
    "        latents = latents + updated_latents        # Residual\n",
    "        latents = self.cross_ln(latents)           # LayerNorm\n",
    "\n",
    "        # 2) Self Attention 반복\n",
    "        for layer in self.self_attn_layers:\n",
    "            latents = layer(latents)  # 내부적으로 residual/LayerNorm 포함\n",
    "\n",
    "        return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, latent_size, num_classes,\n",
    "                 num_blocks, self_attn_layers_per_block=1):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(latent_size, latent_dim))\n",
    "        self.input_projection = nn.Linear(input_dim, latent_dim)\n",
    "\n",
    "        # 반복될 PerceiverBlock을 여러 개 쌓는다.\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PerceiverBlock(\n",
    "                latent_dim=latent_dim,\n",
    "                n_heads=8,\n",
    "                self_attn_layers=self_attn_layers_per_block\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) = (배치, 시퀀스길이, 피처차원)\n",
    "        \"\"\"\n",
    "        B, T, F = x.size()\n",
    "        x = self.input_projection(x)                 # (B, T, latent_dim)\n",
    "\n",
    "        # latents: (latent_size, latent_dim) -> 배치 차원 확장 (B, latent_size, latent_dim)\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        # MultiHeadAttention은 (seq, batch, dim) 순서를 권장하므로 permute\n",
    "        x = x.permute(1, 0, 2)        # (T, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2)  # (latent_size, B, latent_dim)\n",
    "\n",
    "        # 여러 개의 PerceiverBlock 반복\n",
    "        for block in self.blocks:\n",
    "            latents = block(latents, x)\n",
    "\n",
    "        # 최종 latents: (latent_size, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2).mean(dim=1)  # (B, latent_dim)\n",
    "        return self.output_layer(latents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/Minju/Perceiver/loader/'\n",
    "loader_dir = '/home/Minju/Perceiver/loader/'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126162/3766695569.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'model_image_{i}.pkl')\n"
     ]
    }
   ],
   "source": [
    "text_models = []\n",
    "image_models = []\n",
    "valid_loaders = []\n",
    "\n",
    "for i in range (6):\n",
    "    #text_model = torch.load(root_dir + f'model_text_{i}')\n",
    "    #text_models.append(text_model)\n",
    "\n",
    "    img_model = torch.load(root_dir + f'model_image_{i}.pkl')\n",
    "    image_models.append(img_model)\n",
    "\n",
    "    with open(loader_dir+f'val_loader_{i}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ModelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = -1\n",
    "lr = 0.1\n",
    "batch_size = 32\n",
    "val_batch_size = 100\n",
    "workers = 24\n",
    "weight_decay = 4e-5\n",
    "dataset_name = ''\n",
    "train_path = ''\n",
    "val_path = ''\n",
    "cuda = True\n",
    "seed = 1\n",
    "epochs = 160\n",
    "restore_epoch = 0\n",
    "save_folder = ''\n",
    "load_folder = ''\n",
    "one_shot_prune_perc = 0.5\n",
    "mode = ''\n",
    "logfile = ''\n",
    "initial_from_task = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    'Opinion','Art & Design','Television',\n",
    "    'Music','Travel','Real Estate',\n",
    "    'Books','Theater','Health',\n",
    "    'Sports','Science','Food',\n",
    "    'Fashion & Style','Movies','Technology',\n",
    "    'Dance', 'Media', 'Style'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유사도검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: 특정 input data로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv_cos(model1, model2, inputs):\n",
    "    global outputs\n",
    "    global outputs2\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        dists = []\n",
    "        outputs = model1(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs):\n",
    "            ya = outputs[i]\n",
    "            yb = outputs[i + n_pairs]\n",
    "            dist = spatial.distance.cosine(ya, yb)\n",
    "            dists.append(dist)\n",
    "\n",
    "        dists2 = []\n",
    "        outputs2 = model2(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs2 = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs2):\n",
    "            ya = outputs2[i]\n",
    "            yb = outputs2[i + n_pairs]\n",
    "            dist = spatial.distance.cosine(ya, yb)\n",
    "            dists2.append(dist)\n",
    "    return np.array(dists), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv_euc(model1, model2, inputs):\n",
    "    global outputs\n",
    "    global outputs2\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        dists = []\n",
    "        outputs = model1(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs):\n",
    "            ya = outputs[i]\n",
    "            yb = outputs[i + n_pairs]\n",
    "            dist = spatial.distance.euclidean(ya, yb) # dist = spatial.distance.cosine(ya, yb)\n",
    "            dists.append(dist)\n",
    "\n",
    "        dists2 = []\n",
    "        outputs2 = model2(torch.Tensor(inputs).cuda()).to('cpu').tolist()\n",
    "        n_pairs2 = int(len(list(inputs)) / 2)\n",
    "        for i in range(n_pairs2):\n",
    "            ya = outputs2[i]\n",
    "            yb = outputs2[i + n_pairs]\n",
    "            dist = spatial.distance.euclidean(ya, yb) # dist = spatial.distance.cosine(ya, yb)\n",
    "            dists2.append(dist)\n",
    "    return np.array(dists), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### compute_similarity #####\n",
    "def compute_sim_cos(ddv1, ddv2):\n",
    "    return spatial.distance.cosine(ddv1, ddv2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got DataLoader)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_id \u001b[38;5;241m==\u001b[39m target_id:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ddv1, ddv2 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ddv_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m ddv_distance \u001b[38;5;241m=\u001b[39m compute_sim_cos(ddv1, ddv2)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDV cos-cos [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(task_id, target_id, ddv_distance))\n",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m, in \u001b[0;36mcompute_ddv_cos\u001b[0;34m(model1, model2, inputs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     dists \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model1(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda())\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m     n_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(inputs)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_pairs):\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got DataLoader)"
     ]
    }
   ],
   "source": [
    "ddvcc_list = []\n",
    "ddvec_list = []\n",
    "\n",
    "for task_id in range(6):\n",
    "    if task_id == target_id:\n",
    "        continue\n",
    "    ddv1, ddv2 = compute_ddv_cos(image_models[target_id], image_models[task_id], valid_loaders[target_id])\n",
    "    ddv_distance = compute_sim_cos(ddv1, ddv2)\n",
    "    print('DDV cos-cos [%d => %d] %.5f'%(task_id, target_id, ddv_distance))\n",
    "    ddvcc_list.append(ddv_distance)\n",
    "\n",
    "    # DDV-EC\n",
    "    ddv1, ddv2 = compute_ddv_euc(image_models[target_id], image_models[task_id], valid_loaders[target_id])\n",
    "    ddv_distance = compute_sim_cos(ddv1, ddv2)\n",
    "    print('DDV euc-cos [%d => %d] %.5f'%(task_id, target_id, ddv_distance))\n",
    "    ddvec_list.append(ddv_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: latent vector로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
