{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "from scipy import spatial\n",
    "import csv\n",
    "\n",
    "from perceiver import crop, patchify, get_patch_coords, ImageDataset, PerceiverBlock, Perceiver, CustomDataset, CombinedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/youlee/perceiver/perceiver/model/'\n",
    "loader_dir = '/home/youlee/perceiver/perceiver/loader/'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_models = []\n",
    "valid_loaders = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206376/2310590346.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model 1번 불러오기 완료.\n",
      "Text val. loader 0번 불러오기 완료.\n",
      "Text model 2번 불러오기 완료.\n",
      "Text val. loader 1번 불러오기 완료.\n",
      "Text model 3번 불러오기 완료.\n",
      "Text val. loader 2번 불러오기 완료.\n",
      "Text model 4번 불러오기 완료.\n",
      "Text val. loader 3번 불러오기 완료.\n",
      "Text model 5번 불러오기 완료.\n",
      "Text val. loader 4번 불러오기 완료.\n",
      "Text model 6번 불러오기 완료.\n",
      "Text val. loader 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range (6):\n",
    "    text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n",
    "    input_models.append(text_model)\n",
    "    print(f\"Text model {i+1}번 불러오기 완료.\")\n",
    "\n",
    "    with open(loader_dir+f'text_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loaders.append(loaded_valid_dataset)\n",
    "    print(f\"Text val. loader {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 0번 불러오기 완료.\n",
      "Image val. loader 0번 불러오기 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206376/520048891.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 1번 불러오기 완료.\n",
      "Image val. loader 1번 불러오기 완료.\n",
      "Image model 2번 불러오기 완료.\n",
      "Image val. loader 2번 불러오기 완료.\n",
      "Image model 3번 불러오기 완료.\n",
      "Image val. loader 3번 불러오기 완료.\n",
      "Image model 4번 불러오기 완료.\n",
      "Image val. loader 4번 불러오기 완료.\n",
      "Image model 5번 불러오기 완료.\n",
      "Image val. loader 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n",
    "    input_models.append(img_model)\n",
    "    print(f\"Image model {i}번 불러오기 완료.\")\n",
    "\n",
    "    with open(loader_dir+f'image_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)\n",
    "    print(f\"Image val. loader {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataloader.DataLoader at 0x7f2226794430>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f219afa2a40>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d5639660>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d563b3a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f2225305630>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d4dd1d50>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d53d7bb0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d55fbdc0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d4aeba90>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d4997dc0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d4463ca0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f20d41c8040>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loaders  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Valid Loader 1 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 2 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 3 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 4 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 5 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 6 ---\n",
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'labels': torch.Size([32])}\n",
      "--- Valid Loader 7 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 8 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 9 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 10 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 11 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n",
      "--- Valid Loader 12 ---\n",
      "Inputs shape: torch.Size([32, 196, 770]), Labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# valid_loader 1-12까지 첫번째 배치 shape 확인\n",
    "\n",
    "for idx, loader in enumerate(valid_loaders):\n",
    "    print(f\"--- Valid Loader {idx + 1} ---\")\n",
    "    \n",
    "    batch = next(iter(loader))  \n",
    "    \n",
    "    # 배치가 리스트 형식일 경우, inputs과 labels 출력\n",
    "    if isinstance(batch, list):\n",
    "        print(f\"Inputs shape: {batch[0].shape}, Labels shape: {batch[1].shape}\")\n",
    "    elif isinstance(batch, dict):\n",
    "        # 배치가 딕셔너리 형식일 때, 각 key-value 쌍의 shape 출력\n",
    "        print({key: value.shape for key, value in batch.items()})\n",
    "    elif isinstance(batch, tuple):\n",
    "        # 배치가 튜플 형식일 때, Inputs과 Labels의 shape 출력\n",
    "        print(f\"Inputs shape: {batch[0].shape}, Labels shape: {batch[1].shape}\")\n",
    "    else:\n",
    "        print(f\"Unknown batch format: {type(batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Model:\n",
      "Perceiver(\n",
      "  (input_projection): Linear(in_features=770, out_features=128, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x PerceiverBlock(\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (cross_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn_layers): ModuleList(\n",
      "        (0-9): 10 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=24, bias=True)\n",
      ")\n",
      "\n",
      "Text Model:\n",
      "CombinedModel(\n",
      "  (embedding): Embedding(30522, 128)\n",
      "  (perceiver): Perceiver(\n",
      "    (input_projection): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x PerceiverBlock(\n",
      "        (cross_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (cross_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn_layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206376/3364340435.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image_model = torch.load(image_model_path)\n",
      "/tmp/ipykernel_206376/3364340435.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(text_model_path)\n"
     ]
    }
   ],
   "source": [
    "# image, text 모델 확인\n",
    "image_model_path = \"/home/youlee/perceiver/perceiver/model/image_model_1.pkl\"\n",
    "text_model_path = \"/home/youlee/perceiver/perceiver/model/text_model_1.pkl\"\n",
    "\n",
    "image_model = torch.load(image_model_path)\n",
    "print(\"Image Model:\")\n",
    "print(image_model)\n",
    "\n",
    "text_model = torch.load(text_model_path)\n",
    "print(\"\\nText Model:\")\n",
    "print(text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ModelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = -1\n",
    "lr = 0.1\n",
    "batch_size = 32\n",
    "val_batch_size = 100\n",
    "workers = 24\n",
    "weight_decay = 4e-5\n",
    "dataset_name = ''\n",
    "train_path = ''\n",
    "val_path = ''\n",
    "cuda = True\n",
    "seed = 1\n",
    "epochs = 160\n",
    "restore_epoch = 0\n",
    "save_folder = ''\n",
    "load_folder = ''\n",
    "one_shot_prune_perc = 0.5\n",
    "mode = ''\n",
    "logfile = ''\n",
    "initial_from_task = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    'Opinion','Art & Design','Television',\n",
    "    'Music','Travel','Real Estate',\n",
    "    'Books','Theater','Health',\n",
    "    'Sports','Science','Food',\n",
    "    'Fashion & Style','Movies','Technology',\n",
    "    'Dance', 'Media', 'Style'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task_id 책정방식: \\\n",
    "0~5 : text modality \\\n",
    "6~11: image modality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: 특정 input data로 유사도 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(output_layer): Linear(in_features=128, out_features=24, bias=True)\n",
    "이미지 모델 출력: (batch_size, 24)\n",
    "\n",
    "(output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
    "텍스트 모델 출력: (batch_size, 3)\n",
    "\n",
    "perceiver 모델에서 output은 (batch_size, num_classes)\n",
    "이미지와 텍스트의 num_classes가 다르게 되어있음 \n",
    "\n",
    "text baseline 코드에서 out_feature를 24로 수정하면 되지 않음?\n",
    "-> 데이터셋을 class 3개인 6개 그룹으로 나눴고 num_classes = len(label_encoder.classes_)로 해뒀음..\n",
    "\n",
    "=> 이미지랑 텍스트를 같은 차원으로 매핑하는 projection layer 넣음\n",
    "\"\"\"\n",
    "\n",
    "# Projection Layer 정의\n",
    "projection_dim = 24  # 이 값에 따라서 best target_id 조금씩 달라짐 (128로 해봤는데 달라짐)\n",
    "image_projection = nn.Linear(24, projection_dim).to(DEVICE)\n",
    "text_projection = nn.Linear(3, projection_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDV 계산: Cosine Distance\n",
    "def compute_ddv_cos(\n",
    "    model1, model2,\n",
    "    inputs1, inputs2,            \n",
    "    proj1, proj2,\n",
    "    is_model1_text=False, # model1과 model2가 텍스트 모델인지 여부 확인인\n",
    "    is_model2_text=False,\n",
    "    device='cuda'\n",
    "):\n",
    "\n",
    "    # 두 모델의 입력 데이터를 받고고 프로젝션 후 pair-wise Cosine Distance를 계산하는 함수\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # model1 \n",
    "        if is_model1_text:\n",
    "            output1 = model1(inputs1)  \n",
    "        else:\n",
    "            output1 = model1(inputs1.float())  # 이미지는 float\n",
    "        output1 = proj1(output1).cpu().numpy() # Projection Layer\n",
    "\n",
    "        n_pairs = output1.shape[0] // 2\n",
    "        dists1 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output1[i]\n",
    "            yb = output1[i + n_pairs]\n",
    "            dist = spatial.distance.cosine(ya, yb) # Cosine Distance 계산\n",
    "            dists1.append(dist)\n",
    "\n",
    "        # model2\n",
    "        if is_model2_text:\n",
    "            output2 = model2(inputs2)  \n",
    "        else:\n",
    "            output2 = model2(inputs2.float())  # 이미지는 float\n",
    "        output2 = proj2(output2).cpu().numpy() # Projection Layer\n",
    "\n",
    "        dists2 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output2[i]\n",
    "            yb = output2[i + n_pairs]\n",
    "            dist = spatial.distance.cosine(ya, yb) # Cosine Distance 계산\n",
    "            dists2.append(dist)\n",
    "\n",
    "    return np.array(dists1), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDV 계산: Euclidean Distance \n",
    "def compute_ddv_euc(\n",
    "    model1, model2,\n",
    "    inputs1, inputs2,\n",
    "    proj1, proj2,\n",
    "    is_model1_text=False,\n",
    "    is_model2_text=False,\n",
    "    device='cuda'\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        # model1\n",
    "        if is_model1_text:\n",
    "            output1 = model1(inputs1)        \n",
    "        else:\n",
    "            output1 = model1(inputs1.float()) \n",
    "        output1 = proj1(output1).cpu().numpy()\n",
    "\n",
    "        n_pairs = output1.shape[0] // 2\n",
    "        dists1 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output1[i]\n",
    "            yb = output1[i + n_pairs]\n",
    "            dist = spatial.distance.euclidean(ya, yb)\n",
    "            dists1.append(dist)\n",
    "\n",
    "        # model2\n",
    "        if is_model2_text:\n",
    "            output2 = model2(inputs2)\n",
    "        else:\n",
    "            output2 = model2(inputs2.float())\n",
    "        output2 = proj2(output2).cpu().numpy()\n",
    "\n",
    "        dists2 = []\n",
    "        for i in range(n_pairs):\n",
    "            ya = output2[i]\n",
    "            yb = output2[i + n_pairs]\n",
    "            dist = spatial.distance.euclidean(ya, yb)\n",
    "            dists2.append(dist)\n",
    "\n",
    "    return np.array(dists1), np.array(dists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Distance 계산 함수\n",
    "def compute_sim_cos(ddv1, ddv2):\n",
    "    return spatial.distance.cosine(ddv1, ddv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelDiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= TARGET MODEL: 0 =================\n",
      "[task_id 1 => target_id 0] DDV cos-cos: 0.24807, DDV euc-cos: 0.15501\n",
      "[task_id 2 => target_id 0] DDV cos-cos: 0.25558, DDV euc-cos: 0.16784\n",
      "[task_id 3 => target_id 0] DDV cos-cos: 0.26441, DDV euc-cos: 0.23380\n",
      "[task_id 4 => target_id 0] DDV cos-cos: 0.26474, DDV euc-cos: 0.22376\n",
      "[task_id 5 => target_id 0] DDV cos-cos: 0.34324, DDV euc-cos: 0.23314\n",
      "[task_id 6 => target_id 0] DDV cos-cos: 0.26272, DDV euc-cos: 0.17879\n",
      "[task_id 7 => target_id 0] DDV cos-cos: 0.29565, DDV euc-cos: 0.16340\n",
      "[task_id 8 => target_id 0] DDV cos-cos: 0.21350, DDV euc-cos: 0.13613\n",
      "[task_id 9 => target_id 0] DDV cos-cos: 0.31290, DDV euc-cos: 0.19864\n",
      "[task_id 10 => target_id 0] DDV cos-cos: 0.44360, DDV euc-cos: 0.29714\n",
      "[task_id 11 => target_id 0] DDV cos-cos: 0.65315, DDV euc-cos: 0.42663\n",
      "\n",
      "================= TARGET MODEL: 1 =================\n",
      "[task_id 0 => target_id 1] DDV cos-cos: 0.24807, DDV euc-cos: 0.15501\n",
      "[task_id 2 => target_id 1] DDV cos-cos: 0.34595, DDV euc-cos: 0.26451\n",
      "[task_id 3 => target_id 1] DDV cos-cos: 0.26152, DDV euc-cos: 0.19815\n",
      "[task_id 4 => target_id 1] DDV cos-cos: 0.40054, DDV euc-cos: 0.30480\n",
      "[task_id 5 => target_id 1] DDV cos-cos: 0.14462, DDV euc-cos: 0.13168\n",
      "[task_id 6 => target_id 1] DDV cos-cos: 0.30045, DDV euc-cos: 0.16033\n",
      "[task_id 7 => target_id 1] DDV cos-cos: 0.24088, DDV euc-cos: 0.15049\n",
      "[task_id 8 => target_id 1] DDV cos-cos: 0.52730, DDV euc-cos: 0.26982\n",
      "[task_id 9 => target_id 1] DDV cos-cos: 0.30856, DDV euc-cos: 0.18260\n",
      "[task_id 10 => target_id 1] DDV cos-cos: 0.27657, DDV euc-cos: 0.20504\n",
      "[task_id 11 => target_id 1] DDV cos-cos: 0.37387, DDV euc-cos: 0.22310\n",
      "\n",
      "================= TARGET MODEL: 2 =================\n",
      "[task_id 0 => target_id 2] DDV cos-cos: 0.25558, DDV euc-cos: 0.16784\n",
      "[task_id 1 => target_id 2] DDV cos-cos: 0.34595, DDV euc-cos: 0.26451\n",
      "[task_id 3 => target_id 2] DDV cos-cos: 0.30549, DDV euc-cos: 0.23999\n",
      "[task_id 4 => target_id 2] DDV cos-cos: 0.32614, DDV euc-cos: 0.26385\n",
      "[task_id 5 => target_id 2] DDV cos-cos: 0.27388, DDV euc-cos: 0.19838\n",
      "[task_id 6 => target_id 2] DDV cos-cos: 0.38682, DDV euc-cos: 0.31017\n",
      "[task_id 7 => target_id 2] DDV cos-cos: 0.26476, DDV euc-cos: 0.13323\n",
      "[task_id 8 => target_id 2] DDV cos-cos: 0.33768, DDV euc-cos: 0.20317\n",
      "[task_id 9 => target_id 2] DDV cos-cos: 0.26826, DDV euc-cos: 0.16414\n",
      "[task_id 10 => target_id 2] DDV cos-cos: 0.50878, DDV euc-cos: 0.34327\n",
      "[task_id 11 => target_id 2] DDV cos-cos: 0.71245, DDV euc-cos: 0.45961\n",
      "\n",
      "================= TARGET MODEL: 3 =================\n",
      "[task_id 0 => target_id 3] DDV cos-cos: 0.26441, DDV euc-cos: 0.23380\n",
      "[task_id 1 => target_id 3] DDV cos-cos: 0.26152, DDV euc-cos: 0.19815\n",
      "[task_id 2 => target_id 3] DDV cos-cos: 0.30549, DDV euc-cos: 0.23999\n",
      "[task_id 4 => target_id 3] DDV cos-cos: 0.27621, DDV euc-cos: 0.19958\n",
      "[task_id 5 => target_id 3] DDV cos-cos: 0.27291, DDV euc-cos: 0.22140\n",
      "[task_id 6 => target_id 3] DDV cos-cos: 0.24985, DDV euc-cos: 0.19657\n",
      "[task_id 7 => target_id 3] DDV cos-cos: 0.48854, DDV euc-cos: 0.23423\n",
      "[task_id 8 => target_id 3] DDV cos-cos: 0.41206, DDV euc-cos: 0.23193\n",
      "[task_id 9 => target_id 3] DDV cos-cos: 0.20899, DDV euc-cos: 0.13373\n",
      "[task_id 10 => target_id 3] DDV cos-cos: 0.44775, DDV euc-cos: 0.30578\n",
      "[task_id 11 => target_id 3] DDV cos-cos: 0.44356, DDV euc-cos: 0.26170\n",
      "\n",
      "================= TARGET MODEL: 4 =================\n",
      "[task_id 0 => target_id 4] DDV cos-cos: 0.26474, DDV euc-cos: 0.22376\n",
      "[task_id 1 => target_id 4] DDV cos-cos: 0.40054, DDV euc-cos: 0.30480\n",
      "[task_id 2 => target_id 4] DDV cos-cos: 0.32614, DDV euc-cos: 0.26385\n",
      "[task_id 3 => target_id 4] DDV cos-cos: 0.27621, DDV euc-cos: 0.19958\n",
      "[task_id 5 => target_id 4] DDV cos-cos: 0.35299, DDV euc-cos: 0.27357\n",
      "[task_id 6 => target_id 4] DDV cos-cos: 0.26931, DDV euc-cos: 0.16538\n",
      "[task_id 7 => target_id 4] DDV cos-cos: 0.50181, DDV euc-cos: 0.25058\n",
      "[task_id 8 => target_id 4] DDV cos-cos: 0.25263, DDV euc-cos: 0.15815\n",
      "[task_id 9 => target_id 4] DDV cos-cos: 0.27054, DDV euc-cos: 0.17472\n",
      "[task_id 10 => target_id 4] DDV cos-cos: 0.48142, DDV euc-cos: 0.34710\n",
      "[task_id 11 => target_id 4] DDV cos-cos: 0.84394, DDV euc-cos: 0.55828\n",
      "\n",
      "================= TARGET MODEL: 5 =================\n",
      "[task_id 0 => target_id 5] DDV cos-cos: 0.34324, DDV euc-cos: 0.23314\n",
      "[task_id 1 => target_id 5] DDV cos-cos: 0.14462, DDV euc-cos: 0.13168\n",
      "[task_id 2 => target_id 5] DDV cos-cos: 0.27388, DDV euc-cos: 0.19838\n",
      "[task_id 3 => target_id 5] DDV cos-cos: 0.27291, DDV euc-cos: 0.22140\n",
      "[task_id 4 => target_id 5] DDV cos-cos: 0.35299, DDV euc-cos: 0.27357\n",
      "[task_id 6 => target_id 5] DDV cos-cos: 0.38521, DDV euc-cos: 0.27597\n",
      "[task_id 7 => target_id 5] DDV cos-cos: 0.30092, DDV euc-cos: 0.14365\n",
      "[task_id 8 => target_id 5] DDV cos-cos: 0.44005, DDV euc-cos: 0.24037\n",
      "[task_id 9 => target_id 5] DDV cos-cos: 0.21206, DDV euc-cos: 0.15205\n",
      "[task_id 10 => target_id 5] DDV cos-cos: 0.21853, DDV euc-cos: 0.16015\n",
      "[task_id 11 => target_id 5] DDV cos-cos: 0.44121, DDV euc-cos: 0.29348\n",
      "\n",
      "================= TARGET MODEL: 6 =================\n",
      "[task_id 0 => target_id 6] DDV cos-cos: 0.26272, DDV euc-cos: 0.17879\n",
      "[task_id 1 => target_id 6] DDV cos-cos: 0.30045, DDV euc-cos: 0.16033\n",
      "[task_id 2 => target_id 6] DDV cos-cos: 0.38682, DDV euc-cos: 0.31017\n",
      "[task_id 3 => target_id 6] DDV cos-cos: 0.24985, DDV euc-cos: 0.19657\n",
      "[task_id 4 => target_id 6] DDV cos-cos: 0.26931, DDV euc-cos: 0.16538\n",
      "[task_id 5 => target_id 6] DDV cos-cos: 0.38521, DDV euc-cos: 0.27597\n",
      "[task_id 7 => target_id 6] DDV cos-cos: 0.64526, DDV euc-cos: 0.33001\n",
      "[task_id 8 => target_id 6] DDV cos-cos: 0.48168, DDV euc-cos: 0.25797\n",
      "[task_id 9 => target_id 6] DDV cos-cos: 0.41127, DDV euc-cos: 0.26165\n",
      "[task_id 10 => target_id 6] DDV cos-cos: 0.63705, DDV euc-cos: 0.42323\n",
      "[task_id 11 => target_id 6] DDV cos-cos: 0.82726, DDV euc-cos: 0.50766\n",
      "\n",
      "================= TARGET MODEL: 7 =================\n",
      "[task_id 0 => target_id 7] DDV cos-cos: 0.29565, DDV euc-cos: 0.16340\n",
      "[task_id 1 => target_id 7] DDV cos-cos: 0.24088, DDV euc-cos: 0.15049\n",
      "[task_id 2 => target_id 7] DDV cos-cos: 0.26476, DDV euc-cos: 0.13323\n",
      "[task_id 3 => target_id 7] DDV cos-cos: 0.48854, DDV euc-cos: 0.23423\n",
      "[task_id 4 => target_id 7] DDV cos-cos: 0.50181, DDV euc-cos: 0.25058\n",
      "[task_id 5 => target_id 7] DDV cos-cos: 0.30092, DDV euc-cos: 0.14365\n",
      "[task_id 6 => target_id 7] DDV cos-cos: 0.64526, DDV euc-cos: 0.33001\n",
      "[task_id 8 => target_id 7] DDV cos-cos: 0.47903, DDV euc-cos: 0.19608\n",
      "[task_id 9 => target_id 7] DDV cos-cos: 0.24907, DDV euc-cos: 0.07335\n",
      "[task_id 10 => target_id 7] DDV cos-cos: 0.25069, DDV euc-cos: 0.12272\n",
      "[task_id 11 => target_id 7] DDV cos-cos: 0.55612, DDV euc-cos: 0.27822\n",
      "\n",
      "================= TARGET MODEL: 8 =================\n",
      "[task_id 0 => target_id 8] DDV cos-cos: 0.21350, DDV euc-cos: 0.13613\n",
      "[task_id 1 => target_id 8] DDV cos-cos: 0.52730, DDV euc-cos: 0.26982\n",
      "[task_id 2 => target_id 8] DDV cos-cos: 0.33768, DDV euc-cos: 0.20317\n",
      "[task_id 3 => target_id 8] DDV cos-cos: 0.41206, DDV euc-cos: 0.23193\n",
      "[task_id 4 => target_id 8] DDV cos-cos: 0.25263, DDV euc-cos: 0.15815\n",
      "[task_id 5 => target_id 8] DDV cos-cos: 0.44005, DDV euc-cos: 0.24037\n",
      "[task_id 6 => target_id 8] DDV cos-cos: 0.48168, DDV euc-cos: 0.25797\n",
      "[task_id 7 => target_id 8] DDV cos-cos: 0.47903, DDV euc-cos: 0.19608\n",
      "[task_id 9 => target_id 8] DDV cos-cos: 0.39115, DDV euc-cos: 0.21319\n",
      "[task_id 10 => target_id 8] DDV cos-cos: 0.56352, DDV euc-cos: 0.32391\n",
      "[task_id 11 => target_id 8] DDV cos-cos: 0.86176, DDV euc-cos: 0.52512\n",
      "\n",
      "================= TARGET MODEL: 9 =================\n",
      "[task_id 0 => target_id 9] DDV cos-cos: 0.31290, DDV euc-cos: 0.19864\n",
      "[task_id 1 => target_id 9] DDV cos-cos: 0.30856, DDV euc-cos: 0.18260\n",
      "[task_id 2 => target_id 9] DDV cos-cos: 0.26826, DDV euc-cos: 0.16414\n",
      "[task_id 3 => target_id 9] DDV cos-cos: 0.20899, DDV euc-cos: 0.13373\n",
      "[task_id 4 => target_id 9] DDV cos-cos: 0.27054, DDV euc-cos: 0.17472\n",
      "[task_id 5 => target_id 9] DDV cos-cos: 0.21206, DDV euc-cos: 0.15205\n",
      "[task_id 6 => target_id 9] DDV cos-cos: 0.41127, DDV euc-cos: 0.26165\n",
      "[task_id 7 => target_id 9] DDV cos-cos: 0.24907, DDV euc-cos: 0.07335\n",
      "[task_id 8 => target_id 9] DDV cos-cos: 0.39115, DDV euc-cos: 0.21319\n",
      "[task_id 10 => target_id 9] DDV cos-cos: 0.31842, DDV euc-cos: 0.23189\n",
      "[task_id 11 => target_id 9] DDV cos-cos: 0.51678, DDV euc-cos: 0.25081\n",
      "\n",
      "================= TARGET MODEL: 10 =================\n",
      "[task_id 0 => target_id 10] DDV cos-cos: 0.44360, DDV euc-cos: 0.29714\n",
      "[task_id 1 => target_id 10] DDV cos-cos: 0.27657, DDV euc-cos: 0.20504\n",
      "[task_id 2 => target_id 10] DDV cos-cos: 0.50878, DDV euc-cos: 0.34327\n",
      "[task_id 3 => target_id 10] DDV cos-cos: 0.44775, DDV euc-cos: 0.30578\n",
      "[task_id 4 => target_id 10] DDV cos-cos: 0.48142, DDV euc-cos: 0.34710\n",
      "[task_id 5 => target_id 10] DDV cos-cos: 0.21853, DDV euc-cos: 0.16015\n",
      "[task_id 6 => target_id 10] DDV cos-cos: 0.63705, DDV euc-cos: 0.42323\n",
      "[task_id 7 => target_id 10] DDV cos-cos: 0.25069, DDV euc-cos: 0.12272\n",
      "[task_id 8 => target_id 10] DDV cos-cos: 0.56352, DDV euc-cos: 0.32391\n",
      "[task_id 9 => target_id 10] DDV cos-cos: 0.31842, DDV euc-cos: 0.23189\n",
      "[task_id 11 => target_id 10] DDV cos-cos: 0.38932, DDV euc-cos: 0.26708\n",
      "\n",
      "================= TARGET MODEL: 11 =================\n",
      "[task_id 0 => target_id 11] DDV cos-cos: 0.65315, DDV euc-cos: 0.42663\n",
      "[task_id 1 => target_id 11] DDV cos-cos: 0.37387, DDV euc-cos: 0.22310\n",
      "[task_id 2 => target_id 11] DDV cos-cos: 0.71245, DDV euc-cos: 0.45961\n",
      "[task_id 3 => target_id 11] DDV cos-cos: 0.44356, DDV euc-cos: 0.26170\n",
      "[task_id 4 => target_id 11] DDV cos-cos: 0.84394, DDV euc-cos: 0.55828\n",
      "[task_id 5 => target_id 11] DDV cos-cos: 0.44121, DDV euc-cos: 0.29348\n",
      "[task_id 6 => target_id 11] DDV cos-cos: 0.82726, DDV euc-cos: 0.50766\n",
      "[task_id 7 => target_id 11] DDV cos-cos: 0.55612, DDV euc-cos: 0.27822\n",
      "[task_id 8 => target_id 11] DDV cos-cos: 0.86176, DDV euc-cos: 0.52512\n",
      "[task_id 9 => target_id 11] DDV cos-cos: 0.51678, DDV euc-cos: 0.25081\n",
      "[task_id 10 => target_id 11] DDV cos-cos: 0.38932, DDV euc-cos: 0.26708\n"
     ]
    }
   ],
   "source": [
    "ddvcc_list_all = []  # (target_id, task_id, cos-distance) \n",
    "ddvec_list_all = []  # (target_id, task_id, euc-distance) \n",
    "\n",
    "\n",
    "# target_id vs. task_id\n",
    "for target_id in range(12):\n",
    "    print(f\"\\n================= TARGET MODEL: {target_id} =================\")\n",
    "    \n",
    "    is_target_text = (target_id < 6) # Target 모델이 텍스트 모델인지 확인\n",
    "\n",
    "    for task_id in range(12):\n",
    "        if task_id == target_id: # 동일 모델은 skip\n",
    "            continue\n",
    "\n",
    "        is_task_text = (task_id < 6) # task 모델이 텍스트 모델인지 확인\n",
    "\n",
    "        # target\n",
    "        batch_target = next(iter(valid_loaders[target_id]))\n",
    "        if is_target_text:\n",
    "            # 텍스트 (B, seq_len)\n",
    "            inputs_target = batch_target['input_ids'].to(DEVICE).long()\n",
    "        else:\n",
    "            # 이미지 (B, T, F)\n",
    "            inputs_target = batch_target[0].to(DEVICE)\n",
    "\n",
    "        # task \n",
    "        batch_task = next(iter(valid_loaders[task_id]))\n",
    "        if is_task_text:\n",
    "            inputs_task = batch_task['input_ids'].to(DEVICE).long()\n",
    "        else:\n",
    "            inputs_task = batch_task[0].to(DEVICE)\n",
    "\n",
    "        # DDV Cosine \n",
    "        ddv1, ddv2 = compute_ddv_cos(\n",
    "            model1=input_models[target_id],\n",
    "            model2=input_models[task_id],\n",
    "            inputs1=inputs_target,\n",
    "            inputs2=inputs_task,\n",
    "            proj1=(text_projection if is_target_text else image_projection),\n",
    "            proj2=(text_projection if is_task_text else image_projection),\n",
    "            is_model1_text=is_target_text,\n",
    "            is_model2_text=is_task_text,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        ddv_distance_cos = compute_sim_cos(ddv1, ddv2)  # cos-distance\n",
    "\n",
    "        # DDV Euclidean \n",
    "        ddv1, ddv2 = compute_ddv_euc(\n",
    "            model1=input_models[target_id],\n",
    "            model2=input_models[task_id],\n",
    "            inputs1=inputs_target,\n",
    "            inputs2=inputs_task,\n",
    "            proj1=(text_projection if is_target_text else image_projection),\n",
    "            proj2=(text_projection if is_task_text else image_projection),\n",
    "            is_model1_text=is_target_text,\n",
    "            is_model2_text=is_task_text,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        ddv_distance_euc = compute_sim_cos(ddv1, ddv2)  # euc-distance\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"[task_id {task_id} => target_id {target_id}] \"\n",
    "              f\"DDV cos-cos: {ddv_distance_cos:.5f}, \"\n",
    "              f\"DDV euc-cos: {ddv_distance_euc:.5f}\")\n",
    "\n",
    "        # 리스트에 기록\n",
    "        ddvcc_list_all.append((target_id, task_id, ddv_distance_cos))\n",
    "        ddvec_list_all.append((target_id, task_id, ddv_distance_euc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task_id별 best target_id 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Best target_id by cos-distance for each task_id =====\n",
      "- task_id 0: best target_id = 8, cos_distance = 0.21350\n",
      "- task_id 1: best target_id = 5, cos_distance = 0.14462\n",
      "- task_id 2: best target_id = 0, cos_distance = 0.25558\n",
      "- task_id 3: best target_id = 9, cos_distance = 0.20899\n",
      "- task_id 4: best target_id = 8, cos_distance = 0.25263\n",
      "- task_id 5: best target_id = 1, cos_distance = 0.14462\n",
      "- task_id 6: best target_id = 3, cos_distance = 0.24985\n",
      "- task_id 7: best target_id = 1, cos_distance = 0.24088\n",
      "- task_id 8: best target_id = 0, cos_distance = 0.21350\n",
      "- task_id 9: best target_id = 3, cos_distance = 0.20899\n",
      "- task_id 10: best target_id = 5, cos_distance = 0.21853\n",
      "- task_id 11: best target_id = 1, cos_distance = 0.37387\n",
      "\n",
      "===== Best target_id by euc-distance for each task_id =====\n",
      "- task_id 0: best target_id = 8, euc_distance = 0.13613\n",
      "- task_id 1: best target_id = 5, euc_distance = 0.13168\n",
      "- task_id 2: best target_id = 7, euc_distance = 0.13323\n",
      "- task_id 3: best target_id = 9, euc_distance = 0.13373\n",
      "- task_id 4: best target_id = 8, euc_distance = 0.15815\n",
      "- task_id 5: best target_id = 1, euc_distance = 0.13168\n",
      "- task_id 6: best target_id = 1, euc_distance = 0.16033\n",
      "- task_id 7: best target_id = 9, euc_distance = 0.07335\n",
      "- task_id 8: best target_id = 0, euc_distance = 0.13613\n",
      "- task_id 9: best target_id = 7, euc_distance = 0.07335\n",
      "- task_id 10: best target_id = 7, euc_distance = 0.12272\n",
      "- task_id 11: best target_id = 1, euc_distance = 0.22310\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# cos-distance 기준\n",
    "\n",
    "data_by_task_cos = defaultdict(list)\n",
    "\n",
    "for (t_id, s_id, dist) in ddvcc_list_all:\n",
    "    data_by_task_cos[s_id].append((t_id, dist))\n",
    "\n",
    "# task_id마다 distance가 가장 작은 target_id 찾음\n",
    "best_target_for_task_cos = {}\n",
    "for s_id, pairs in data_by_task_cos.items():\n",
    "    best_t_id = None\n",
    "    min_dist = float(\"inf\")\n",
    "    for t_id, dist in pairs:\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_t_id = t_id\n",
    "    best_target_for_task_cos[s_id] = (best_t_id, min_dist)\n",
    "\n",
    "print(\"\\n===== Best target_id by cos-distance for each task_id =====\")\n",
    "for s_id in sorted(best_target_for_task_cos.keys()):\n",
    "    t_id, dist = best_target_for_task_cos[s_id]\n",
    "    print(f\"- task_id {s_id}: best target_id = {t_id}, cos_distance = {dist:.5f}\")\n",
    "\n",
    "\n",
    "# euc-distance 기준\n",
    "data_by_task_euc = defaultdict(list)\n",
    "for (t_id, s_id, dist) in ddvec_list_all:\n",
    "    data_by_task_euc[s_id].append((t_id, dist))\n",
    "\n",
    "best_target_for_task_euc = {}\n",
    "for s_id, pairs in data_by_task_euc.items():\n",
    "    best_t_id = None\n",
    "    min_dist = float(\"inf\")\n",
    "    for t_id, dist in pairs:\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_t_id = t_id\n",
    "    best_target_for_task_euc[s_id] = (best_t_id, min_dist)\n",
    "\n",
    "print(\"\\n===== Best target_id by euc-distance for each task_id =====\")\n",
    "for s_id in sorted(best_target_for_task_euc.keys()):\n",
    "    t_id, dist = best_target_for_task_euc[s_id]\n",
    "    print(f\"- task_id {s_id}: best target_id = {t_id}, euc_distance = {dist:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
