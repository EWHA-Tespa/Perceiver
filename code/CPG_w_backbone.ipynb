{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import copy, math\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import models.shared_perceiver as sp\n",
    "import models.layers as nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torchë¥¼ ê±°ì¹˜ëŠ” ëª¨ë“  ë‚œìˆ˜ë“¤ì˜ ìƒì„±ìˆœì„œë¥¼ ê³ ì •í•œë‹¤\n",
    "    torch.cuda.manual_seed(seed) #cudaë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ì†Œë“œë“¤ì˜ ë‚œìˆ˜ì‹œë“œëŠ” ë”°ë¡œ ê³ ì •í•´ì¤˜ì•¼í•œë‹¤ \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #ë”¥ëŸ¬ë‹ì— íŠ¹í™”ëœ CuDNNì˜ ë‚œìˆ˜ì‹œë“œë„ ê³ ì • \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpyë¥¼ ì‚¬ìš©í•  ê²½ìš° ê³ ì •\n",
    "    random.seed(seed) #íŒŒì´ì¬ ìì²´ ëª¨ë“ˆ random ëª¨ë“ˆì˜ ì‹œë“œ ê³ ì •\n",
    "\n",
    "def seed_worker(worker_id): #ë°ì´í„°ë¡œë” ë‚œìˆ˜ê³ ì •\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "NUM_WORKERS = 4 # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ê´€ë¦¬ì ìˆ˜. ë‚œìˆ˜ìƒì„±ê³¼ ê´€ë ¨ìˆìŠµë‹ˆë‹¤. ì¼ë‹¨ì€ 4ë¡œ ê³ ì •í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_EPOCHS = 50\n",
    "NETWORK_WIDTH_MULTIPLIER = 1.0\n",
    "MAX_NETWORK_WIDTH_MULTIPLIER = 2.0\n",
    "\n",
    "LR = 1e-2\n",
    "LR_MASK = 1e-4\n",
    "WEIGHT_DECAY = 4e-5\n",
    "BATCH_SIZE = 32\n",
    "TOTAL_NUM_TASKS = 6\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "EMBED_DIM = 128\n",
    "LATENT_DIM = 64\n",
    "LATENT_SIZE = 64\n",
    "NUM_BLOCKS = 4\n",
    "\n",
    "\n",
    "task_id = 1\n",
    "target_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/youlee/n24news/n24news'\n",
    "\n",
    "file_path = '/home/Minju/Perceiver'\n",
    "\n",
    "groups_path = data_path + '/captions_and_labels.csv'\n",
    "\n",
    "model_path = file_path + '/shared_layer_model/'\n",
    "loader_path = file_path + '/shared_layer_loader/'\n",
    "\n",
    "save_folder = file_path + '/finetune/'\n",
    "load_folder = file_path + f'/{task_id}/{target_id}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ ê·¸ë£¹ë³„ CSV íŒŒì¼ ê²½ë¡œ:\n",
      "/home/Minju/Perceiver/regroup/regroup_1.csv\n",
      "/home/Minju/Perceiver/regroup/regroup_2.csv\n",
      "/home/Minju/Perceiver/regroup/regroup_3.csv\n",
      "/home/Minju/Perceiver/regroup/regroup_4.csv\n",
      "/home/Minju/Perceiver/regroup/regroup_5.csv\n",
      "/home/Minju/Perceiver/regroup/regroup_6.csv\n"
     ]
    }
   ],
   "source": [
    "groups_df = pd.read_csv(groups_path)\n",
    "\n",
    "groups = [\n",
    "    [\"Opinion\", \"Art & Design\", \"Television\"],\n",
    "    [\"Music\", \"Travel\", \"Real Estate\"],\n",
    "    [\"Books\", \"Theater\", \"Health\"],\n",
    "    [\"Sports\", \"Science\", \"Food\"],\n",
    "    [\"Fashion & Style\", \"Movies\", \"Technology\"],\n",
    "    [\"Dance\", \"Media\", \"Style\"]\n",
    "]\n",
    "\n",
    "output_paths = []\n",
    "for i, group_labels in enumerate(groups, 1):\n",
    "    group_data = groups_df[groups_df['Label'].isin(group_labels)]\n",
    "    output_path = file_path + f'/regroup/regroup_{i}.csv'\n",
    "    group_data.to_csv(output_path, index=False)\n",
    "    output_paths.append(output_path)\n",
    "\n",
    "print(\"ìƒì„±ëœ ê·¸ë£¹ë³„ CSV íŒŒì¼ ê²½ë¡œ:\")\n",
    "for path in output_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(model_path + (f'text_model_{target_id%6}.pkl' if target_id < 6 else f'image_model_{target_id}_fold1.pkl'))\n",
    "# #if isinstance(checkpoint, sp.CombinedModel):\n",
    "    \n",
    "# checkpoint = checkpoint.state_dict()\n",
    "\n",
    "# model = sp.Perceiver(input_dim=770, latent_dim=64, \n",
    "#                      latent_size=64, num_classes=NUM_CLASSES,\n",
    "#                    num_blocks=4, self_attn_layers_per_block=10)\n",
    "\n",
    "# model = sp.CombinedModel(vocab_size=tokenizer.vocab_size, embed_dim=128, perceiver_model=model)\n",
    "\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1728028/4201899745.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path + (f'text_model_{target_id%6}.pkl' if target_id < 6 else f'image_model_{target_id}_fold1.pkl'))\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_path + (f'text_model_{target_id%6}.pkl' if target_id < 6 else f'image_model_{target_id}_fold1.pkl'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     \"\"\"batchë¥¼ PyTorch Tensorë¡œ ë³€í™˜\"\"\"\n",
    "#     input_ids = torch.stack([torch.tensor(b['input_ids']) for b in batch])\n",
    "#     attention_masks = torch.stack([torch.tensor(b['attention_mask']) for b in batch])\n",
    "#     labels = torch.tensor([b['labels'] for b in batch])  # labelsë„ í…ì„œë¡œ ë³€í™˜\n",
    "#     return {'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "group 1 ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "group 2 ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "group 3 ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "group 4 ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "group 5 ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "group 6 ì²˜ë¦¬ ì¤‘...\n"
     ]
    }
   ],
   "source": [
    "for idx, group_file in enumerate(output_paths, start=1):\n",
    "    print(f\"\\ngroup {idx} ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "    df = pd.read_csv(group_file)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    input_ids, attention_masks = sp.tokenize_data(df, tokenizer=tokenizer, MAX_LENGTH=MAX_LENGTH)\n",
    "    labels = torch.tensor(df['Label'].values)\n",
    "\n",
    "    dataset = sp.CustomDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    indices = torch.randperm(len(dataset))\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=g)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if task_id < 6:\n",
    "#     dataset = sp.CustomDataset(input_ids, attention_masks, labels)\n",
    "#     indices = torch.randperm(len(dataset))\n",
    "\n",
    "#     train_size = int(0.8 * len(dataset))\n",
    "#     test_size = len(dataset) - train_size\n",
    "\n",
    "#     train_idx = indices[:train_size]\n",
    "#     test_idx = indices[train_size:]\n",
    "\n",
    "#     train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "#     test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "#     train_loader = DataLoader(train_subset, batch_size=32, shuffle=True,\n",
    "#                               num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=g)\n",
    "#     test_loader = DataLoader(test_subset, batch_size=32, shuffle=False,\n",
    "#                              num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train dataset ìƒ˜í”Œ ì˜ˆì‹œ: <torch.utils.data.dataset.Subset object at 0x719de0748f10>\n"
     ]
    }
   ],
   "source": [
    "print(f\"âœ… Train dataset ìƒ˜í”Œ ì˜ˆì‹œ: {train_subset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = dataset.load_cifar100_train(DATASET_DIR, batch_size)\n",
    "# val_loader = dataset.load_cifar100_valid(VALIDSET_DIR, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(FINETUNE_EPOCHS):\n\u001b[0;32m----> 3\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, test_loader, criterion)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINETUNE_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/Minju/Perceiver/code/models/shared_perceiver.py:206\u001b[0m, in \u001b[0;36mPerceiver.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    x: (B, T, F) = (ë°°ì¹˜, ì‹œí€€ìŠ¤ê¸¸ì´, í”¼ì²˜ì°¨ì›)\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     B, T, F \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    207\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(x)                 \u001b[38;5;66;03m# (B, T, latent_dim)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# latents: (latent_size, latent_dim) -> ë°°ì¹˜ ì°¨ì› í™•ì¥ (B, latent_size, latent_dim)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = eval_epoch(model, test_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{FINETUNE_EPOCHS}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}\")\n",
    "    print(f\"                  Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # ìµœê³  ì •í™•ë„ ëª¨ë¸ ì €ì¥\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, \"fine_tuned_model.pth.tar\"))\n",
    "\n",
    "print(f\"ğŸ”¹ Fine-tuning ì™„ë£Œ! ìµœê³  ì •í™•ë„: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print(f\"ğŸš€ input_ids.shape: {input_ids.shape}\") \u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# CombinedModelì˜ forward í˜¸ì¶œ\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print(f\"ğŸš€ outputs.shape: {outputs.shape}\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/Minju/Perceiver/code/models/shared_perceiver.py:206\u001b[0m, in \u001b[0;36mPerceiver.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    x: (B, T, F) = (ë°°ì¹˜, ì‹œí€€ìŠ¤ê¸¸ì´, í”¼ì²˜ì°¨ì›)\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     B, T, F \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    207\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(x)                 \u001b[38;5;66;03m# (B, T, latent_dim)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# latents: (latent_size, latent_dim) -> ë°°ì¹˜ ì°¨ì› í™•ì¥ (B, latent_size, latent_dim)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# for epoch in range(FINETUNE_EPOCHS):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "    # for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{FINETUNE_EPOCHS}\"):\n",
    "    #     inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs = model(inputs)\n",
    "    #     loss = criterion(outputs, labels)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "# for batch in dataloader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)  \n",
    "#         labels = batch['labels'].to(device)\n",
    "#         #print(f\"ğŸš€ input_ids.shape: {input_ids.shape}\") \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids)  # CombinedModelì˜ forward í˜¸ì¶œ\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         #print(f\"ğŸš€ outputs.shape: {outputs.shape}\")\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     inputs = batch['input_ids'].to(device)\n",
    "#     labels = batch['labels'].to(device)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs)  # ğŸ”¹ attention_mask ì œê±°\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     running_loss += loss.item()\n",
    "#     _, predicted = outputs.max(1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#     train_acc = 100. * correct / total\n",
    "#     logging.info(f\"Epoch [{epoch+1}/{FINETUNE_EPOCHS}], Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     inputs = batch['input_ids'].to(device)\n",
    "#     attention_masks = batch['attention_mask'].to(device)\n",
    "#     labels = batch['labels'].to(device)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs)  # ëª¨ë¸ ì…ë ¥ ë°©ì‹ì— ë”°ë¼ ìˆ˜ì •\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     running_loss += loss.item()\n",
    "#     _, predicted = outputs.max(1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#     train_acc = 100. * correct / total\n",
    "#     logging.info(f\"Epoch [{epoch+1}/{FINETUNE_EPOCHS}], Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    torch.save({'state_dict': model.state_dict()}, os.path.join(save_folder, \"fine_tuned_model.pth.tar\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
