{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img_tensor, crop_size=32):\n",
    "    C, H, W = img_tensor.shape\n",
    "    img_tensor = img_tensor[:, crop_size:H-crop_size, crop_size:W-crop_size]\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(img_tensor, patch_size=16):\n",
    "    \"\"\"\n",
    "    img_tensor: (C, H, W) 형태 (예: (3, 224, 224))\n",
    "    patch_size: 패치 크기 (16, 16)\n",
    "    return: (num_patches, patch_dim)\n",
    "            예) (196, 768)  # (H/16)*(W/16)=14*14=196, 768=16*16*3\n",
    "    \"\"\"\n",
    "    C, H, W = img_tensor.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"이미지 크기는 patch_size로 나누어 떨어져야 함\"\n",
    "\n",
    "    # unfold로 (patch_size, patch_size)씩 잘라내기\n",
    "    patches = img_tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    " \n",
    "    patches = patches.permute(1, 3, 0, 2, 4).contiguous()\n",
    "\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    num_patches = num_patches_h * num_patches_w\n",
    "\n",
    "    patches = patches.view(num_patches, -1) \n",
    "    return patches\n",
    "\n",
    "def get_patch_coords(num_patches_h, num_patches_w):\n",
    "\n",
    "    y_coord = torch.linspace(0, 1, steps=num_patches_h)\n",
    "    x_coord = torch.linspace(0, 1, steps=num_patches_w)\n",
    "    grid_y, grid_x = torch.meshgrid(y_coord, x_coord, indexing='ij')  # (14,14) each\n",
    "\n",
    "    coords = torch.stack([grid_x, grid_y], dim=-1)  # (14,14,2)\n",
    "    coords = coords.view(-1, 2)                     # (196, 2)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, crop_size=32, patch_size=16):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.crop_size = crop_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # 데이터 및 레이블 추출\n",
    "        labels = []\n",
    "        for folder in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "                if len(image_files) == 1:\n",
    "                    image_path = image_files[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"폴더 {folder}에 JPG 파일이 하나가 아닙니다.\")\n",
    "\n",
    "                label_path = os.path.join(folder_path, \"label.txt\")\n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, \"r\") as f:\n",
    "                        label = f.read().strip()\n",
    "                        labels.append(label)\n",
    "                        self.data.append((image_path, label))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"폴더 {folder}에 label.txt가 없습니다.\")\n",
    "\n",
    "        self.label_encoder.fit(labels)\n",
    "        self.data = [(image_path, self.label_encoder.transform([label])[0]) \n",
    "                     for image_path, label in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # (3, 224, 224)\n",
    "        image = crop(image, crop_size=self.crop_size)\n",
    "        patches = patchify(image, patch_size=self.patch_size)  # (num_patches, patch_dim)\n",
    "\n",
    "        H, W = image.shape[1], image.shape[2]  \n",
    "        num_patches_h = H // self.patch_size   \n",
    "        num_patches_w = W // self.patch_size   \n",
    "        coords = get_patch_coords(num_patches_h, num_patches_w)  \n",
    "\n",
    "        combined = torch.cat([patches, coords], dim=1)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return combined, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    - Cross Attention (latents -> x)\n",
    "    - 이어서 Self Attention (latent들 끼리)\n",
    "    - 보통은 LayerNorm, MLP(FeedForward) 등을 곁들여 residual branch를 구성\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_heads=8, self_attn_layers=1):\n",
    "        super().__init__()\n",
    "        # Cross Attention\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=n_heads)\n",
    "        self.cross_ln = nn.LayerNorm(latent_dim)  # 잊지 말고 layernorm\n",
    "\n",
    "        # Self Attention 여러 층\n",
    "        self.self_attn_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=latent_dim, nhead=n_heads)\n",
    "            for _ in range(self_attn_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, latents, x):\n",
    "        # latents, x: (T, B, dim) 형태로 가정 (주의!)\n",
    "        # Perceiver 원리상 latents는 query, x는 key/value\n",
    "\n",
    "        # 1) Cross Attention\n",
    "        updated_latents, _ = self.cross_attn(latents, x, x)\n",
    "        latents = latents + updated_latents        # Residual\n",
    "        latents = self.cross_ln(latents)           # LayerNorm\n",
    "\n",
    "        # 2) Self Attention 반복\n",
    "        for layer in self.self_attn_layers:\n",
    "            latents = layer(latents)  # 내부적으로 residual/LayerNorm 포함\n",
    "\n",
    "        return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, latent_size, num_classes,\n",
    "                 num_blocks, self_attn_layers_per_block=1):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(latent_size, latent_dim))\n",
    "        self.input_projection = nn.Linear(input_dim, latent_dim)\n",
    "\n",
    "        # 반복될 PerceiverBlock을 여러 개 쌓는다.\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PerceiverBlock(\n",
    "                latent_dim=latent_dim,\n",
    "                n_heads=8,\n",
    "                self_attn_layers=self_attn_layers_per_block\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) = (배치, 시퀀스길이, 피처차원)\n",
    "        \"\"\"\n",
    "        B, T, F = x.size()\n",
    "        x = self.input_projection(x)                 # (B, T, latent_dim)\n",
    "\n",
    "        # latents: (latent_size, latent_dim) -> 배치 차원 확장 (B, latent_size, latent_dim)\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        # MultiHeadAttention은 (seq, batch, dim) 순서를 권장하므로 permute\n",
    "        x = x.permute(1, 0, 2)        # (T, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2)  # (latent_size, B, latent_dim)\n",
    "\n",
    "        # 여러 개의 PerceiverBlock 반복\n",
    "        for block in self.blocks:\n",
    "            latents = block(latents, x)\n",
    "\n",
    "        # 최종 latents: (latent_size, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2).mean(dim=1)  # (B, latent_dim)\n",
    "        return self.output_layer(latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    start = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # GPU로 옮기기\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        accuracy = evaluate_model(model, valid_loader, device=device, log_results=False)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.2f}%\")\n",
    "    end = time.perf_counter()\n",
    "    hour = (end-start) // 3600\n",
    "    min = ((end-start) % 3600) // 60\n",
    "    sec = (end-start) % 60\n",
    "    print(f\"Total Train time: {hour}h {min}m {sec}s\")\n",
    "\n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, log_results=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        start = time.perf_counter()\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        end = time.perf_counter()\n",
    "        hour = (end-start) // 3600\n",
    "        min = ((end-start) % 3600) // 60\n",
    "        sec = (end-start) % 60\n",
    "        print(f\"Elapsed time on CPU: {hour}h {min}m {sec}s\")\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    if log_results:\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_losses, val_accuracies):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Train Loss', color='tab:blue')\n",
    "    ax1.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Validation Accuracy (%)', color='tab:orange')\n",
    "    ax2.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/youlee/n24news/n24news/image'\n",
    "crop_size = 0\n",
    "patch_size = 16\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "group_class = 3\n",
    "\n",
    "dataset = ImageDataset(root_dir=data_dir, transform=transform, crop_size=crop_size, patch_size=patch_size)\n",
    "label_encoder = dataset.label_encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "class_counts = {label:0 for label in class_labels}\n",
    "for _, label in dataset.data:\n",
    "    class_counts[label_encoder.inverse_transform([label])[0]] += 1\n",
    "\n",
    "target_classes = [label for label, count in class_counts.items() if count >= 2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.str_('Real Estate'),\n",
       " np.str_('Opinion'),\n",
       " np.str_('Technology'),\n",
       " np.str_('Style'),\n",
       " np.str_('Books'),\n",
       " np.str_('Music'),\n",
       " np.str_('Health'),\n",
       " np.str_('Science'),\n",
       " np.str_('Sports'),\n",
       " np.str_('Travel'),\n",
       " np.str_('Art & Design'),\n",
       " np.str_('Television'),\n",
       " np.str_('Media'),\n",
       " np.str_('Theater'),\n",
       " np.str_('Dance'),\n",
       " np.str_('Movies'),\n",
       " np.str_('Fashion & Style'),\n",
       " np.str_('Food')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(target_classes)\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 1 시작\n",
      "Selected Feature: [np.str_('Real Estate'), np.str_('Opinion'), np.str_('Technology')]\n",
      "train: 5780, valid: 1446\n",
      "Elapsed time on CPU: 0.0h 0.0m 11.819845393998548s\n",
      "Epoch 1/10, Loss: 1.1184, Val Accuracy: 42.95%\n",
      "Elapsed time on CPU: 0.0h 0.0m 9.84867274100543s\n",
      "Epoch 2/10, Loss: 1.0465, Val Accuracy: 45.71%\n",
      "Elapsed time on CPU: 0.0h 0.0m 10.057482668984449s\n",
      "Epoch 3/10, Loss: 1.0291, Val Accuracy: 45.85%\n",
      "Elapsed time on CPU: 0.0h 0.0m 10.117796637990978s\n",
      "Epoch 4/10, Loss: 1.0204, Val Accuracy: 47.30%\n",
      "Elapsed time on CPU: 0.0h 0.0m 10.074323453009129s\n",
      "Epoch 5/10, Loss: 1.0091, Val Accuracy: 46.89%\n",
      "Elapsed time on CPU: 0.0h 0.0m 9.944024508004077s\n",
      "Epoch 6/10, Loss: 0.9334, Val Accuracy: 67.91%\n",
      "Elapsed time on CPU: 0.0h 0.0m 10.135335883998778s\n",
      "Epoch 7/10, Loss: 0.6994, Val Accuracy: 65.84%\n",
      "Elapsed time on CPU: 0.0h 0.0m 10.133414960990194s\n",
      "Epoch 8/10, Loss: 0.6471, Val Accuracy: 75.66%\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, len(target_classes), group_class):\n",
    "    print(f\"실험 {i//group_class + 1} 시작\")\n",
    "    selected_classes = target_classes[i:i+group_class]\n",
    "    print(f\"Selected Feature: {selected_classes}\")\n",
    "\n",
    "    filtered_data = [\n",
    "        item for item in dataset.data\n",
    "        if label_encoder.inverse_transform([item[1]])[0] in selected_classes\n",
    "    ]\n",
    "\n",
    "    filtered_dataset = ImageDataset(root_dir=data_dir, transform=transform, crop_size=crop_size, patch_size=patch_size)\n",
    "    filtered_dataset.data = filtered_data\n",
    "    filtered_dataset.label_encoder = LabelEncoder()\n",
    "    filtered_dataset.label_encoder.fit(selected_classes)\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    train_size = int(len(filtered_dataset) * train_ratio)\n",
    "    valid_size = len(filtered_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(filtered_dataset, [train_size, valid_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"train: {train_size}, valid: {valid_size}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    NUM_CLASSES = len(dataset.label_encoder.classes_)\n",
    "    model = Perceiver(input_dim=(patch_size**2) * 3 + 2,\n",
    "                        latent_dim=128, \n",
    "                        latent_size=64, \n",
    "                        num_classes=NUM_CLASSES, \n",
    "                        num_blocks=4, \n",
    "                        self_attn_layers_per_block=10).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_losses, val_accuracies = train_model(\n",
    "        model, train_loader, valid_loader,\n",
    "        criterion, optimizer, epochs,\n",
    "        device=device\n",
    "    )\n",
    "    final_acc = evaluate_model(model, valid_loader, device=device, log_results=True)\n",
    "    print(f\"Final Validation Accuracy: {final_acc:.2f}%\")\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(train_losses, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best trial: 15 epoch, train loss: 2.7125210852525674, val. acc: 16.942233108797712\n"
     ]
    }
   ],
   "source": [
    "# mx = 0\n",
    "# for i, acc in enumerate(val_accuracies):\n",
    "#     if(val_accuracies[mx] < acc):\n",
    "#         mx = i\n",
    "# print(f'best trial: {mx+1} epoch, train loss: {train_losses[mx]}, val. acc: {val_accuracies[mx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
