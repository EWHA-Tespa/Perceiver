{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from perceiver import tokenize_data, CustomDataset, PerceiverBlock, Perceiver, CombinedModel\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/jisoo/Perceiver/model/'\n",
    "loader_dir = '/home/jisoo/Perceiver/loader/'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, input_ids, labels):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'input_ids': self.input_ids[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model, Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_278211/3027492582.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model 1번 불러오기 완료.\n",
      "Text model 2번 불러오기 완료.\n",
      "Text model 3번 불러오기 완료.\n",
      "Text model 4번 불러오기 완료.\n",
      "Text model 5번 불러오기 완료.\n",
      "Text model 6번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "input_models = []\n",
    "valid_loaders = []\n",
    "for i in range (6):\n",
    "    text_model = torch.load(root_dir + f'text_model_{i+1}.pkl')\n",
    "    input_models.append(text_model)\n",
    "    print(f\"Text model {i+1}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_278211/2086765618.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model 0번 불러오기 완료.\n",
      "Image model 1번 불러오기 완료.\n",
      "Image model 2번 불러오기 완료.\n",
      "Image model 3번 불러오기 완료.\n",
      "Image model 4번 불러오기 완료.\n",
      "Image model 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    img_model = torch.load(root_dir + f'image_model_{i+1}.pkl')\n",
    "    input_models.append(img_model)\n",
    "    print(f\"Image model {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의: 현재 text 모달리티는 dataloader 자체가 저장되어있지만 image 모달리티는 데이터가 그대로 저장되어있어 Dataloader로 변환해주어야 합니다. \\\n",
    "일단 지금은 이대로 두지만 언젠가 에러나면 수정이 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text val. loader 0번 불러오기 완료.\n",
      "Text val. loader 1번 불러오기 완료.\n",
      "Text val. loader 2번 불러오기 완료.\n",
      "Text val. loader 3번 불러오기 완료.\n",
      "Text val. loader 4번 불러오기 완료.\n",
      "Text val. loader 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    with open(loader_dir+f'text_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "    valid_loaders.append(loaded_valid_dataset)\n",
    "    print(f\"Text val. loader {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image val. loader 0번 불러오기 완료.\n",
      "Image val. loader 1번 불러오기 완료.\n",
      "Image val. loader 2번 불러오기 완료.\n",
      "Image val. loader 3번 불러오기 완료.\n",
      "Image val. loader 4번 불러오기 완료.\n",
      "Image val. loader 5번 불러오기 완료.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    with open(loader_dir+f'image_val_loader_{i+1}.pkl', 'rb') as f:\n",
    "        loaded_valid_dataset = pickle.load(f)\n",
    "\n",
    "    valid_loader = DataLoader(loaded_valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loaders.append(valid_loader)\n",
    "    print(f\"Image val. loader {i}번 불러오기 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PackNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(PackNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.masks = {}\n",
    "        self.current_task = None\n",
    "\n",
    "    def set_task(self, task_id):\n",
    "        self.current_task = task_id\n",
    "        if task_id not in self.masks:\n",
    "            self.masks[task_id] = {\n",
    "                name: torch.ones_like(param, device=param.device)\n",
    "                for name, param in self.model.named_parameters()\n",
    "                if param.requires_grad\n",
    "            }\n",
    "\n",
    "    def prune(self, target_sparsity=0.2):\n",
    "        if self.current_task is None:\n",
    "            raise ValueError(\"Task must be set before pruning.\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                mask = self.masks[self.current_task][name]\n",
    "                threshold = torch.quantile(param.abs(), target_sparsity)\n",
    "                mask[param.abs() < threshold] = 0\n",
    "                self.masks[self.current_task][name] = mask\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        if self.current_task in self.masks:\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        param.data *= self.masks[self.current_task][name]\n",
    "        return self.model(input_ids, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)  \n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_pruning(packnet_model, model_type, model_index, criterion, device, start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders):\n",
    "    model_path = f\"{checkpoint_dir}/{model_type}_model_{model_index+1}_pruned.pkl\"\n",
    "    \n",
    "    sparsity_increment = (end_sparsity - start_sparsity) / pruning_steps\n",
    "    current_sparsity = start_sparsity\n",
    "    \n",
    "    test_loader = valid_loaders[model_index] if model_type == \"text\" else valid_loaders[model_index + 6]\n",
    "\n",
    "    for step in range(pruning_steps):\n",
    "        print(f\"[{model_type.upper()} Model {model_index+1}] Pruning Step {step+1}/{pruning_steps} with sparsity {current_sparsity:.2f}\")\n",
    "        packnet_model.prune(target_sparsity=current_sparsity)\n",
    "        \n",
    "        current_sparsity += sparsity_increment\n",
    "    \n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"model_state_dict\": packnet_model.state_dict(),\n",
    "            \"masks\": packnet_model.masks\n",
    "        }, f)\n",
    "    print(f\"[{model_type.upper()} Model {model_index+1}] Pruned model saved at {model_path}.\")\n",
    "    \n",
    "    test_loss, test_acc = eval_epoch(packnet_model, test_loader, criterion, device)\n",
    "    print(f\"[{model_type.upper()} Model {model_index+1}] Final Test Accuracy: {test_acc:.4f}\")\n",
    "    print(\"---------\")\n",
    "    \n",
    "    return packnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradual pruning process...\n",
      "[TEXT Model 1] Pruning Step 1/5 with sparsity 0.05\n",
      "[TEXT Model 1] Pruning Step 2/5 with sparsity 0.08\n",
      "[TEXT Model 1] Pruning Step 3/5 with sparsity 0.11\n",
      "[TEXT Model 1] Pruning Step 4/5 with sparsity 0.14\n",
      "[TEXT Model 1] Pruning Step 5/5 with sparsity 0.17\n",
      "[TEXT Model 1] Pruned model saved at /home/jisoo/Perceiver/checkpoints_pruned/text_model_1_pruned.pkl.\n",
      "[TEXT Model 1] Final Test Accuracy: 0.8661\n",
      "---------\n",
      "[TEXT Model 2] Pruning Step 1/5 with sparsity 0.05\n",
      "[TEXT Model 2] Pruning Step 2/5 with sparsity 0.08\n",
      "[TEXT Model 2] Pruning Step 3/5 with sparsity 0.11\n",
      "[TEXT Model 2] Pruning Step 4/5 with sparsity 0.14\n",
      "[TEXT Model 2] Pruning Step 5/5 with sparsity 0.17\n",
      "[TEXT Model 2] Pruned model saved at /home/jisoo/Perceiver/checkpoints_pruned/text_model_2_pruned.pkl.\n",
      "[TEXT Model 2] Final Test Accuracy: 0.8057\n",
      "---------\n",
      "[TEXT Model 3] Pruning Step 1/5 with sparsity 0.05\n",
      "[TEXT Model 3] Pruning Step 2/5 with sparsity 0.08\n",
      "[TEXT Model 3] Pruning Step 3/5 with sparsity 0.11\n",
      "[TEXT Model 3] Pruning Step 4/5 with sparsity 0.14\n",
      "[TEXT Model 3] Pruning Step 5/5 with sparsity 0.17\n",
      "[TEXT Model 3] Pruned model saved at /home/jisoo/Perceiver/checkpoints_pruned/text_model_3_pruned.pkl.\n",
      "[TEXT Model 3] Final Test Accuracy: 0.8156\n",
      "---------\n",
      "[TEXT Model 4] Pruning Step 1/5 with sparsity 0.05\n",
      "[TEXT Model 4] Pruning Step 2/5 with sparsity 0.08\n",
      "[TEXT Model 4] Pruning Step 3/5 with sparsity 0.11\n",
      "[TEXT Model 4] Pruning Step 4/5 with sparsity 0.14\n",
      "[TEXT Model 4] Pruning Step 5/5 with sparsity 0.17\n",
      "[TEXT Model 4] Pruned model saved at /home/jisoo/Perceiver/checkpoints_pruned/text_model_4_pruned.pkl.\n",
      "[TEXT Model 4] Final Test Accuracy: 0.8762\n",
      "---------\n",
      "[TEXT Model 5] Pruning Step 1/5 with sparsity 0.05\n",
      "[TEXT Model 5] Pruning Step 2/5 with sparsity 0.08\n",
      "[TEXT Model 5] Pruning Step 3/5 with sparsity 0.11\n",
      "[TEXT Model 5] Pruning Step 4/5 with sparsity 0.14\n",
      "[TEXT Model 5] Pruning Step 5/5 with sparsity 0.17\n",
      "[TEXT Model 5] Pruned model saved at /home/jisoo/Perceiver/checkpoints_pruned/text_model_5_pruned.pkl.\n",
      "[TEXT Model 5] Final Test Accuracy: 0.8038\n",
      "---------\n",
      "[TEXT Model 6] Pruning Step 1/5 with sparsity 0.05\n",
      "[TEXT Model 6] Pruning Step 2/5 with sparsity 0.08\n",
      "[TEXT Model 6] Pruning Step 3/5 with sparsity 0.11\n",
      "[TEXT Model 6] Pruning Step 4/5 with sparsity 0.14\n",
      "[TEXT Model 6] Pruning Step 5/5 with sparsity 0.17\n",
      "[TEXT Model 6] Pruned model saved at /home/jisoo/Perceiver/checkpoints_pruned/text_model_6_pruned.pkl.\n",
      "[TEXT Model 6] Final Test Accuracy: 0.8826\n",
      "---------\n",
      "Gradual pruning process finished for both text and image models.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_sparsity = 0.05\n",
    "    end_sparsity = 0.2\n",
    "    pruning_steps = 5\n",
    "    checkpoint_dir = \"/home/jisoo/Perceiver/checkpoints_pruned\"\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting gradual pruning process...\")\n",
    "    text_models = input_models[:6]\n",
    "    image_models = input_models[6:]\n",
    "\n",
    "    pruned_text_models = []\n",
    "    for i, model in enumerate(text_models):\n",
    "        packnet_model = PackNet(model)\n",
    "        packnet_model.set_task(f\"text_task_{i+1}\")\n",
    "        pruned_text_models.append(gradual_pruning(packnet_model, \"text\", i, criterion, device, start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders))\n",
    "    \n",
    "    pruned_image_models = []\n",
    "    for i, model in enumerate(image_models):\n",
    "        packnet_model = PackNet(model)\n",
    "        packnet_model.set_task(f\"image_task_{i+1}\")\n",
    "        #pruned_image_models.append(gradual_pruning(packnet_model, \"image\", i, criterion, device, start_sparsity, end_sparsity, pruning_steps, checkpoint_dir, valid_loaders))\n",
    "    \n",
    "    pruned_models = pruned_text_models + pruned_image_models\n",
    "    print(\"Gradual pruning process finished for both text and image models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
