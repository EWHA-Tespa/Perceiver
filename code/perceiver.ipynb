{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from perceiver_pytorch.perceiver_pytorch import exists, default, cache_fn, fourier_encode, PreNorm, FeeodForward, Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Feature Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FourierFeatureEncoding(nn.Module):\n",
    "#     '''MNIST에 테스트해보기 위해 임의로\n",
    "#     chatGPT를 사용해 만들었습니다. 추후 수정 필요'''\n",
    "#     def __init__(self, num_features, max_freq=10.0):\n",
    "#         super(FourierFeatureEncoding, self).__init__()\n",
    "#         self.num_features = num_features\n",
    "#         self.max_freq = max_freq\n",
    "#         # 주파수 벡터 생성 (로그 스케일링 권장)\n",
    "#         self.register_buffer('freq_bands', torch.linspace(1, max_freq, num_features))\n",
    "\n",
    "#     def forward(self, x, height, width):\n",
    "#         \"\"\"\n",
    "#         x: 입력 텐서 (batch_size, input_len, input_dim)\n",
    "#         height: 이미지의 높이\n",
    "#         width: 이미지의 너비\n",
    "#         \"\"\"\n",
    "#         batch_size, input_len, input_dim = x.shape\n",
    "#         # (input_len,) -> (height, width)\n",
    "#         assert input_len == height * width, \"input_len must be height * width\"\n",
    "\n",
    "#         # 좌표 그리드 생성\n",
    "#         y_coords = torch.arange(0, height).float() / height  # [0, 1)\n",
    "#         x_coords = torch.arange(0, width).float() / width    # [0, 1)\n",
    "#         y_grid, x_grid = torch.meshgrid(y_coords, x_coords)\n",
    "#         y_grid = y_grid.flatten().to(x.device)  # (input_len,)\n",
    "#         x_grid = x_grid.flatten().to(x.device)  # (input_len,)\n",
    "\n",
    "#         # 좌표를 [batch_size, input_len, 2] 형태로 확장\n",
    "#         coords = torch.stack([x_grid, y_grid], dim=1).unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, input_len, 2)\n",
    "\n",
    "#         # 주파수 변환\n",
    "#         coords = coords * 2 * torch.pi  # 주기적 함수의 주기를 맞추기 위해 2π를 곱함\n",
    "#         coords = coords.unsqueeze(-1) * self.freq_bands  # (batch_size, input_len, 2, num_features)\n",
    "#         coords = coords.view(batch_size, input_len, -1)  # (batch_size, input_len, 2 * num_features)\n",
    "\n",
    "#         # 사인과 코사인 적용\n",
    "#         fourier_feats = torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)  # (batch_size, input_len, 4 * num_features)\n",
    "\n",
    "#         return fourier_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.key_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.query_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.value_proj = nn.Linear(d_in, d_out_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)           # 이게 뭐지\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        keys = self.key_proj(x)\n",
    "        queries = self.query_proj(latent)\n",
    "        values = self.value_proj(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        attended_values = torch.matmul(attention_probs, values)\n",
    "        return attended_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTransformer():\n",
    "    def __init__(self, latent_dim, num_heads, num_layers):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads) # trasformer 로 latent array 반복적으로 update\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        latent = latent.permute(1,0,2)  # Transformer는 (seq_len, batch_size, latent_dim) 형식으로 데이터 받음.\n",
    "        latent = self.transformer(latent)\n",
    "        return latent.permute(1,0,2)    # 이걸 다시 (batch_size, latent_len, latent_dim으로 바꿈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averaging():\n",
    "    def forward(self, latent):\n",
    "        return latent.mean(dim=1)   # latent vector를 평균내서 최종 logits 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, embed_dim, num_heads, num_layers, num_classes):\n",
    "        super(Perceiver, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, latent_dim, embed_dim))\n",
    "\n",
    "        self.cross_attention = CrossAttention(d_in=embed_dim, d_out_kq=embed_dim, d_out_v=embed_dim)\n",
    "        self.latent_transformer = LatentTransformer(latent_dim=latent_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "        \n",
    "        self.averaging = nn.Averaging()\n",
    "        self.classificer = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, latent, batch_size):\n",
    "        x = self.input_proj(x)\n",
    "        latent = self.latents.repeat(batch_size, 1, 1)    # batch 학습 시, batch 내 각각 샘플에 서로 다른 독립적인 latent값을 제공\n",
    "        latent = self.cross_attention(x, latent)\n",
    "        latent = self.latent_transformer(latent)\n",
    "        latent_avg = self.averaging(latent)\n",
    "        logits = self.classifier(latent_avg)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
