{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- coco_어쩌구 돼있는거 수정해야함.. >> 그냥 지우고 다시하죠\n",
    "\n",
    "1. 데이터셋에서 캡션 로드 ????? 수정해야함\n",
    "2. 각 캡션에서 15%의 단어 마스킹\n",
    "3. 마스킹된 캡션을 UTF-8 바이트 시퀀스로 변환\n",
    "4. 바이트와 특별 토큰을 모델 입력에 적합한 인덱스로 매핑\n",
    "5. 입력 시퀀스, 어텐션 마스크, 타깃 레이블 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 논문 내용\n",
    "1. 데이터 준비\n",
    "    1. pretraining 데이터 준비\n",
    "    2. 데이터 전처리 \n",
    "        - SentencePiece: Devlin et al. (2019)에 따라 32,000개의 서브워드로 Vocabulary 생성\n",
    "        - UTF-8 bytes: 전처리 없이 데이터를 UTF-8 바이트 시퀀스로 변환\n",
    "\n",
    "2. Perceiver IO 구조 정의\n",
    "    - 입력: SentencePiece 토큰 또는 UTF-8 bytes\n",
    "    - 출력: Latent query를 사용해 Masked된 입력의 원래 값을 예측\n",
    "    - Latent 크기: 256 \n",
    "    - Processing layers: 26층 (SentencePiece 기준)\n",
    "\n",
    "3. 토큰 마스크 생성\n",
    "    - 입력 데이터 15% 랜덤 마스킹\n",
    "    - BERT의 Masked Language Model(MLM) 방식 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    '[PAD]': 256,\n",
    "    '[MASK]': 257,\n",
    "    '[CLS]': 258,\n",
    "    '[SEP]': 259\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_caption(caption, mask_token='[MASK]', max_length=2048):\n",
    "\n",
    "    # 단어로 분리 \n",
    "    words = caption.strip().split()\n",
    "    \n",
    "    # 마스킹할 단어 수\n",
    "    num_words_to_mask = max(1, int(0.15 * len(words)))\n",
    "    \n",
    "    # 마스킹할 단어의 인덱스 랜덤 선택\n",
    "    masked_word_indices = random.sample(range(len(words)), num_words_to_mask)\n",
    "    \n",
    "    # 마스킹된 캡션 생성\n",
    "    masked_words = words.copy()\n",
    "    for idx in masked_word_indices:\n",
    "        masked_words[idx] = mask_token  # 단어를 [MASK]로 대체\n",
    "    \n",
    "    # 원본 캡션 바이트 (타깃)\n",
    "    target_text = ' '.join(words)\n",
    "    target_bytes = target_text.encode('utf-8')\n",
    "    \n",
    "    # 마스킹된 캡션 바이트 (입력)\n",
    "    masked_text = ' '.join(masked_words)\n",
    "    masked_bytes = masked_text.encode('utf-8')\n",
    "    \n",
    "    # [MASK] 토큰의 바이트 표현\n",
    "    mask_token_bytes = mask_token.encode('utf-8')\n",
    "\n",
    "    # 시퀀스에서 부분 시퀀스의 모든 위치 찾기\n",
    "    def find_subsequence_indices(sequence, subsequence):\n",
    "        indices = []\n",
    "        seq_len = len(sequence)\n",
    "        sub_len = len(subsequence)\n",
    "        i = 0\n",
    "        while i <= seq_len - sub_len:\n",
    "            if sequence[i:i+sub_len] == subsequence:\n",
    "                indices.append((i, i+sub_len))\n",
    "                i += sub_len\n",
    "            else:\n",
    "                i += 1\n",
    "        return indices\n",
    "\n",
    "    # 마스킹된 바이트에서 [MASK] 토큰의 위치 찾기\n",
    "    mask_positions = find_subsequence_indices(masked_bytes, mask_token_bytes)\n",
    "\n",
    "    # 바이트를 인덱스로 매핑 (0-255), 특별 토큰은 256-259\n",
    "    input_indices = []\n",
    "    i = 0\n",
    "    while i < len(masked_bytes):\n",
    "        # 현재 위치가 [MASK] 토큰의 시작인지 확인\n",
    "        is_mask = False\n",
    "        for start, end in mask_positions:\n",
    "            if i == start:\n",
    "                # [MASK] 토큰\n",
    "                input_indices.append(SPECIAL_TOKENS['[MASK]'])  # [MASK]의 인덱스\n",
    "                i = end  # 인덱스를 [MASK]의 끝으로 이동\n",
    "                is_mask = True\n",
    "                break\n",
    "        if not is_mask:\n",
    "            # 일반 바이트\n",
    "            input_indices.append(masked_bytes[i])\n",
    "            i += 1\n",
    "\n",
    "    # max_length에 맞게 패딩 또는 잘라내기\n",
    "    if len(input_indices) < max_length:\n",
    "        input_indices += [SPECIAL_TOKENS['[PAD]']] * (max_length - len(input_indices))  # [PAD]로 패딩\n",
    "    else:\n",
    "        input_indices = input_indices[:max_length]\n",
    "\n",
    "    # 레이블 생성: 마스킹되지 않은 위치는 -100, 마스킹된 위치는 타깃 바이트 인덱스\n",
    "    labels = [-100] * len(input_indices)\n",
    "\n",
    "    # 원본 캡션에서 단어의 바이트 위치 얻기\n",
    "    def get_word_byte_positions(text):\n",
    "        words = text.strip().split()\n",
    "        positions = []\n",
    "        pos = 0\n",
    "        for word in words:\n",
    "            word_bytes = word.encode('utf-8')\n",
    "            word_len = len(word_bytes)\n",
    "            positions.append((pos, pos + word_len))\n",
    "            pos += word_len\n",
    "            # 마지막 단어가 아니면 공백 문자 추가\n",
    "            if word != words[-1]:\n",
    "                pos += 1  # 공백 문자\n",
    "        return positions\n",
    "\n",
    "    # 타깃 바이트에서 단어의 위치 얻기\n",
    "    word_positions = get_word_byte_positions(target_text)\n",
    "\n",
    "    # 마스킹된 단어 위치에 레이블 설정\n",
    "    for idx in masked_word_indices:\n",
    "        start_pos, end_pos = word_positions[idx]\n",
    "        # max_length를 초과하지 않도록 확인\n",
    "        if end_pos > max_length:\n",
    "            continue\n",
    "        labels[start_pos:end_pos] = [target_bytes[i] for i in range(start_pos, end_pos)]\n",
    "\n",
    "    # max_length에 맞게 레이블 패딩 또는 잘라내기\n",
    "    if len(labels) < max_length:\n",
    "        labels += [-100] * (max_length - len(labels))\n",
    "    else:\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    # 어텐션 마스크 생성 (실제 토큰은 1, 패딩은 0)\n",
    "    attention_mask = [1 if idx != SPECIAL_TOKENS['[PAD]'] else 0 for idx in input_indices]\n",
    "\n",
    "    return input_indices, labels, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캡션 로드 \n",
    "captions = load_coco_captions('captions_train2017.json') # 고쳐야함\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "dataset = CocoCaptionsDataset(captions)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 데이터로더 반복\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    labels = batch['labels']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
