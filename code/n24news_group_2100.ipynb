{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped data files saved:\n",
      "/home/youlee/n24news/n24news/filtered_group_1.csv\n",
      "/home/youlee/n24news/n24news/filtered_group_2.csv\n",
      "/home/youlee/n24news/n24news/filtered_group_3.csv\n",
      "/home/youlee/n24news/n24news/filtered_group_4.csv\n",
      "/home/youlee/n24news/n24news/filtered_group_5.csv\n",
      "/home/youlee/n24news/n24news/filtered_group_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43860/4187732925.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_data_sampled = filtered_data.groupby('Label').apply(\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/youlee/n24news/n24news/captions_and_labels.csv'\n",
    "\n",
    "categories = [\n",
    "    \"Opinion\", \"Art & Design\", \"Television\", \"Music\", \"Travel\",\n",
    "    \"Real Estate\", \"Books\", \"Theater\", \"Health\", \"Sports\",\n",
    "    \"Science\", \"Food\", \"Fashion & Style\", \"Movies\", \"Technology\",\n",
    "    \"Dance\", \"Media\", \"Style\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "filtered_data = data[data['Label'].isin(categories)]\n",
    "\n",
    "\n",
    "filtered_data_sampled = filtered_data.groupby('Label').apply(\n",
    "    lambda x: x.sample(n=min(2100, len(x)), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "label_groups = [categories[i:i + 3] for i in range(0, len(categories), 3)]\n",
    "group_files = []\n",
    "for i, group in enumerate(label_groups, start=1):\n",
    "    group_data = filtered_data_sampled[filtered_data_sampled['Label'].isin(group)]\n",
    "    output_file_path = f'/home/youlee/n24news/n24news/filtered_group_{i}.csv'\n",
    "    group_data.to_csv(output_file_path, index=False)\n",
    "    group_files.append(output_file_path)\n",
    "\n",
    "print(\"Grouped data files saved:\")\n",
    "for file in group_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def tokenize_data(df):\n",
    "    input_ids, attention_masks = [], []\n",
    "    \n",
    "    df['Caption'] = df['Caption'].astype(str).fillna(\"\")\n",
    "\n",
    "    for text in df['Caption']:\n",
    "        encoded = tokenizer(\n",
    "            text, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'].squeeze(0))\n",
    "        attention_masks.append(encoded['attention_mask'].squeeze(0))\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.key_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.query_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.value_proj = nn.Linear(d_in, d_out_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        keys = self.key_proj(x)\n",
    "        queries = self.query_proj(latent)\n",
    "        values = self.value_proj(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        attended_values = torch.matmul(attention_probs, values)\n",
    "        return attended_values\n",
    "\n",
    "class LatentTransformer(nn.Module):\n",
    "    def __init__(self, latent_dim, num_heads, num_layers, embed_dim):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        latent = latent.permute(1, 0, 2)\n",
    "        latent = self.transformer(latent)\n",
    "        return latent.permute(1, 0, 2)\n",
    "\n",
    "class Averaging(nn.Module):\n",
    "    def forward(self, latent):\n",
    "        return latent.mean(dim=1)\n",
    "\n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, latent_dim, num_heads, num_layers, num_classes):\n",
    "        super(Perceiver, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.input_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, latent_dim, embed_dim))\n",
    "        self.cross_attention = CrossAttention(d_in=embed_dim, d_out_kq=embed_dim, d_out_v=embed_dim)\n",
    "        self.latent_transformer = LatentTransformer(latent_dim=latent_dim, num_heads=num_heads,\n",
    "                                                    num_layers=num_layers, embed_dim=embed_dim)\n",
    "        self.averaging = Averaging()\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        latent = self.latents.repeat(batch_size, 1, 1)\n",
    "        latent = self.cross_attention(x, latent)\n",
    "        latent = self.latent_transformer(latent)\n",
    "        latent_avg = self.averaging(latent)\n",
    "        logits = self.classifier(latent_avg)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "results = []\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "LATENT_DIM = 64\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Group 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 1 Epoch 1/10: Train Loss: 0.7258, Train Acc: 0.6944\n",
      "                             Test Loss: 0.6151, Test Acc: 0.7540\n",
      "  Group 1 Epoch 2/10: Train Loss: 0.5311, Train Acc: 0.7935\n",
      "                             Test Loss: 0.5451, Test Acc: 0.7802\n",
      "  Group 1 Epoch 3/10: Train Loss: 0.4459, Train Acc: 0.8306\n",
      "                             Test Loss: 0.5186, Test Acc: 0.7976\n",
      "  Group 1 Epoch 4/10: Train Loss: 0.3894, Train Acc: 0.8514\n",
      "                             Test Loss: 0.4974, Test Acc: 0.8095\n",
      "  Group 1 Epoch 5/10: Train Loss: 0.3404, Train Acc: 0.8764\n",
      "                             Test Loss: 0.4798, Test Acc: 0.8119\n",
      "  Group 1 Epoch 6/10: Train Loss: 0.2971, Train Acc: 0.8933\n",
      "                             Test Loss: 0.5036, Test Acc: 0.8103\n",
      "  Group 1 Epoch 7/10: Train Loss: 0.2664, Train Acc: 0.9032\n",
      "                             Test Loss: 0.4897, Test Acc: 0.8286\n",
      "  Group 1 Epoch 8/10: Train Loss: 0.2245, Train Acc: 0.9177\n",
      "                             Test Loss: 0.4962, Test Acc: 0.8310\n",
      "  Group 1 Epoch 9/10: Train Loss: 0.1811, Train Acc: 0.9353\n",
      "                             Test Loss: 0.5392, Test Acc: 0.8246\n",
      "  Group 1 Epoch 10/10: Train Loss: 0.1519, Train Acc: 0.9498\n",
      "                             Test Loss: 0.5744, Test Acc: 0.8278\n",
      "\n",
      "Processing Group 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 2 Epoch 1/10: Train Loss: 0.9137, Train Acc: 0.5712\n",
      "                             Test Loss: 0.8192, Test Acc: 0.6460\n",
      "  Group 2 Epoch 2/10: Train Loss: 0.6840, Train Acc: 0.7173\n",
      "                             Test Loss: 0.6870, Test Acc: 0.7079\n",
      "  Group 2 Epoch 3/10: Train Loss: 0.5825, Train Acc: 0.7704\n",
      "                             Test Loss: 0.6554, Test Acc: 0.7389\n",
      "  Group 2 Epoch 4/10: Train Loss: 0.4976, Train Acc: 0.8091\n",
      "                             Test Loss: 0.6345, Test Acc: 0.7413\n",
      "  Group 2 Epoch 5/10: Train Loss: 0.4412, Train Acc: 0.8276\n",
      "                             Test Loss: 0.6053, Test Acc: 0.7548\n",
      "  Group 2 Epoch 6/10: Train Loss: 0.3815, Train Acc: 0.8528\n",
      "                             Test Loss: 0.5938, Test Acc: 0.7706\n",
      "  Group 2 Epoch 7/10: Train Loss: 0.3166, Train Acc: 0.8813\n",
      "                             Test Loss: 0.6390, Test Acc: 0.7635\n",
      "  Group 2 Epoch 8/10: Train Loss: 0.2756, Train Acc: 0.9018\n",
      "                             Test Loss: 0.6130, Test Acc: 0.7730\n",
      "  Group 2 Epoch 9/10: Train Loss: 0.2348, Train Acc: 0.9169\n",
      "                             Test Loss: 0.6491, Test Acc: 0.7778\n",
      "  Group 2 Epoch 10/10: Train Loss: 0.1895, Train Acc: 0.9345\n",
      "                             Test Loss: 0.7162, Test Acc: 0.7802\n",
      "\n",
      "Processing Group 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 3 Epoch 1/10: Train Loss: 0.7393, Train Acc: 0.6823\n",
      "                             Test Loss: 0.5993, Test Acc: 0.7524\n",
      "  Group 3 Epoch 2/10: Train Loss: 0.5414, Train Acc: 0.7786\n",
      "                             Test Loss: 0.5549, Test Acc: 0.7778\n",
      "  Group 3 Epoch 3/10: Train Loss: 0.4438, Train Acc: 0.8310\n",
      "                             Test Loss: 0.5002, Test Acc: 0.7905\n",
      "  Group 3 Epoch 4/10: Train Loss: 0.3923, Train Acc: 0.8429\n",
      "                             Test Loss: 0.4559, Test Acc: 0.8127\n",
      "  Group 3 Epoch 5/10: Train Loss: 0.3293, Train Acc: 0.8760\n",
      "                             Test Loss: 0.4494, Test Acc: 0.8222\n",
      "  Group 3 Epoch 6/10: Train Loss: 0.2998, Train Acc: 0.8899\n",
      "                             Test Loss: 0.4632, Test Acc: 0.8238\n",
      "  Group 3 Epoch 7/10: Train Loss: 0.2666, Train Acc: 0.9006\n",
      "                             Test Loss: 0.5293, Test Acc: 0.8095\n",
      "  Group 3 Epoch 8/10: Train Loss: 0.2298, Train Acc: 0.9163\n",
      "                             Test Loss: 0.5202, Test Acc: 0.8206\n",
      "  Group 3 Epoch 9/10: Train Loss: 0.2041, Train Acc: 0.9272\n",
      "                             Test Loss: 0.4864, Test Acc: 0.8365\n",
      "  Group 3 Epoch 10/10: Train Loss: 0.1663, Train Acc: 0.9423\n",
      "                             Test Loss: 0.5109, Test Acc: 0.8317\n",
      "\n",
      "Processing Group 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 4 Epoch 1/10: Train Loss: 0.9199, Train Acc: 0.5605\n",
      "                             Test Loss: 0.7402, Test Acc: 0.6857\n",
      "  Group 4 Epoch 2/10: Train Loss: 0.6595, Train Acc: 0.7218\n",
      "                             Test Loss: 0.6317, Test Acc: 0.7444\n",
      "  Group 4 Epoch 3/10: Train Loss: 0.5324, Train Acc: 0.7833\n",
      "                             Test Loss: 0.5453, Test Acc: 0.7730\n",
      "  Group 4 Epoch 4/10: Train Loss: 0.4396, Train Acc: 0.8252\n",
      "                             Test Loss: 0.5118, Test Acc: 0.7952\n",
      "  Group 4 Epoch 5/10: Train Loss: 0.3676, Train Acc: 0.8601\n",
      "                             Test Loss: 0.4944, Test Acc: 0.8000\n",
      "  Group 4 Epoch 6/10: Train Loss: 0.2911, Train Acc: 0.8925\n",
      "                             Test Loss: 0.4633, Test Acc: 0.8206\n",
      "  Group 4 Epoch 7/10: Train Loss: 0.2263, Train Acc: 0.9169\n",
      "                             Test Loss: 0.5120, Test Acc: 0.8183\n",
      "  Group 4 Epoch 8/10: Train Loss: 0.1813, Train Acc: 0.9349\n",
      "                             Test Loss: 0.5645, Test Acc: 0.8040\n",
      "  Group 4 Epoch 9/10: Train Loss: 0.1262, Train Acc: 0.9542\n",
      "                             Test Loss: 0.6288, Test Acc: 0.8222\n",
      "  Group 4 Epoch 10/10: Train Loss: 0.0992, Train Acc: 0.9681\n",
      "                             Test Loss: 0.6008, Test Acc: 0.8230\n",
      "\n",
      "Processing Group 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 5 Epoch 1/10: Train Loss: 0.7709, Train Acc: 0.6450\n",
      "                             Test Loss: 0.5872, Test Acc: 0.7603\n",
      "  Group 5 Epoch 2/10: Train Loss: 0.5623, Train Acc: 0.7716\n",
      "                             Test Loss: 0.5316, Test Acc: 0.7897\n",
      "  Group 5 Epoch 3/10: Train Loss: 0.4726, Train Acc: 0.8103\n",
      "                             Test Loss: 0.4749, Test Acc: 0.8119\n",
      "  Group 5 Epoch 4/10: Train Loss: 0.4171, Train Acc: 0.8395\n",
      "                             Test Loss: 0.4721, Test Acc: 0.8206\n",
      "  Group 5 Epoch 5/10: Train Loss: 0.3695, Train Acc: 0.8579\n",
      "                             Test Loss: 0.4233, Test Acc: 0.8325\n",
      "  Group 5 Epoch 6/10: Train Loss: 0.3352, Train Acc: 0.8756\n",
      "                             Test Loss: 0.4164, Test Acc: 0.8397\n",
      "  Group 5 Epoch 7/10: Train Loss: 0.2811, Train Acc: 0.8942\n",
      "                             Test Loss: 0.4360, Test Acc: 0.8389\n",
      "  Group 5 Epoch 8/10: Train Loss: 0.2421, Train Acc: 0.9121\n",
      "                             Test Loss: 0.4671, Test Acc: 0.8294\n",
      "  Group 5 Epoch 9/10: Train Loss: 0.2003, Train Acc: 0.9284\n",
      "                             Test Loss: 0.4771, Test Acc: 0.8397\n",
      "  Group 5 Epoch 10/10: Train Loss: 0.1637, Train Acc: 0.9450\n",
      "                             Test Loss: 0.4713, Test Acc: 0.8579\n",
      "\n",
      "Processing Group 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 6 Epoch 1/10: Train Loss: 0.8641, Train Acc: 0.5871\n",
      "                             Test Loss: 0.6871, Test Acc: 0.6968\n",
      "  Group 6 Epoch 2/10: Train Loss: 0.6735, Train Acc: 0.6966\n",
      "                             Test Loss: 0.6952, Test Acc: 0.6841\n",
      "  Group 6 Epoch 3/10: Train Loss: 0.5948, Train Acc: 0.7401\n",
      "                             Test Loss: 0.6202, Test Acc: 0.7238\n",
      "  Group 6 Epoch 4/10: Train Loss: 0.5306, Train Acc: 0.7726\n",
      "                             Test Loss: 0.5713, Test Acc: 0.7675\n",
      "  Group 6 Epoch 5/10: Train Loss: 0.4729, Train Acc: 0.8038\n",
      "                             Test Loss: 0.6446, Test Acc: 0.7222\n",
      "  Group 6 Epoch 6/10: Train Loss: 0.4369, Train Acc: 0.8234\n",
      "                             Test Loss: 0.5462, Test Acc: 0.7730\n",
      "  Group 6 Epoch 7/10: Train Loss: 0.3864, Train Acc: 0.8433\n",
      "                             Test Loss: 0.5572, Test Acc: 0.7730\n",
      "  Group 6 Epoch 8/10: Train Loss: 0.3355, Train Acc: 0.8663\n",
      "                             Test Loss: 0.6047, Test Acc: 0.7746\n",
      "  Group 6 Epoch 9/10: Train Loss: 0.2953, Train Acc: 0.8897\n",
      "                             Test Loss: 0.5861, Test Acc: 0.7905\n",
      "  Group 6 Epoch 10/10: Train Loss: 0.2466, Train Acc: 0.9113\n",
      "                             Test Loss: 0.5933, Test Acc: 0.7762\n",
      "\n",
      "Group 1 Results:\n",
      "Test Accuracy: 0.8278\n",
      "              precision    recall  f1-score      support\n",
      "0              0.834951  0.792627  0.813239   434.000000\n",
      "1              0.835411  0.813107  0.824108   412.000000\n",
      "2              0.814318  0.879227  0.845528   414.000000\n",
      "accuracy       0.827778  0.827778  0.827778     0.827778\n",
      "macro avg      0.828227  0.828320  0.827625  1260.000000\n",
      "weighted avg   0.828322  0.827778  0.827402  1260.000000\n",
      "\n",
      "Group 2 Results:\n",
      "Test Accuracy: 0.7802\n",
      "              precision    recall  f1-score      support\n",
      "0              0.861111  0.833741  0.847205   409.000000\n",
      "1              0.790640  0.732877  0.760664   438.000000\n",
      "2              0.700873  0.777240  0.737084   413.000000\n",
      "accuracy       0.780159  0.780159  0.780159     0.780159\n",
      "macro avg      0.784208  0.781286  0.781651  1260.000000\n",
      "weighted avg   0.784092  0.780159  0.781026  1260.000000\n",
      "\n",
      "Group 3 Results:\n",
      "Test Accuracy: 0.8317\n",
      "              precision    recall  f1-score      support\n",
      "0              0.731618  0.888393  0.802419   448.000000\n",
      "1              0.929619  0.776961  0.846462   408.000000\n",
      "2              0.888000  0.824257  0.854942   404.000000\n",
      "accuracy       0.831746  0.831746  0.831746     0.831746\n",
      "macro avg      0.849745  0.829870  0.834608  1260.000000\n",
      "weighted avg   0.845874  0.831746  0.833521  1260.000000\n",
      "\n",
      "Group 4 Results:\n",
      "Test Accuracy: 0.8230\n",
      "              precision    recall  f1-score      support\n",
      "0              0.810989  0.854167  0.832018   432.000000\n",
      "1              0.805128  0.758454  0.781095   414.000000\n",
      "2              0.853012  0.855072  0.854041   414.000000\n",
      "accuracy       0.823016  0.823016  0.823016     0.823016\n",
      "macro avg      0.823043  0.822564  0.822385  1260.000000\n",
      "weighted avg   0.822871  0.823016  0.822522  1260.000000\n",
      "\n",
      "Group 5 Results:\n",
      "Test Accuracy: 0.8579\n",
      "              precision    recall  f1-score      support\n",
      "0              0.851385  0.795294  0.822384   425.000000\n",
      "1              0.866667  0.926437  0.895556   435.000000\n",
      "2              0.854271  0.850000  0.852130   400.000000\n",
      "accuracy       0.857937  0.857937  0.857937     0.857937\n",
      "macro avg      0.857441  0.857244  0.856690  1260.000000\n",
      "weighted avg   0.857577  0.857937  0.857089  1260.000000\n",
      "\n",
      "Group 6 Results:\n",
      "Test Accuracy: 0.7762\n",
      "              precision    recall  f1-score     support\n",
      "0              0.890736  0.856164  0.873108   438.00000\n",
      "1              0.778667  0.715686  0.745849   408.00000\n",
      "2              0.670259  0.751208  0.708428   414.00000\n",
      "accuracy       0.776190  0.776190  0.776190     0.77619\n",
      "macro avg      0.779887  0.774353  0.775795  1260.00000\n",
      "weighted avg   0.782004  0.776190  0.777791  1260.00000\n"
     ]
    }
   ],
   "source": [
    "for idx, group_file in enumerate(group_files, start=1):\n",
    "    print(f\"\\nProcessing Group {idx}...\")\n",
    "\n",
    "    df = pd.read_csv(group_file)\n",
    "    df['Label'] = LabelEncoder().fit_transform(df['Label'])\n",
    "\n",
    "    input_ids, attention_masks = tokenize_data(df)\n",
    "    labels = torch.tensor(df['Label'].values)\n",
    "\n",
    "    dataset = CustomDataset(input_ids, attention_masks, labels)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = Perceiver(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, latent_dim=LATENT_DIM,\n",
    "                      num_heads=NUM_HEADS, num_layers=NUM_LAYERS, num_classes=len(df['Label'].unique()))\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = eval_epoch(model, test_loader, criterion, device)\n",
    "        print(f'  Group {idx} Epoch {epoch+1}/{EPOCHS}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'                             Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    results.append({\n",
    "        \"Group\": idx,\n",
    "        \"Test Accuracy\": test_acc,\n",
    "        \"Classification Report\": report\n",
    "    })\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\nGroup {result['Group']} Results:\")\n",
    "    print(f\"Test Accuracy: {result['Test Accuracy']:.4f}\")\n",
    "    print(pd.DataFrame(result['Classification Report']).transpose())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
