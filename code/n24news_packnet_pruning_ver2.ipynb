{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/jisoo/n24news/n24news/captions_and_labels.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "groups = [\n",
    "    ['Opinion', 'Food', 'Movies'],\n",
    "    ['Art & Design', 'Science', 'Fashion & Style'],\n",
    "    ['Television', 'Sports', 'Style'],\n",
    "    ['Music', 'Health', 'Dance'],\n",
    "    ['Real Estate', 'Books', 'Media'],\n",
    "    ['Travel', 'Theater', 'Technology']\n",
    "]\n",
    "\n",
    "output_paths = []\n",
    "for i, group_labels in enumerate(groups, 1):\n",
    "    group_data = data[data['Label'].isin(group_labels)]\n",
    "    output_path = f'/home/jisoo/n24news/n24news/regroup_{i}.csv'\n",
    "    group_data.to_csv(output_path, index=False)\n",
    "    output_paths.append(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def tokenize_data(df):\n",
    "    input_ids = []\n",
    "\n",
    "    df['Caption'] = df['Caption'].astype(str).fillna(\"\")\n",
    "\n",
    "    for text in df['Caption']:\n",
    "        encoded = tokenizer(\n",
    "            text, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'].squeeze(0))\n",
    "    return torch.stack(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    - 크로스 어텐션 (latents -> x)\n",
    "    - 이어서 셀프 어텐션 (latent들끼리)\n",
    "    - 보통은 LayerNorm, MLP(FeedForward) 등을 곁들여 잔차 연결(residual branch)을 구성\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_heads=8, self_attn_layers=1):\n",
    "        super().__init__()\n",
    "        # 크로스 어텐션\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=n_heads)\n",
    "        self.cross_ln = nn.LayerNorm(latent_dim)  # Layer Normalization\n",
    "\n",
    "        # 여러 층의 셀프 어텐션\n",
    "        self.self_attn_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=latent_dim, nhead=n_heads)\n",
    "            for _ in range(self_attn_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, latents, x):\n",
    "        # latents, x: (T, B, dim) 형태로 가정\n",
    "        # Perceiver 원리상 latents는 query, x는 key/value\n",
    "\n",
    "        # 1) 크로스 어텐션\n",
    "        updated_latents, _ = self.cross_attn(latents, x, x)\n",
    "        latents = latents + updated_latents        # 잔차 연결\n",
    "        latents = self.cross_ln(latents)           # LayerNorm\n",
    "\n",
    "        # 2) 셀프 어텐션 반복\n",
    "        for layer in self.self_attn_layers:\n",
    "            latents = layer(latents)  # 내부적으로 잔차 연결 및 LayerNorm 포함\n",
    "\n",
    "        return latents\n",
    "\n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, latent_size, num_classes,\n",
    "                 num_blocks, self_attn_layers_per_block=1):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(latent_size, latent_dim))\n",
    "        self.input_projection = nn.Linear(input_dim, latent_dim)\n",
    "\n",
    "        # 여러 개의 PerceiverBlock을 쌓음\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PerceiverBlock(\n",
    "                latent_dim=latent_dim,\n",
    "                n_heads=8,\n",
    "                self_attn_layers=self_attn_layers_per_block\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) = (배치, 시퀀스 길이, 피처 차원)\n",
    "        \"\"\"\n",
    "        B, T, F = x.size()\n",
    "        x = self.input_projection(x)                 # (B, T, latent_dim)\n",
    "\n",
    "        # latents: (latent_size, latent_dim) -> 배치 차원 확장 (B, latent_size, latent_dim)\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        # MultiHeadAttention은 (T, B, dim) 순서를 권장하므로 permute\n",
    "        x = x.permute(1, 0, 2)        # (T, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2)  # (latent_size, B, latent_dim)\n",
    "\n",
    "        # PerceiverBlock을 여러 번 반복\n",
    "        for block in self.blocks:\n",
    "            latents = block(latents, x)\n",
    "\n",
    "        # 최종 latents: (latent_size, B, latent_dim)\n",
    "        latents = latents.permute(1, 0, 2).mean(dim=1)  # (B, latent_dim)\n",
    "        return self.output_layer(latents)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, perceiver_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.perceiver = perceiver_model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)  # (B, T, embed_dim)\n",
    "        return self.perceiver(embeddings)       # Perceiver에 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(PackNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.masks = {}\n",
    "        self.current_task = None\n",
    "\n",
    "    def set_task(self, task_id):\n",
    "        self.current_task = task_id\n",
    "        if task_id not in self.masks:\n",
    "            self.masks[task_id] = {\n",
    "                name: torch.ones_like(param, device=param.device)\n",
    "                for name, param in self.model.named_parameters()\n",
    "                if param.requires_grad\n",
    "            }\n",
    "\n",
    "    def prune(self, sparsity=0.2):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                mask = self.masks[self.current_task][name]\n",
    "                threshold = torch.quantile(param.abs(), sparsity)\n",
    "                mask[param.abs() < threshold] = 0\n",
    "                self.masks[self.current_task][name] = mask\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        if self.current_task in self.masks:\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        param.data *= self.masks[self.current_task][name]\n",
    "        return self.model(input_ids, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "K_FOLDS = 5\n",
    "EMBED_DIM = 128  \n",
    "LATENT_DIM = 64\n",
    "LATENT_SIZE = 64\n",
    "NUM_BLOCKS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning_with_intervals(packnet_model, test_loader, criterion, device, start_sparsity, end_sparsity, pruning_ratio):\n",
    "    current_sparsity = start_sparsity\n",
    "    while current_sparsity <= end_sparsity:\n",
    "        print(f\"Applying pruning with sparsity: {current_sparsity:.2f}\")\n",
    "        packnet_model.prune(sparsity=current_sparsity)\n",
    "\n",
    "        print(\"Evaluating after pruning...\")\n",
    "        pruned_test_loss, pruned_test_acc = eval_epoch(packnet_model, test_loader, criterion, device)\n",
    "        print(f\"Pruned Test Loss: {pruned_test_loss:.4f}, Test Accuracy: {pruned_test_acc:.4f}\")\n",
    "\n",
    "        current_sparsity += pruning_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# def save_final_model_after_kfold(results, all_learning_curves, checkpoint_path):\n",
    "#     print(\"Saving final model after K-Fold training...\")\n",
    "\n",
    "#     # Assuming the last fold's model represents the final model\n",
    "#     final_model_state_dict = results[-1]['Fold Results'][-1]['model_state_dict']\n",
    "\n",
    "#     # Save final model checkpoint\n",
    "#     torch.save({\n",
    "#         'model_state_dict': final_model_state_dict,\n",
    "#         'learning_curves': all_learning_curves,\n",
    "#         'results': results\n",
    "#     }, checkpoint_path)\n",
    "\n",
    "#     print(f\"Final model saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group 1 처리 중...\n",
      "\n",
      "  Fold 1/5 처리 중...\n",
      "Pruning 이전 성능:\n",
      "  Test Loss: 1.1096, Test Accuracy: 0.3897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 67\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacknet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_epoch(packnet_model, test_loader, criterion, device)\n\u001b[1;32m     70\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 15\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "all_learning_curves = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, group_file in enumerate(output_paths, start=1):\n",
    "    print(f\"\\nGroup {idx} 처리 중...\")\n",
    "\n",
    "    df = pd.read_csv(group_file)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    input_ids = tokenize_data(df)\n",
    "    labels = torch.tensor(df['Label'].values)\n",
    "\n",
    "    dataset = CustomDataset(input_ids, labels)\n",
    "    kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_learning_curves = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset), start=1):\n",
    "        print(f\"\\n  Fold {fold}/{K_FOLDS} 처리 중...\")\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Perceiver 모델 초기화\n",
    "        perceiver = Perceiver(\n",
    "            input_dim=EMBED_DIM,\n",
    "            latent_dim=LATENT_DIM,\n",
    "            latent_size=LATENT_SIZE,\n",
    "            num_classes=num_classes,\n",
    "            num_blocks=NUM_BLOCKS,\n",
    "            self_attn_layers_per_block=1\n",
    "        )\n",
    "\n",
    "        # CombinedModel 초기화\n",
    "        combined_model = CombinedModel(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            perceiver_model=perceiver\n",
    "        )\n",
    "\n",
    "        # PackNet\n",
    "        packnet_model = PackNet(combined_model)\n",
    "        packnet_model.to(device)\n",
    "        packnet_model.set_task(f\"task_{idx}_{fold}\")\n",
    "\n",
    "        optimizer = optim.Adam(packnet_model.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_losses, test_losses = [], []\n",
    "        train_accuracies, test_accuracies = [], []\n",
    "\n",
    "        # Pruning 이전 성능 평가\n",
    "        print(\"Pruning 이전 성능:\")\n",
    "        initial_test_loss, initial_test_acc = eval_epoch(packnet_model, test_loader, criterion, device)\n",
    "        print(f\"  Test Loss: {initial_test_loss:.4f}, Test Accuracy: {initial_test_acc:.4f}\")\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc = train_epoch(packnet_model, train_loader, criterion, optimizer, device)\n",
    "            test_loss, test_acc = eval_epoch(packnet_model, test_loader, criterion, device)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"epoch {epoch+1}/{EPOCHS}: train loss {train_loss:.4f}, train acc {train_acc:.4f}\")\n",
    "                print(f\"                         test loss {test_loss:.4f}, test acc {test_acc:.4f}\")\n",
    "\n",
    "        # 프루닝 이전 학습된 모델의 체크포인트 저장\n",
    "        checkpoint_path = f\"/checkpoints/group_{idx}_fold_{fold}_pre_pruning.pth.tar\"\n",
    "        torch.save(packnet_model.state_dict(), checkpoint_path)\n",
    "        print(f\"Pre-pruning model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        # 결과 저장\n",
    "        fold_results.append({\n",
    "            \"Fold\": fold,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Confusion Matrix\": None,\n",
    "            \"Classification Report\": None\n",
    "        })\n",
    "        # learning curve\n",
    "        fold_learning_curves.append({\n",
    "            \"Fold\": fold,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"test_accuracies\": test_accuracies\n",
    "        })\n",
    "\n",
    "        # confusion matrix\n",
    "        y_true, y_pred = [], []\n",
    "        packnet_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids_batch = batch['input_ids'].to(device)\n",
    "                labels_batch = batch['labels'].to(device)\n",
    "\n",
    "                outputs = packnet_model(input_ids_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(labels_batch.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.ndim != 2:\n",
    "            raise ValueError(f\"Confusion Matrix must be 2D, but got shape {cm.shape}.\")\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "        fold_results.append({\n",
    "        \"Fold\": fold,\n",
    "        \"Test Accuracy\": test_acc,\n",
    "        \"Confusion Matrix\": cm,\n",
    "        \"Classification Report\": classification_report(y_true, y_pred, output_dict=True)\n",
    "        })\n",
    "    \n",
    "    avg_accuracy = np.mean([fr[\"Test Accuracy\"] for fr in fold_results])\n",
    "    results.append({\n",
    "        \"Group\": idx,\n",
    "        \"Average Test Accuracy\": avg_accuracy,\n",
    "        \"Fold Results\": fold_results\n",
    "    })\n",
    "\n",
    "    all_learning_curves.append({\n",
    "        \"Group\": idx,\n",
    "        \"Fold Learning Curves\": fold_learning_curves\n",
    "    })\n",
    "\n",
    "    print(f\"\\n그룹 {idx}의 {K_FOLDS} 폴드 평균 테스트 정확도: {avg_accuracy:.4f}\")\n",
    "\n",
    "    for curve in fold_learning_curves:\n",
    "        fold_idx = curve[\"Fold\"]\n",
    "\n",
    "      \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, EPOCHS + 1), curve[\"train_losses\"], label=\"Train Loss\")\n",
    "        plt.plot(range(1, EPOCHS + 1), curve[\"test_losses\"], label=\"Test Loss\")\n",
    "        plt.title(f\"Group {idx} - Fold {fold_idx} Learning Curve (Loss)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, EPOCHS + 1), curve[\"train_accuracies\"], label=\"Train Accuracy\")\n",
    "        plt.plot(range(1, EPOCHS + 1), curve[\"test_accuracies\"], label=\"Test Accuracy\")\n",
    "        plt.title(f\"Group {idx} - Fold {fold_idx} Learning Curve (Accuracy)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    for fold_result in fold_results:\n",
    "        fold_idx = fold_result[\"Fold\"]\n",
    "        cm = fold_result[\"Confusion Matrix\"]\n",
    "\n",
    "        if cm is not None and cm.ndim == 2:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=label_encoder.classes_,\n",
    "                        yticklabels=label_encoder.classes_)\n",
    "            plt.title(f\"Group {idx} - Fold {fold_idx} Confusion Matrix\")\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Confusion Matrix for Fold {fold_idx} is invalid or missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_and_prune(group_idx, num_folds, start_sparsity, end_sparsity, pruning_ratio, device):\n",
    "    # Determine the checkpoint path for the last fold\n",
    "    checkpoint_path = f\"/home/jisoo/Perceiver/Perceiver/checkpoints/group_{group_idx}_fold_{num_folds}_pre_pruning.pth.tar\"\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Reinitialize Perceiver and PackNet models\n",
    "    perceiver = Perceiver(\n",
    "        input_dim=128,  # Assuming 128 as input dim for tokenized data\n",
    "        latent_dim=64,\n",
    "        latent_size=64,\n",
    "        num_classes=checkpoint['num_classes'],\n",
    "        num_blocks=4,\n",
    "        self_attn_layers_per_block=1\n",
    "    )\n",
    "\n",
    "    combined_model = CombinedModel(\n",
    "        vocab_size=checkpoint['vocab_size'],\n",
    "        embed_dim=128,\n",
    "        perceiver_model=perceiver\n",
    "    )\n",
    "\n",
    "    packnet_model = PackNet(combined_model)\n",
    "    packnet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    packnet_model.to(device)\n",
    "    print(\"Model successfully loaded from checkpoint.\")\n",
    "\n",
    "    # Pruning logic\n",
    "    current_sparsity = start_sparsity\n",
    "    while current_sparsity <= end_sparsity:\n",
    "        print(f\"Applying pruning at sparsity {current_sparsity:.2f}...\")\n",
    "        packnet_model.prune(sparsity=current_sparsity)\n",
    "\n",
    "        # Optionally evaluate after pruning if test_loader is provided\n",
    "        pruned_test_loss, pruned_test_acc = eval_epoch(packnet_model, test_loader, criterion, device)\n",
    "        print(f\"After pruning (sparsity {current_sparsity:.2f}): Test Loss: {pruned_test_loss:.4f}, Test Acc: {pruned_test_acc:.4f}\")\n",
    "\n",
    "        current_sparsity += pruning_ratio\n",
    "\n",
    "    # Save the pruned model\n",
    "    pruned_checkpoint_path = checkpoint_path.replace(\"pre_pruning\", \"post_pruning\")\n",
    "    torch.save(packnet_model.state_dict(), pruned_checkpoint_path)\n",
    "    print(f\"Pruned model saved at {pruned_checkpoint_path}.\")\n",
    "\n",
    "groups = [1, 2, 3, 4, 5, 6]  # Example group indices\n",
    "num_folds = 5  # Assuming 5 folds for each group\n",
    "start_sparsity = 0.1\n",
    "end_sparsity = 0.3\n",
    "pruning_ratio = 0.1\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "for group_idx in groups:\n",
    "    load_checkpoint_and_prune(\n",
    "        group_idx=group_idx,\n",
    "        num_folds=num_folds,\n",
    "        start_sparsity=start_sparsity,\n",
    "        end_sparsity=end_sparsity,\n",
    "        pruning_ratio=pruning_ratio,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
