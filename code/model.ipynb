{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from perceiver_pytorch.perceiver_pytorch import exists, default, cache_fn, fourier_encode, PreNorm, FeeodForward, Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Feature Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fourier_Encoding(x):\n",
    "    '''Fourier feature position encoding 구현. 논문 5페이지 참고'''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.key_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.query_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.value_proj = nn.Linear(d_in, d_out_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)           # 이게 뭐지\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        keys = self.key_proj(x)\n",
    "        queries = self.query_proj(latent)\n",
    "        values = self.value_proj(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        attended_values = torch.matmul(attention_probs, values)\n",
    "        return attended_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTransformer():\n",
    "    def __init__(self, latent_dim, num_heads, num_layers):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads) # trasformer 로 latent array 반복적으로 update\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        latent = latent.permute(1,0,2)  # Transformer는 (seq_len, batch_size, latent_dim) 형식으로 데이터 받음.\n",
    "        latent = self.transformer(latent)\n",
    "        return latent.permute(1,0,2)    # 이걸 다시 (batch_size, latent_len, latent_dim으로 바꿈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averaging():\n",
    "    def forward(self, latent):\n",
    "        return latent.mean(dim=1)   # latent vector를 평균내서 최종 logits 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, embed_dim, num_heads, num_layers, num_classes):\n",
    "        super(Perceiver, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, latent_dim, embed_dim))\n",
    "\n",
    "        self.cross_attention = CrossAttention(d_in=embed_dim, d_out_kq=embed_dim, d_out_v=embed_dim)\n",
    "        self.latent_transformer = LatentTransformer(latent_dim=latent_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "        \n",
    "        self.averaging = nn.Averaging()\n",
    "        self.classificer = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, latent, batch_size):\n",
    "        x = self.input_proj(x)\n",
    "        latent = self.latents.repeat(batch_size, 1, 1)    # batch 학습 시, batch 내 각각 샘플에 서로 다른 독립적인 latent값을 제공\n",
    "        latent = self.cross_attention(x, latent)\n",
    "        latent = self.latent_transformer(latent)\n",
    "        latent_avg = self.averaging(latent)\n",
    "        logits = self.classifier(latent_avg)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
