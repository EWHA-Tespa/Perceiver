{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped data files saved:\n",
      "/home/youlee/n24news/n24news/group_1.csv\n",
      "/home/youlee/n24news/n24news/group_2.csv\n",
      "/home/youlee/n24news/n24news/group_3.csv\n",
      "/home/youlee/n24news/n24news/group_4.csv\n",
      "/home/youlee/n24news/n24news/group_5.csv\n",
      "/home/youlee/n24news/n24news/group_6.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/youlee/n24news/n24news/captions_and_labels.csv'\n",
    "\n",
    "categories = [\n",
    "    \"Opinion\", \"Art & Design\", \"Television\", \"Music\", \"Travel\",\n",
    "    \"Real Estate\", \"Books\", \"Theater\", \"Health\", \"Sports\",\n",
    "    \"Science\", \"Food\", \"Fashion & Style\", \"Movies\", \"Technology\",\n",
    "    \"Dance\", \"Media\", \"Style\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "filtered_data = data[data['Label'].isin(categories)]\n",
    "\n",
    "\n",
    "label_groups = [categories[i:i + 3] for i in range(0, len(categories), 3)]\n",
    "group_files = []\n",
    "for i, group in enumerate(label_groups, start=1):\n",
    "    group_data = filtered_data[filtered_data['Label'].isin(group)]\n",
    "    output_file_path = f'/home/youlee/n24news/n24news/group_{i}.csv'\n",
    "    group_data.to_csv(output_file_path, index=False)\n",
    "    group_files.append(output_file_path)\n",
    "\n",
    "print(\"Grouped data files saved:\")\n",
    "for file in group_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def tokenize_data(df):\n",
    "    input_ids, attention_masks = [], []\n",
    "    \n",
    "    df['Caption'] = df['Caption'].astype(str).fillna(\"\")\n",
    "\n",
    "    for text in df['Caption']:\n",
    "        encoded = tokenizer(\n",
    "            text, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'].squeeze(0))\n",
    "        attention_masks.append(encoded['attention_mask'].squeeze(0))\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.key_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.query_proj = nn.Linear(d_in, d_out_kq)\n",
    "        self.value_proj = nn.Linear(d_in, d_out_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        keys = self.key_proj(x)\n",
    "        queries = self.query_proj(latent)\n",
    "        values = self.value_proj(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        attended_values = torch.matmul(attention_probs, values)\n",
    "        return attended_values\n",
    "\n",
    "class LatentTransformer(nn.Module):\n",
    "    def __init__(self, latent_dim, num_heads, num_layers, embed_dim):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        latent = latent.permute(1, 0, 2)\n",
    "        latent = self.transformer(latent)\n",
    "        return latent.permute(1, 0, 2)\n",
    "\n",
    "class Averaging(nn.Module):\n",
    "    def forward(self, latent):\n",
    "        return latent.mean(dim=1)\n",
    "\n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, latent_dim, num_heads, num_layers, num_classes):\n",
    "        super(Perceiver, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.input_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, latent_dim, embed_dim))\n",
    "        self.cross_attention = CrossAttention(d_in=embed_dim, d_out_kq=embed_dim, d_out_v=embed_dim)\n",
    "        self.latent_transformer = LatentTransformer(latent_dim=latent_dim, num_heads=num_heads,\n",
    "                                                    num_layers=num_layers, embed_dim=embed_dim)\n",
    "        self.averaging = Averaging()\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        latent = self.latents.repeat(batch_size, 1, 1)\n",
    "        latent = self.cross_attention(x, latent)\n",
    "        latent = self.latent_transformer(latent)\n",
    "        latent_avg = self.averaging(latent)\n",
    "        logits = self.classifier(latent_avg)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "results = []\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "LATENT_DIM = 64\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Group 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 1 Epoch 1/10: Train Loss: 0.7289, Train Acc: 0.6979\n",
      "                             Test Loss: 0.5983, Test Acc: 0.7778\n",
      "  Group 1 Epoch 2/10: Train Loss: 0.5293, Train Acc: 0.7991\n",
      "                             Test Loss: 0.5231, Test Acc: 0.7908\n",
      "  Group 1 Epoch 3/10: Train Loss: 0.4550, Train Acc: 0.8298\n",
      "                             Test Loss: 0.5147, Test Acc: 0.8114\n",
      "  Group 1 Epoch 4/10: Train Loss: 0.3917, Train Acc: 0.8607\n",
      "                             Test Loss: 0.4681, Test Acc: 0.8278\n",
      "  Group 1 Epoch 5/10: Train Loss: 0.3525, Train Acc: 0.8755\n",
      "                             Test Loss: 0.4898, Test Acc: 0.8217\n",
      "  Group 1 Epoch 6/10: Train Loss: 0.3051, Train Acc: 0.8924\n",
      "                             Test Loss: 0.5371, Test Acc: 0.8176\n",
      "  Group 1 Epoch 7/10: Train Loss: 0.2752, Train Acc: 0.9015\n",
      "                             Test Loss: 0.5252, Test Acc: 0.8224\n",
      "  Group 1 Epoch 8/10: Train Loss: 0.2359, Train Acc: 0.9194\n",
      "                             Test Loss: 0.5131, Test Acc: 0.8285\n",
      "  Group 1 Epoch 9/10: Train Loss: 0.2052, Train Acc: 0.9297\n",
      "                             Test Loss: 0.5927, Test Acc: 0.8251\n",
      "  Group 1 Epoch 10/10: Train Loss: 0.1710, Train Acc: 0.9413\n",
      "                             Test Loss: 0.5450, Test Acc: 0.8354\n",
      "\n",
      "Processing Group 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 2 Epoch 1/10: Train Loss: 0.9197, Train Acc: 0.5693\n",
      "                             Test Loss: 0.8336, Test Acc: 0.6211\n",
      "  Group 2 Epoch 2/10: Train Loss: 0.7159, Train Acc: 0.7024\n",
      "                             Test Loss: 0.9288, Test Acc: 0.6094\n",
      "  Group 2 Epoch 3/10: Train Loss: 0.6236, Train Acc: 0.7471\n",
      "                             Test Loss: 0.6342, Test Acc: 0.7467\n",
      "  Group 2 Epoch 4/10: Train Loss: 0.5277, Train Acc: 0.7916\n",
      "                             Test Loss: 0.5748, Test Acc: 0.7757\n",
      "  Group 2 Epoch 5/10: Train Loss: 0.4651, Train Acc: 0.8212\n",
      "                             Test Loss: 0.5920, Test Acc: 0.7688\n",
      "  Group 2 Epoch 6/10: Train Loss: 0.4118, Train Acc: 0.8448\n",
      "                             Test Loss: 0.5829, Test Acc: 0.7750\n",
      "  Group 2 Epoch 7/10: Train Loss: 0.3671, Train Acc: 0.8641\n",
      "                             Test Loss: 0.6339, Test Acc: 0.7764\n",
      "  Group 2 Epoch 8/10: Train Loss: 0.3321, Train Acc: 0.8761\n",
      "                             Test Loss: 0.5860, Test Acc: 0.7874\n",
      "  Group 2 Epoch 9/10: Train Loss: 0.2808, Train Acc: 0.8980\n",
      "                             Test Loss: 0.5870, Test Acc: 0.7930\n",
      "  Group 2 Epoch 10/10: Train Loss: 0.2350, Train Acc: 0.9152\n",
      "                             Test Loss: 0.6615, Test Acc: 0.7854\n",
      "\n",
      "Processing Group 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 3 Epoch 1/10: Train Loss: 0.7049, Train Acc: 0.6852\n",
      "                             Test Loss: 0.5894, Test Acc: 0.7545\n",
      "  Group 3 Epoch 2/10: Train Loss: 0.4881, Train Acc: 0.8013\n",
      "                             Test Loss: 0.5089, Test Acc: 0.8008\n",
      "  Group 3 Epoch 3/10: Train Loss: 0.4200, Train Acc: 0.8352\n",
      "                             Test Loss: 0.4721, Test Acc: 0.8140\n",
      "  Group 3 Epoch 4/10: Train Loss: 0.3620, Train Acc: 0.8650\n",
      "                             Test Loss: 0.4688, Test Acc: 0.8313\n",
      "  Group 3 Epoch 5/10: Train Loss: 0.3186, Train Acc: 0.8830\n",
      "                             Test Loss: 0.4483, Test Acc: 0.8292\n",
      "  Group 3 Epoch 6/10: Train Loss: 0.2797, Train Acc: 0.9023\n",
      "                             Test Loss: 0.4844, Test Acc: 0.8278\n",
      "  Group 3 Epoch 7/10: Train Loss: 0.2517, Train Acc: 0.9108\n",
      "                             Test Loss: 0.4652, Test Acc: 0.8340\n",
      "  Group 3 Epoch 8/10: Train Loss: 0.2193, Train Acc: 0.9244\n",
      "                             Test Loss: 0.4797, Test Acc: 0.8382\n",
      "  Group 3 Epoch 9/10: Train Loss: 0.1836, Train Acc: 0.9369\n",
      "                             Test Loss: 0.5146, Test Acc: 0.8299\n",
      "  Group 3 Epoch 10/10: Train Loss: 0.1518, Train Acc: 0.9483\n",
      "                             Test Loss: 0.5760, Test Acc: 0.8347\n",
      "\n",
      "Processing Group 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 4 Epoch 1/10: Train Loss: 0.9534, Train Acc: 0.5269\n",
      "                             Test Loss: 0.7733, Test Acc: 0.6623\n",
      "  Group 4 Epoch 2/10: Train Loss: 0.6885, Train Acc: 0.7036\n",
      "                             Test Loss: 0.6692, Test Acc: 0.7117\n",
      "  Group 4 Epoch 3/10: Train Loss: 0.5625, Train Acc: 0.7684\n",
      "                             Test Loss: 0.6177, Test Acc: 0.7458\n",
      "  Group 4 Epoch 4/10: Train Loss: 0.4610, Train Acc: 0.8142\n",
      "                             Test Loss: 0.5410, Test Acc: 0.7967\n",
      "  Group 4 Epoch 5/10: Train Loss: 0.3789, Train Acc: 0.8522\n",
      "                             Test Loss: 0.5716, Test Acc: 0.7869\n",
      "  Group 4 Epoch 6/10: Train Loss: 0.3179, Train Acc: 0.8767\n",
      "                             Test Loss: 0.5314, Test Acc: 0.8127\n",
      "  Group 4 Epoch 7/10: Train Loss: 0.2495, Train Acc: 0.9070\n",
      "                             Test Loss: 0.4594, Test Acc: 0.8336\n",
      "  Group 4 Epoch 8/10: Train Loss: 0.2114, Train Acc: 0.9201\n",
      "                             Test Loss: 0.4744, Test Acc: 0.8370\n",
      "  Group 4 Epoch 9/10: Train Loss: 0.1463, Train Acc: 0.9490\n",
      "                             Test Loss: 0.5033, Test Acc: 0.8419\n",
      "  Group 4 Epoch 10/10: Train Loss: 0.1154, Train Acc: 0.9593\n",
      "                             Test Loss: 0.5107, Test Acc: 0.8426\n",
      "\n",
      "Processing Group 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 5 Epoch 1/10: Train Loss: 0.7131, Train Acc: 0.6865\n",
      "                             Test Loss: 0.6116, Test Acc: 0.7516\n",
      "  Group 5 Epoch 2/10: Train Loss: 0.5252, Train Acc: 0.7962\n",
      "                             Test Loss: 0.5015, Test Acc: 0.7999\n",
      "  Group 5 Epoch 3/10: Train Loss: 0.4339, Train Acc: 0.8361\n",
      "                             Test Loss: 0.4639, Test Acc: 0.8153\n",
      "  Group 5 Epoch 4/10: Train Loss: 0.3816, Train Acc: 0.8583\n",
      "                             Test Loss: 0.4897, Test Acc: 0.8090\n",
      "  Group 5 Epoch 5/10: Train Loss: 0.3391, Train Acc: 0.8728\n",
      "                             Test Loss: 0.4590, Test Acc: 0.8293\n",
      "  Group 5 Epoch 6/10: Train Loss: 0.2962, Train Acc: 0.8954\n",
      "                             Test Loss: 0.4687, Test Acc: 0.8293\n",
      "  Group 5 Epoch 7/10: Train Loss: 0.2499, Train Acc: 0.9108\n",
      "                             Test Loss: 0.4727, Test Acc: 0.8244\n",
      "  Group 5 Epoch 8/10: Train Loss: 0.2187, Train Acc: 0.9244\n",
      "                             Test Loss: 0.5047, Test Acc: 0.8279\n",
      "  Group 5 Epoch 9/10: Train Loss: 0.1848, Train Acc: 0.9365\n",
      "                             Test Loss: 0.5240, Test Acc: 0.8307\n",
      "  Group 5 Epoch 10/10: Train Loss: 0.1514, Train Acc: 0.9501\n",
      "                             Test Loss: 0.6006, Test Acc: 0.8265\n",
      "\n",
      "Processing Group 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youlee/n24news/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group 6 Epoch 1/10: Train Loss: 0.8411, Train Acc: 0.5975\n",
      "                             Test Loss: 0.7238, Test Acc: 0.6800\n",
      "  Group 6 Epoch 2/10: Train Loss: 0.6242, Train Acc: 0.7187\n",
      "                             Test Loss: 0.6761, Test Acc: 0.7171\n",
      "  Group 6 Epoch 3/10: Train Loss: 0.5418, Train Acc: 0.7647\n",
      "                             Test Loss: 0.6024, Test Acc: 0.7404\n",
      "  Group 6 Epoch 4/10: Train Loss: 0.5019, Train Acc: 0.7900\n",
      "                             Test Loss: 0.5945, Test Acc: 0.7542\n",
      "  Group 6 Epoch 5/10: Train Loss: 0.4523, Train Acc: 0.8127\n",
      "                             Test Loss: 0.5802, Test Acc: 0.7615\n",
      "  Group 6 Epoch 6/10: Train Loss: 0.4071, Train Acc: 0.8342\n",
      "                             Test Loss: 0.5884, Test Acc: 0.7578\n",
      "  Group 6 Epoch 7/10: Train Loss: 0.3706, Train Acc: 0.8553\n",
      "                             Test Loss: 0.5685, Test Acc: 0.7636\n",
      "  Group 6 Epoch 8/10: Train Loss: 0.3188, Train Acc: 0.8787\n",
      "                             Test Loss: 0.6020, Test Acc: 0.7695\n",
      "  Group 6 Epoch 9/10: Train Loss: 0.2823, Train Acc: 0.8929\n",
      "                             Test Loss: 0.6603, Test Acc: 0.7520\n",
      "  Group 6 Epoch 10/10: Train Loss: 0.2477, Train Acc: 0.9047\n",
      "                             Test Loss: 0.6969, Test Acc: 0.7644\n",
      "\n",
      "Group 1 Results:\n",
      "Test Accuracy: 0.8354\n",
      "              precision    recall  f1-score      support\n",
      "0              0.858720  0.810417  0.833869   480.000000\n",
      "1              0.793307  0.859275  0.824974   469.000000\n",
      "2              0.857143  0.836935  0.846918   509.000000\n",
      "accuracy       0.835391  0.835391  0.835391     0.835391\n",
      "macro avg      0.836390  0.835542  0.835254  1458.000000\n",
      "weighted avg   0.837128  0.835391  0.835564  1458.000000\n",
      "\n",
      "Group 2 Results:\n",
      "Test Accuracy: 0.7854\n",
      "              precision    recall  f1-score      support\n",
      "0              0.800000  0.863544  0.830558   491.000000\n",
      "1              0.727273  0.839400  0.779324   467.000000\n",
      "2              0.847368  0.655804  0.739380   491.000000\n",
      "accuracy       0.785369  0.785369  0.785369     0.785369\n",
      "macro avg      0.791547  0.786250  0.783087  1449.000000\n",
      "weighted avg   0.792612  0.785369  0.783150  1449.000000\n",
      "\n",
      "Group 3 Results:\n",
      "Test Accuracy: 0.8347\n",
      "              precision    recall  f1-score      support\n",
      "0              0.770563  0.784141  0.777293   454.000000\n",
      "1              0.832685  0.880658  0.856000   486.000000\n",
      "2              0.900000  0.835968  0.866803   506.000000\n",
      "accuracy       0.834716  0.834716  0.834716     0.834716\n",
      "macro avg      0.834416  0.833589  0.833365  1446.000000\n",
      "weighted avg   0.836736  0.834716  0.835069  1446.000000\n",
      "\n",
      "Group 4 Results:\n",
      "Test Accuracy: 0.8426\n",
      "              precision    recall  f1-score      support\n",
      "0              0.828283  0.840164  0.834181   488.000000\n",
      "1              0.819533  0.817797  0.818664   472.000000\n",
      "2              0.880851  0.869748  0.875264   476.000000\n",
      "accuracy       0.842618  0.842618  0.842618     0.842618\n",
      "macro avg      0.842889  0.842569  0.842703  1436.000000\n",
      "weighted avg   0.842832  0.842618  0.842699  1436.000000\n",
      "\n",
      "Group 5 Results:\n",
      "Test Accuracy: 0.8265\n",
      "              precision    recall  f1-score      support\n",
      "0              0.751606  0.776549  0.763874   452.000000\n",
      "1              0.910112  0.823171  0.864461   492.000000\n",
      "2              0.822050  0.876289  0.848303   485.000000\n",
      "accuracy       0.826452  0.826452  0.826452     0.826452\n",
      "macro avg      0.827923  0.825336  0.825546  1429.000000\n",
      "weighted avg   0.830088  0.826452  0.827161  1429.000000\n",
      "\n",
      "Group 6 Results:\n",
      "Test Accuracy: 0.7644\n",
      "              precision    recall  f1-score      support\n",
      "0              0.898734  0.785398  0.838253   452.000000\n",
      "1              0.769874  0.751020  0.760331   490.000000\n",
      "2              0.653386  0.757506  0.701604   433.000000\n",
      "accuracy       0.764364  0.764364  0.764364     0.764364\n",
      "macro avg      0.773998  0.764641  0.766729  1375.000000\n",
      "weighted avg   0.775551  0.764364  0.767452  1375.000000\n"
     ]
    }
   ],
   "source": [
    "for idx, group_file in enumerate(group_files, start=1):\n",
    "    print(f\"\\nProcessing Group {idx}...\")\n",
    "\n",
    "    df = pd.read_csv(group_file)\n",
    "    df['Label'] = LabelEncoder().fit_transform(df['Label'])\n",
    "\n",
    "    input_ids, attention_masks = tokenize_data(df)\n",
    "    labels = torch.tensor(df['Label'].values)\n",
    "\n",
    "    dataset = CustomDataset(input_ids, attention_masks, labels)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = Perceiver(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, latent_dim=LATENT_DIM,\n",
    "                      num_heads=NUM_HEADS, num_layers=NUM_LAYERS, num_classes=len(df['Label'].unique()))\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = eval_epoch(model, test_loader, criterion, device)\n",
    "        print(f'  Group {idx} Epoch {epoch+1}/{EPOCHS}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'                             Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    results.append({\n",
    "        \"Group\": idx,\n",
    "        \"Test Accuracy\": test_acc,\n",
    "        \"Classification Report\": report\n",
    "    })\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\nGroup {result['Group']} Results:\")\n",
    "    print(f\"Test Accuracy: {result['Test Accuracy']:.4f}\")\n",
    "    print(pd.DataFrame(result['Classification Report']).transpose())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
